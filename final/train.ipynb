{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import random_split, TensorDataset\n",
    "\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "import optuna\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_loss_in_csv(model_num, loss):\n",
    "    df = pd.read_csv('loss.csv')\n",
    "    df = df.reindex(range(len(loss)))\n",
    "    df[f\"{model_num}\"] = loss\n",
    "    df.to_csv('loss.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the gradient boosting regression model with PyTorch\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, dropout_rate):\n",
    "        super(Net, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout_rate)\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3= nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc4= nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc5= nn.Linear(hidden_size, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.dropout(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc3(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc4(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc5(out)\n",
    "        return out\n",
    "    \n",
    "class CustomLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomLoss, self).__init__()\n",
    "        \n",
    "    def forward(self, outputs, labels):\n",
    "        # Define your custom loss function here\n",
    "        # This is a simple example, replace it with your own function\n",
    "        custom_loss = 3*(torch.abs(labels - outputs))*(torch.abs(labels - 1/3) + torch.abs(labels -2/3))\n",
    "        return custom_loss\n",
    "    \n",
    "\n",
    "    \n",
    "def objective(trial):\n",
    "    # Sample hyperparameters\n",
    "    input_size = 8\n",
    "    hidden_size = trial.suggest_int('hidden_size', 3, 1000, log=True)  # Single hidden size for all layers\n",
    "    output_size = 1\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-1, log=True)\n",
    "    dropout_rate = trial.suggest_float('dropout_rate', 0.0, 0.3)\n",
    "\n",
    "    # Instantiate the model with sampled hyperparameters\n",
    "    model = Net(input_size, hidden_size, output_size, dropout_rate)\n",
    "\n",
    "    # Define the loss function and optimizer\n",
    "    criterion = CustomLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(100):\n",
    "        training_loss = 0.0\n",
    "        eval_loss = 0.0\n",
    "\n",
    "        # Iterate over the DataLoader for training data\n",
    "        for i, data in enumerate(train_loader):\n",
    "            inputs, labels = data\n",
    "            inputs = inputs.float()\n",
    "            labels = labels.float()\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            outputs = outputs.reshape(-1)\n",
    "            loss = criterion(outputs, labels).sum()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            training_loss += loss.item()\n",
    "\n",
    "        model.eval()\n",
    "        outloss = 0\n",
    "        # Iterate over the DataLoader for test data\n",
    "        with torch.no_grad():\n",
    "            for i, data in enumerate(test_loader):\n",
    "                inputs, labels = data\n",
    "                inputs = inputs.float()\n",
    "                labels = labels.float()\n",
    "                outputs = model(inputs).reshape(-1)\n",
    "                loss = criterion(outputs, labels).sum()\n",
    "                eval_loss += loss\n",
    "\n",
    "        # Optuna logs the running loss for each epoch\n",
    "        trial.report(eval_loss, epoch)\n",
    "\n",
    "        # Handle pruning based on the intermediate value\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "    # Return the running loss as the objective value to minimize\n",
    "    return eval_loss\n",
    "\n",
    "\n",
    "class station_model():\n",
    "    def __init__(self, station):\n",
    "        self.station = station\n",
    "        self.epoch_cnt = 40\n",
    "        self.trial_cnt = 20\n",
    "\n",
    "    def get_data_loader(self):\n",
    "        df = pd.read_parquet(f'parquets/{self.station}train.parquet')\n",
    "        TOT = df['tot'].iloc[0]\n",
    "        df['sbi'] = df['sbi']/TOT\n",
    "        df['time'] = df['time']/1440\n",
    "\n",
    "        # x is dataset without 'sbi', y is 'sbi'\n",
    "        X = df.drop(['tot', 'sbi','bemp' ,'act', 'tot', 'station'], axis=1)\n",
    "        y = df['sbi']\n",
    "\n",
    "        X = X.to_numpy()\n",
    "        y = y.to_numpy()\n",
    "\n",
    "        X = torch.from_numpy(X)\n",
    "        y = torch.from_numpy(y)\n",
    "        dataset = TensorDataset(X, y)\n",
    "\n",
    "        # get train, test loader\n",
    "        self.all_data_loader = torch.utils.data.DataLoader(dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "        # split train, test\n",
    "        train_size = int(0.8 * len(dataset))\n",
    "        test_size = len(dataset) - train_size\n",
    "        train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "        # get train, test loader\n",
    "        self.train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "        self.test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=128, shuffle=True)\n",
    "        print(f'get data loader for {self.station}')\n",
    "\n",
    "    def objective(self,trial):\n",
    "        # Sample hyperparameters\n",
    "        input_size = 8\n",
    "        hidden_size = trial.suggest_int('hidden_size', 3, 2000, log=True)  # Single hidden size for all layers\n",
    "        output_size = 1\n",
    "        learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-1, log=True)\n",
    "        dropout_rate = trial.suggest_float('dropout_rate', 0.2, 0.5)\n",
    "\n",
    "        # Instantiate the model with sampled hyperparameters\n",
    "        model = Net(input_size, hidden_size, output_size, dropout_rate)\n",
    "\n",
    "        # Define the loss function and optimizer\n",
    "        criterion = CustomLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "        # Training loop\n",
    "        for epoch in range(self.epoch_cnt):\n",
    "            training_loss = 0.0\n",
    "            eval_loss = 0.0\n",
    "            # Iterate over the DataLoader for training data\n",
    "            for i, data in enumerate(self.train_loader):\n",
    "                inputs, labels = data\n",
    "                inputs = inputs.float()\n",
    "                labels = labels.float()\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                outputs = outputs.reshape(-1)\n",
    "                loss = criterion(outputs, labels).sum()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                training_loss += loss.item()\n",
    "\n",
    "            model.eval()\n",
    "            # Iterate over the DataLoader for test data\n",
    "            with torch.no_grad():\n",
    "                for i, data in enumerate(self.test_loader):\n",
    "                    inputs, labels = data\n",
    "                    inputs = inputs.float()\n",
    "                    labels = labels.float()\n",
    "                    outputs = model(inputs).reshape(-1)\n",
    "                    loss = criterion(outputs, labels).sum()\n",
    "                    eval_loss += loss\n",
    "\n",
    "            # Optuna logs the running loss for each epoch\n",
    "            trial.report(eval_loss, epoch)\n",
    "            # Handle pruning based on the intermediate value\n",
    "            if trial.should_prune():\n",
    "                raise optuna.exceptions.TrialPruned()\n",
    "        # Return the running loss as the objective value to minimize\n",
    "        return eval_loss      \n",
    "\n",
    "\n",
    "    def get_best_param(self):\n",
    "        # Create an Optuna Study\n",
    "        study = optuna.create_study(direction='minimize', storage='sqlite:///db.sqlite3', study_name=f'{self.station}', load_if_exists=True)\n",
    "        num_trials = len(study.trials)\n",
    "\n",
    "        if num_trials < self.trial_cnt:\n",
    "            study.optimize(self.objective, n_trials=self.trial_cnt)\n",
    "        # Run the optimization process\n",
    "\n",
    "        print(f'{self.station} eval_loss: {study.best_trial.value}')\n",
    "\n",
    "        # Access the best hyperparameters\n",
    "        best_params = study.best_trial.params\n",
    "\n",
    "        self.best_params = best_params\n",
    "\n",
    "        print(self.station,' Best hyperparameters:', best_params)\n",
    "\n",
    "    def train(self):\n",
    "        # Instantiate the final model with the best hyperparameters\n",
    "        final_model = Net(8, self.best_params['hidden_size'], 1, self.best_params['dropout_rate'])\n",
    "\n",
    "        # ... rest of your training code for the final model\n",
    "        criterion = CustomLoss()\n",
    "        optimizer = optim.Adam(final_model.parameters(), lr=self.best_params['learning_rate'])\n",
    "\n",
    "        # train with whole dataset\n",
    "        running_loss = 0.0\n",
    "        loss_list = []\n",
    "        for epoch in range(self.epoch_cnt):\n",
    "            for i, data in enumerate(self.all_data_loader):\n",
    "                inputs, labels = data\n",
    "                inputs = inputs.float()\n",
    "                labels = labels.float()\n",
    "                optimizer.zero_grad()\n",
    "                outputs = final_model(inputs)\n",
    "                outputs = outputs.squeeze()\n",
    "                loss = criterion(outputs, labels).sum()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item()/128\n",
    "            print(f'epoch: {epoch+1}, loss: {running_loss/self.all_data_loader.__len__()}')\n",
    "            loss_list.append(running_loss/self.all_data_loader.__len__())\n",
    "            running_loss = 0.0\n",
    "        save_loss_in_csv(self.station, loss_list)\n",
    "        \n",
    "        final_model.eval()\n",
    "        # save model\n",
    "        torch.save(final_model, f'models/{self.station}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['500101035' '500101093' '500101185' '500105066' '500119049' '500119059'\n",
      " '500119069']\n"
     ]
    }
   ],
   "source": [
    "available_station = np.loadtxt('html.2023.final.data/sno_test_set.txt', dtype='str')\n",
    "# available_station = available_station[:35]\n",
    "folder_path = 'models/'  # replace with your actual folder path\n",
    "trained_models = os.listdir(folder_path)\n",
    "available_station = available_station[range(0, 100, 10)]    \n",
    "available_station = np.setdiff1d(available_station, trained_models)\n",
    "print(available_station)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-26 17:32:26,419] Using an existing study with name '500101035' instead of creating a new one.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get data loader for 500101035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-26 17:50:57,589] Trial 4 finished with value: 4585.388671875 and parameters: {'hidden_size': 1790, 'learning_rate': 0.03463508502787367, 'dropout_rate': 0.20666852054502455}. Best is trial 2 with value: 2843.518310546875.\n",
      "[I 2023-12-26 17:51:21,187] Trial 5 finished with value: 4509.77783203125 and parameters: {'hidden_size': 3, 'learning_rate': 0.0028522938552576565, 'dropout_rate': 0.35512273423972884}. Best is trial 2 with value: 2843.518310546875.\n",
      "[I 2023-12-26 17:51:22,644] Trial 6 pruned. \n",
      "[I 2023-12-26 17:51:46,807] Trial 7 finished with value: 2910.866455078125 and parameters: {'hidden_size': 42, 'learning_rate': 0.012223742679691054, 'dropout_rate': 0.3128331746866366}. Best is trial 2 with value: 2843.518310546875.\n",
      "[I 2023-12-26 17:52:56,153] Trial 8 finished with value: 2741.30712890625 and parameters: {'hidden_size': 242, 'learning_rate': 0.0002175697336657348, 'dropout_rate': 0.3345309177813923}. Best is trial 8 with value: 2741.30712890625.\n",
      "[I 2023-12-26 17:52:57,221] Trial 9 pruned. \n",
      "[I 2023-12-26 17:53:35,540] Trial 10 finished with value: 2740.59765625 and parameters: {'hidden_size': 112, 'learning_rate': 0.0012793278061516923, 'dropout_rate': 0.3980241366138075}. Best is trial 10 with value: 2740.59765625.\n",
      "[I 2023-12-26 17:53:36,372] Trial 11 pruned. \n",
      "[I 2023-12-26 17:54:39,679] Trial 12 finished with value: 2736.587646484375 and parameters: {'hidden_size': 199, 'learning_rate': 0.0008005707053409129, 'dropout_rate': 0.3934600214938739}. Best is trial 12 with value: 2736.587646484375.\n",
      "[I 2023-12-26 17:56:02,255] Trial 13 finished with value: 2732.87548828125 and parameters: {'hidden_size': 256, 'learning_rate': 0.0009687431249512314, 'dropout_rate': 0.41129216979747124}. Best is trial 13 with value: 2732.87548828125.\n",
      "[I 2023-12-26 17:57:57,471] Trial 14 finished with value: 2738.5087890625 and parameters: {'hidden_size': 455, 'learning_rate': 0.0005657872016185547, 'dropout_rate': 0.4193746776453901}. Best is trial 13 with value: 2732.87548828125.\n",
      "[I 2023-12-26 18:02:52,978] Trial 15 pruned. \n",
      "[I 2023-12-26 18:03:24,055] Trial 16 pruned. \n",
      "[I 2023-12-26 18:03:40,492] Trial 17 pruned. \n",
      "[I 2023-12-26 18:04:26,329] Trial 18 finished with value: 2766.047119140625 and parameters: {'hidden_size': 247, 'learning_rate': 0.0019587115610069364, 'dropout_rate': 0.37438171908428797}. Best is trial 13 with value: 2732.87548828125.\n",
      "[I 2023-12-26 18:09:15,757] Trial 19 pruned. \n",
      "[I 2023-12-26 18:09:17,111] Trial 20 pruned. \n",
      "[I 2023-12-26 18:09:59,529] Trial 21 pruned. \n",
      "[I 2023-12-26 18:12:04,325] Trial 22 finished with value: 2722.590087890625 and parameters: {'hidden_size': 509, 'learning_rate': 0.000551408530961024, 'dropout_rate': 0.41640643507863745}. Best is trial 22 with value: 2722.590087890625.\n",
      "[I 2023-12-26 18:13:55,737] Trial 23 finished with value: 2717.1982421875 and parameters: {'hidden_size': 492, 'learning_rate': 0.0009311824173693834, 'dropout_rate': 0.41483751855165996}. Best is trial 23 with value: 2717.1982421875.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500101035 eval_loss: 2717.1982421875\n",
      "500101035  Best hyperparameters: {'hidden_size': 492, 'learning_rate': 0.0009311824173693834, 'dropout_rate': 0.41483751855165996}\n",
      "epoch: 1, loss: 0.2740969231927582\n",
      "epoch: 2, loss: 0.22809065771768647\n",
      "epoch: 3, loss: 0.21936198044405766\n",
      "epoch: 4, loss: 0.2141165563781432\n",
      "epoch: 5, loss: 0.21121932394529513\n",
      "epoch: 6, loss: 0.20951932915738325\n",
      "epoch: 7, loss: 0.2079972903557889\n",
      "epoch: 8, loss: 0.20729470674280095\n",
      "epoch: 9, loss: 0.2060112682468604\n",
      "epoch: 10, loss: 0.20474451079470532\n",
      "epoch: 11, loss: 0.20537590629455307\n",
      "epoch: 12, loss: 0.20532331395939799\n",
      "epoch: 13, loss: 0.20400357847126366\n",
      "epoch: 14, loss: 0.20378213665768322\n",
      "epoch: 15, loss: 0.20390419147089514\n",
      "epoch: 16, loss: 0.20365609382043542\n",
      "epoch: 17, loss: 0.20348543230359675\n",
      "epoch: 18, loss: 0.2032204017069119\n",
      "epoch: 19, loss: 0.2028490648479778\n",
      "epoch: 20, loss: 0.20306813441229948\n",
      "epoch: 21, loss: 0.20273200457632853\n",
      "epoch: 22, loss: 0.20177114007256627\n",
      "epoch: 23, loss: 0.20217137907292\n",
      "epoch: 24, loss: 0.20184882455887387\n",
      "epoch: 25, loss: 0.20175405205544378\n",
      "epoch: 26, loss: 0.20157512419094797\n",
      "epoch: 27, loss: 0.20114071092472444\n",
      "epoch: 28, loss: 0.20161723632879075\n",
      "epoch: 29, loss: 0.2017148952625601\n",
      "epoch: 30, loss: 0.20129420727907468\n",
      "epoch: 31, loss: 0.20076629930349751\n",
      "epoch: 32, loss: 0.20134481379394964\n",
      "epoch: 33, loss: 0.2014103423394458\n",
      "epoch: 34, loss: 0.20074645576780795\n",
      "epoch: 35, loss: 0.20105687023041344\n",
      "epoch: 36, loss: 0.20109912123771653\n",
      "epoch: 37, loss: 0.20121918904739733\n",
      "epoch: 38, loss: 0.2010558687059459\n",
      "epoch: 39, loss: 0.20064866389383612\n",
      "epoch: 40, loss: 0.20083756076519818\n",
      "500101035 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-26 18:17:21,597] A new study created in RDB with name: 500101093\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get data loader for 500101093\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-26 18:20:12,113] Trial 0 finished with value: 4300.22705078125 and parameters: {'hidden_size': 492, 'learning_rate': 0.04754380217892838, 'dropout_rate': 0.34089674476571874}. Best is trial 0 with value: 4300.22705078125.\n",
      "[I 2023-12-26 18:20:47,886] Trial 1 finished with value: 3582.864013671875 and parameters: {'hidden_size': 77, 'learning_rate': 0.00041585373621354265, 'dropout_rate': 0.38549708272061417}. Best is trial 1 with value: 3582.864013671875.\n",
      "[I 2023-12-26 18:21:11,444] Trial 2 finished with value: 3719.177978515625 and parameters: {'hidden_size': 17, 'learning_rate': 0.0005736682953449294, 'dropout_rate': 0.27002072821402295}. Best is trial 1 with value: 3582.864013671875.\n",
      "[I 2023-12-26 18:21:47,577] Trial 3 finished with value: 3591.927734375 and parameters: {'hidden_size': 83, 'learning_rate': 0.0008474775513406351, 'dropout_rate': 0.4262298188126612}. Best is trial 1 with value: 3582.864013671875.\n",
      "[I 2023-12-26 18:22:10,719] Trial 4 finished with value: 4066.079345703125 and parameters: {'hidden_size': 9, 'learning_rate': 8.997089702054181e-05, 'dropout_rate': 0.267863271357557}. Best is trial 1 with value: 3582.864013671875.\n",
      "[I 2023-12-26 18:22:43,568] Trial 5 finished with value: 3620.96875 and parameters: {'hidden_size': 126, 'learning_rate': 0.0033946400789697595, 'dropout_rate': 0.32067444854573207}. Best is trial 1 with value: 3582.864013671875.\n",
      "[I 2023-12-26 18:22:44,577] Trial 6 pruned. \n",
      "[I 2023-12-26 18:39:45,651] Trial 7 finished with value: 3544.05517578125 and parameters: {'hidden_size': 1679, 'learning_rate': 0.00024651123757760135, 'dropout_rate': 0.4659815877149898}. Best is trial 7 with value: 3544.05517578125.\n",
      "[I 2023-12-26 18:39:46,324] Trial 8 pruned. \n",
      "[I 2023-12-26 18:40:10,813] Trial 9 pruned. \n",
      "[I 2023-12-26 18:40:48,062] Trial 10 pruned. \n",
      "[I 2023-12-26 18:40:48,804] Trial 11 pruned. \n",
      "[I 2023-12-26 18:40:49,346] Trial 12 pruned. \n",
      "[I 2023-12-26 18:52:16,448] Trial 13 finished with value: 3567.89697265625 and parameters: {'hidden_size': 1366, 'learning_rate': 0.0002492988359011232, 'dropout_rate': 0.48039510336139457}. Best is trial 7 with value: 3544.05517578125.\n",
      "[I 2023-12-26 18:52:52,278] Trial 14 pruned. \n",
      "[I 2023-12-26 18:52:59,605] Trial 15 pruned. \n",
      "[I 2023-12-26 18:53:13,882] Trial 16 pruned. \n",
      "[I 2023-12-26 18:53:17,235] Trial 17 pruned. \n",
      "[I 2023-12-26 19:00:41,505] Trial 18 finished with value: 3588.1005859375 and parameters: {'hidden_size': 1002, 'learning_rate': 0.00015863933546617914, 'dropout_rate': 0.434786363639164}. Best is trial 7 with value: 3544.05517578125.\n",
      "[I 2023-12-26 19:01:56,818] Trial 19 finished with value: 3610.302734375 and parameters: {'hidden_size': 319, 'learning_rate': 0.00074570999823354, 'dropout_rate': 0.4740375090739977}. Best is trial 7 with value: 3544.05517578125.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500101093 eval_loss: 3544.05517578125\n",
      "500101093  Best hyperparameters: {'hidden_size': 1679, 'learning_rate': 0.00024651123757760135, 'dropout_rate': 0.4659815877149898}\n",
      "epoch: 1, loss: 0.28764040240234523\n",
      "epoch: 2, loss: 0.2738801681714532\n",
      "epoch: 3, loss: 0.269760274788264\n",
      "epoch: 4, loss: 0.2678806133451262\n",
      "epoch: 5, loss: 0.2663369642710394\n",
      "epoch: 6, loss: 0.26545295813632885\n",
      "epoch: 7, loss: 0.2646518380334032\n",
      "epoch: 8, loss: 0.2630068545358135\n",
      "epoch: 9, loss: 0.26307037904416497\n",
      "epoch: 10, loss: 0.2620258974155206\n",
      "epoch: 11, loss: 0.2624104671037218\n",
      "epoch: 12, loss: 0.26109575520531253\n",
      "epoch: 13, loss: 0.26057598327674997\n",
      "epoch: 14, loss: 0.2600932569872022\n",
      "epoch: 15, loss: 0.26011074991022315\n",
      "epoch: 16, loss: 0.26009848971329436\n",
      "epoch: 17, loss: 0.2596992101604818\n",
      "epoch: 18, loss: 0.25850501008994914\n",
      "epoch: 19, loss: 0.2586283255986102\n",
      "epoch: 20, loss: 0.25792415203432345\n",
      "epoch: 21, loss: 0.25820063943950294\n",
      "epoch: 22, loss: 0.25760537068673245\n",
      "epoch: 23, loss: 0.2570302409703403\n",
      "epoch: 24, loss: 0.2573214214145617\n",
      "epoch: 25, loss: 0.25632154031365745\n",
      "epoch: 26, loss: 0.25598257366587357\n",
      "epoch: 27, loss: 0.2564126685859853\n",
      "epoch: 28, loss: 0.25563839551249934\n",
      "epoch: 29, loss: 0.25595125722427436\n",
      "epoch: 30, loss: 0.2552145148967157\n",
      "epoch: 31, loss: 0.25530062022945643\n",
      "epoch: 32, loss: 0.25550006930740715\n",
      "epoch: 33, loss: 0.2551625910073675\n",
      "epoch: 34, loss: 0.2548210822162828\n",
      "epoch: 35, loss: 0.2547294739601292\n",
      "epoch: 36, loss: 0.25470200415057037\n",
      "epoch: 37, loss: 0.2544714599119639\n",
      "epoch: 38, loss: 0.2547252507682038\n",
      "epoch: 39, loss: 0.2538816516416027\n",
      "epoch: 40, loss: 0.25406603824613816\n",
      "500101093 done\n",
      "get data loader for 500101185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-26 19:23:39,112] A new study created in RDB with name: 500101185\n",
      "[I 2023-12-26 19:25:19,664] Trial 0 finished with value: 5411.51123046875 and parameters: {'hidden_size': 448, 'learning_rate': 0.00021643027532548698, 'dropout_rate': 0.28467641559346507}. Best is trial 0 with value: 5411.51123046875.\n",
      "[I 2023-12-26 19:37:06,425] Trial 1 finished with value: 5547.64111328125 and parameters: {'hidden_size': 1314, 'learning_rate': 0.006136846340428064, 'dropout_rate': 0.3439467141570499}. Best is trial 0 with value: 5411.51123046875.\n",
      "[I 2023-12-26 19:37:29,901] Trial 2 finished with value: 5423.169921875 and parameters: {'hidden_size': 13, 'learning_rate': 0.004979166469928971, 'dropout_rate': 0.357469929369704}. Best is trial 0 with value: 5411.51123046875.\n",
      "[I 2023-12-26 19:58:13,008] Trial 3 finished with value: 5708.29833984375 and parameters: {'hidden_size': 1945, 'learning_rate': 0.005702159720091305, 'dropout_rate': 0.2497551662039668}. Best is trial 0 with value: 5411.51123046875.\n",
      "[I 2023-12-26 19:58:36,581] Trial 4 finished with value: 5732.0087890625 and parameters: {'hidden_size': 7, 'learning_rate': 0.0006437128974149494, 'dropout_rate': 0.4819139928425887}. Best is trial 0 with value: 5411.51123046875.\n",
      "[I 2023-12-26 19:58:37,135] Trial 5 pruned. \n",
      "[I 2023-12-26 19:59:57,473] Trial 6 finished with value: 5286.84228515625 and parameters: {'hidden_size': 356, 'learning_rate': 0.0007141888333661958, 'dropout_rate': 0.29781193900891495}. Best is trial 6 with value: 5286.84228515625.\n",
      "[I 2023-12-26 19:59:58,064] Trial 7 pruned. \n",
      "[I 2023-12-26 19:59:59,485] Trial 8 pruned. \n",
      "[I 2023-12-26 20:00:00,072] Trial 9 pruned. \n",
      "[I 2023-12-26 20:00:00,936] Trial 10 pruned. \n",
      "[I 2023-12-26 20:01:25,335] Trial 11 finished with value: 5291.0615234375 and parameters: {'hidden_size': 379, 'learning_rate': 0.00031230441537232455, 'dropout_rate': 0.2963966693816283}. Best is trial 6 with value: 5286.84228515625.\n",
      "[I 2023-12-26 20:02:13,996] Trial 12 finished with value: 5368.9521484375 and parameters: {'hidden_size': 220, 'learning_rate': 0.0006125289145192212, 'dropout_rate': 0.32755770229036196}. Best is trial 6 with value: 5286.84228515625.\n",
      "[I 2023-12-26 20:02:17,890] Trial 13 pruned. \n",
      "[I 2023-12-26 20:02:18,683] Trial 14 pruned. \n",
      "[I 2023-12-26 20:02:56,049] Trial 15 finished with value: 5300.4951171875 and parameters: {'hidden_size': 170, 'learning_rate': 0.0008822125185124607, 'dropout_rate': 0.26434247109992287}. Best is trial 6 with value: 5286.84228515625.\n",
      "[I 2023-12-26 20:03:15,796] Trial 16 pruned. \n",
      "[I 2023-12-26 20:03:16,394] Trial 17 pruned. \n",
      "[I 2023-12-26 20:03:38,331] Trial 18 pruned. \n",
      "[I 2023-12-26 20:03:52,161] Trial 19 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500101185 eval_loss: 5286.84228515625\n",
      "500101185  Best hyperparameters: {'hidden_size': 356, 'learning_rate': 0.0007141888333661958, 'dropout_rate': 0.29781193900891495}\n",
      "epoch: 1, loss: 0.49211898176096874\n",
      "epoch: 2, loss: 0.42760113538778305\n",
      "epoch: 3, loss: 0.41101833502031887\n",
      "epoch: 4, loss: 0.40030435186515495\n",
      "epoch: 5, loss: 0.3943195031073191\n",
      "epoch: 6, loss: 0.3921872557170092\n",
      "epoch: 7, loss: 0.3887003171565952\n",
      "epoch: 8, loss: 0.38619846122125195\n",
      "epoch: 9, loss: 0.3845036200893465\n",
      "epoch: 10, loss: 0.38287030204883954\n",
      "epoch: 11, loss: 0.38216417090714183\n",
      "epoch: 12, loss: 0.37995166318062756\n",
      "epoch: 13, loss: 0.38031642250019004\n",
      "epoch: 14, loss: 0.3804293324723061\n",
      "epoch: 15, loss: 0.37918043179431027\n",
      "epoch: 16, loss: 0.3782175440561896\n",
      "epoch: 17, loss: 0.37907421673755143\n",
      "epoch: 18, loss: 0.3779723625110936\n",
      "epoch: 19, loss: 0.3780131310310526\n",
      "epoch: 20, loss: 0.37671382761907596\n",
      "epoch: 21, loss: 0.3768592914144544\n",
      "epoch: 22, loss: 0.3766661037136038\n",
      "epoch: 23, loss: 0.3760965903588159\n",
      "epoch: 24, loss: 0.37570708081489657\n",
      "epoch: 25, loss: 0.375570355895063\n",
      "epoch: 26, loss: 0.3756112512201071\n",
      "epoch: 27, loss: 0.3743773460427124\n",
      "epoch: 28, loss: 0.37551199846969147\n",
      "epoch: 29, loss: 0.37438236957071963\n",
      "epoch: 30, loss: 0.374621623585111\n",
      "epoch: 31, loss: 0.37512499531925714\n",
      "epoch: 32, loss: 0.37517146680517066\n",
      "epoch: 33, loss: 0.3742715302750936\n",
      "epoch: 34, loss: 0.37417645707324093\n",
      "epoch: 35, loss: 0.3738835948431793\n",
      "epoch: 36, loss: 0.37420537489553984\n",
      "epoch: 37, loss: 0.3742182824342067\n",
      "epoch: 38, loss: 0.3744230234333499\n",
      "epoch: 39, loss: 0.3737126530816564\n",
      "epoch: 40, loss: 0.3734081436130332\n",
      "500101185 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-26 20:05:55,228] A new study created in RDB with name: 500105066\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get data loader for 500105066\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-26 20:23:47,467] Trial 0 finished with value: 5372.40087890625 and parameters: {'hidden_size': 1816, 'learning_rate': 3.464900453879686e-05, 'dropout_rate': 0.38971716616633234}. Best is trial 0 with value: 5372.40087890625.\n",
      "[I 2023-12-26 20:24:31,173] Trial 1 finished with value: 5387.50732421875 and parameters: {'hidden_size': 191, 'learning_rate': 0.00033342428600051437, 'dropout_rate': 0.27140469273723855}. Best is trial 0 with value: 5372.40087890625.\n",
      "[I 2023-12-26 20:24:53,656] Trial 2 finished with value: 6927.419921875 and parameters: {'hidden_size': 3, 'learning_rate': 8.159676724467907e-05, 'dropout_rate': 0.22222633480735418}. Best is trial 0 with value: 5372.40087890625.\n",
      "[I 2023-12-26 20:44:19,169] Trial 3 finished with value: 5659.07568359375 and parameters: {'hidden_size': 1900, 'learning_rate': 0.0034217801735884693, 'dropout_rate': 0.26811155431047906}. Best is trial 0 with value: 5372.40087890625.\n",
      "[I 2023-12-26 20:44:43,911] Trial 4 finished with value: 7050.54931640625 and parameters: {'hidden_size': 7, 'learning_rate': 0.012802816476974621, 'dropout_rate': 0.47339969015688366}. Best is trial 0 with value: 5372.40087890625.\n",
      "[I 2023-12-26 20:44:44,587] Trial 5 pruned. \n",
      "[I 2023-12-26 20:44:45,238] Trial 6 pruned. \n",
      "[I 2023-12-26 20:44:46,082] Trial 7 pruned. \n",
      "[I 2023-12-26 20:44:46,863] Trial 8 pruned. \n",
      "[I 2023-12-26 20:44:47,533] Trial 9 pruned. \n",
      "[I 2023-12-26 20:45:19,498] Trial 10 pruned. \n",
      "[I 2023-12-26 20:46:32,753] Trial 11 finished with value: 5397.7216796875 and parameters: {'hidden_size': 333, 'learning_rate': 0.00018453971976187822, 'dropout_rate': 0.37663073235479216}. Best is trial 0 with value: 5372.40087890625.\n",
      "[I 2023-12-26 20:47:13,240] Trial 12 finished with value: 5370.21337890625 and parameters: {'hidden_size': 224, 'learning_rate': 0.00022976864738344293, 'dropout_rate': 0.20056603384926994}. Best is trial 12 with value: 5370.21337890625.\n",
      "[I 2023-12-26 20:49:05,815] Trial 13 finished with value: 5416.79541015625 and parameters: {'hidden_size': 479, 'learning_rate': 0.00027493408604121187, 'dropout_rate': 0.21784084911202364}. Best is trial 12 with value: 5370.21337890625.\n",
      "[I 2023-12-26 20:49:06,584] Trial 14 pruned. \n",
      "[I 2023-12-26 20:49:18,329] Trial 15 pruned. \n",
      "[I 2023-12-26 20:49:43,733] Trial 16 pruned. \n",
      "[I 2023-12-26 20:49:45,120] Trial 17 pruned. \n",
      "[I 2023-12-26 20:49:46,110] Trial 18 pruned. \n",
      "[I 2023-12-26 20:52:25,111] Trial 19 finished with value: 5401.55419921875 and parameters: {'hidden_size': 579, 'learning_rate': 0.00012351247655200838, 'dropout_rate': 0.3132255304849531}. Best is trial 12 with value: 5370.21337890625.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500105066 eval_loss: 5370.21337890625\n",
      "500105066  Best hyperparameters: {'hidden_size': 224, 'learning_rate': 0.00022976864738344293, 'dropout_rate': 0.20056603384926994}\n",
      "epoch: 1, loss: 0.43416018927076516\n",
      "epoch: 2, loss: 0.40372910133415074\n",
      "epoch: 3, loss: 0.39651932568658294\n",
      "epoch: 4, loss: 0.39380427170382326\n",
      "epoch: 5, loss: 0.39242274869382904\n",
      "epoch: 6, loss: 0.3896974433883114\n",
      "epoch: 7, loss: 0.3877331573405607\n",
      "epoch: 8, loss: 0.3869443267947924\n",
      "epoch: 9, loss: 0.38567898925686384\n",
      "epoch: 10, loss: 0.3845668516233954\n",
      "epoch: 11, loss: 0.38306998353562016\n",
      "epoch: 12, loss: 0.38292053230041817\n",
      "epoch: 13, loss: 0.3827641074257995\n",
      "epoch: 14, loss: 0.38165297386533925\n",
      "epoch: 15, loss: 0.3803284428298577\n",
      "epoch: 16, loss: 0.38059912374922533\n",
      "epoch: 17, loss: 0.37957696229167426\n",
      "epoch: 18, loss: 0.3794911571406153\n",
      "epoch: 19, loss: 0.37870571792229724\n",
      "epoch: 20, loss: 0.3784543720527469\n",
      "epoch: 21, loss: 0.3786995183630972\n",
      "epoch: 22, loss: 0.37834811720340455\n",
      "epoch: 23, loss: 0.37792703046432546\n",
      "epoch: 24, loss: 0.37737437739422186\n",
      "epoch: 25, loss: 0.3775608676630373\n",
      "epoch: 26, loss: 0.37697988639327246\n",
      "epoch: 27, loss: 0.3770848000340853\n",
      "epoch: 28, loss: 0.37662346284010856\n",
      "epoch: 29, loss: 0.3770304628587726\n",
      "epoch: 30, loss: 0.37601996588977427\n",
      "epoch: 31, loss: 0.3763830845372631\n",
      "epoch: 32, loss: 0.37652155044399216\n",
      "epoch: 33, loss: 0.3756579276154803\n",
      "epoch: 34, loss: 0.37592804322692114\n",
      "epoch: 35, loss: 0.37572981955493306\n",
      "epoch: 36, loss: 0.37521340834532735\n",
      "epoch: 37, loss: 0.3757722468796409\n",
      "epoch: 38, loss: 0.37495328972268893\n",
      "epoch: 39, loss: 0.3750012329735681\n",
      "epoch: 40, loss: 0.37469805654846977\n",
      "500105066 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-26 20:53:32,726] A new study created in RDB with name: 500119049\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get data loader for 500119049\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-26 20:54:32,255] Trial 0 finished with value: 3856.9951171875 and parameters: {'hidden_size': 269, 'learning_rate': 0.02014127971993433, 'dropout_rate': 0.4810607491603768}. Best is trial 0 with value: 3856.9951171875.\n",
      "[I 2023-12-26 21:09:29,519] Trial 1 finished with value: 2638.207763671875 and parameters: {'hidden_size': 1660, 'learning_rate': 0.0005612105313750439, 'dropout_rate': 0.49949767185673716}. Best is trial 1 with value: 2638.207763671875.\n",
      "[I 2023-12-26 21:09:53,918] Trial 2 finished with value: 3867.568603515625 and parameters: {'hidden_size': 7, 'learning_rate': 0.06727030437179327, 'dropout_rate': 0.4712511193334432}. Best is trial 1 with value: 2638.207763671875.\n",
      "[I 2023-12-26 21:12:55,155] Trial 3 finished with value: 2696.082275390625 and parameters: {'hidden_size': 558, 'learning_rate': 4.250128470478352e-05, 'dropout_rate': 0.2684673869160325}. Best is trial 1 with value: 2638.207763671875.\n",
      "[I 2023-12-26 21:13:18,766] Trial 4 finished with value: 3870.738525390625 and parameters: {'hidden_size': 3, 'learning_rate': 0.02577944599899999, 'dropout_rate': 0.23376926355962419}. Best is trial 1 with value: 2638.207763671875.\n",
      "[I 2023-12-26 21:13:24,156] Trial 5 pruned. \n",
      "[I 2023-12-26 21:13:24,813] Trial 6 pruned. \n",
      "[I 2023-12-26 21:15:34,638] Trial 7 finished with value: 2631.968994140625 and parameters: {'hidden_size': 373, 'learning_rate': 0.0005951517090968878, 'dropout_rate': 0.497688116287083}. Best is trial 7 with value: 2631.968994140625.\n",
      "[I 2023-12-26 21:15:35,262] Trial 8 pruned. \n",
      "[I 2023-12-26 21:15:35,931] Trial 9 pruned. \n",
      "[I 2023-12-26 21:15:37,121] Trial 10 pruned. \n",
      "[I 2023-12-26 21:26:03,684] Trial 11 finished with value: 2713.35595703125 and parameters: {'hidden_size': 1888, 'learning_rate': 0.0014212756236180635, 'dropout_rate': 0.41103759712443516}. Best is trial 7 with value: 2631.968994140625.\n",
      "[I 2023-12-26 21:37:27,738] Trial 12 finished with value: 2699.02392578125 and parameters: {'hidden_size': 1944, 'learning_rate': 0.0012881426937428138, 'dropout_rate': 0.49812536020645637}. Best is trial 7 with value: 2631.968994140625.\n",
      "[I 2023-12-26 21:37:50,039] Trial 13 finished with value: 2640.934326171875 and parameters: {'hidden_size': 178, 'learning_rate': 0.0031107261374649447, 'dropout_rate': 0.42820892902274355}. Best is trial 7 with value: 2631.968994140625.\n",
      "[I 2023-12-26 21:39:56,711] Trial 14 finished with value: 2622.057861328125 and parameters: {'hidden_size': 832, 'learning_rate': 0.0004234918036611038, 'dropout_rate': 0.36964383316497396}. Best is trial 14 with value: 2622.057861328125.\n",
      "[I 2023-12-26 21:41:20,862] Trial 15 finished with value: 2634.2041015625 and parameters: {'hidden_size': 625, 'learning_rate': 0.00022215528993092546, 'dropout_rate': 0.3652179530880037}. Best is trial 14 with value: 2622.057861328125.\n",
      "[I 2023-12-26 21:41:24,287] Trial 16 pruned. \n",
      "[I 2023-12-26 21:41:25,047] Trial 17 pruned. \n",
      "[I 2023-12-26 21:41:29,750] Trial 18 pruned. \n",
      "[I 2023-12-26 21:42:05,213] Trial 19 finished with value: 2615.859130859375 and parameters: {'hidden_size': 297, 'learning_rate': 0.0005197506364937425, 'dropout_rate': 0.3797447979760953}. Best is trial 19 with value: 2615.859130859375.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500119049 eval_loss: 2615.859130859375\n",
      "500119049  Best hyperparameters: {'hidden_size': 297, 'learning_rate': 0.0005197506364937425, 'dropout_rate': 0.3797447979760953}\n",
      "epoch: 1, loss: 0.25241867350138086\n",
      "epoch: 2, loss: 0.21239173992246024\n",
      "epoch: 3, loss: 0.20525210419666495\n",
      "epoch: 4, loss: 0.20294932235701962\n",
      "epoch: 5, loss: 0.20084722940834404\n",
      "epoch: 6, loss: 0.20032481633347782\n",
      "epoch: 7, loss: 0.19914500631170956\n",
      "epoch: 8, loss: 0.19847829273233863\n",
      "epoch: 9, loss: 0.19790417221204148\n",
      "epoch: 10, loss: 0.19736853494173986\n",
      "epoch: 11, loss: 0.19709895114311998\n",
      "epoch: 12, loss: 0.19657270386685044\n",
      "epoch: 13, loss: 0.19607787934273324\n",
      "epoch: 14, loss: 0.19582438211478487\n",
      "epoch: 15, loss: 0.19492659846525542\n",
      "epoch: 16, loss: 0.19484605862520127\n",
      "epoch: 17, loss: 0.1948123254389039\n",
      "epoch: 18, loss: 0.19369733747075366\n",
      "epoch: 19, loss: 0.19290658264676105\n",
      "epoch: 20, loss: 0.19220689721860604\n",
      "epoch: 21, loss: 0.19231758356406425\n",
      "epoch: 22, loss: 0.1918097880602299\n",
      "epoch: 23, loss: 0.1918498385847135\n",
      "epoch: 24, loss: 0.19131932073446675\n",
      "epoch: 25, loss: 0.19144442204197457\n",
      "epoch: 26, loss: 0.19120234230217925\n",
      "epoch: 27, loss: 0.19088923192565146\n",
      "epoch: 28, loss: 0.1907423560948064\n",
      "epoch: 29, loss: 0.19062622562440903\n",
      "epoch: 30, loss: 0.19054727037851724\n",
      "epoch: 31, loss: 0.19081450195212638\n",
      "epoch: 32, loss: 0.19048362744104175\n",
      "epoch: 33, loss: 0.19057201650026165\n",
      "epoch: 34, loss: 0.1903562269077667\n",
      "epoch: 35, loss: 0.19022975097464018\n",
      "epoch: 36, loss: 0.1901875230342306\n",
      "epoch: 37, loss: 0.18978004620441383\n",
      "epoch: 38, loss: 0.1899077068968801\n",
      "epoch: 39, loss: 0.18974574627988625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-26 21:42:59,863] A new study created in RDB with name: 500119059\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 40, loss: 0.19018602287998165\n",
      "500119049 done\n",
      "get data loader for 500119059\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-26 21:43:14,244] Trial 0 finished with value: 2787.516357421875 and parameters: {'hidden_size': 29, 'learning_rate': 2.217273506449726e-05, 'dropout_rate': 0.20187659603249003}. Best is trial 0 with value: 2787.516357421875.\n",
      "[I 2023-12-26 21:47:18,542] Trial 1 finished with value: 2787.76904296875 and parameters: {'hidden_size': 1122, 'learning_rate': 4.6977218053717444e-05, 'dropout_rate': 0.2958066511903811}. Best is trial 0 with value: 2787.516357421875.\n",
      "[I 2023-12-26 21:47:46,678] Trial 2 finished with value: 2919.34814453125 and parameters: {'hidden_size': 243, 'learning_rate': 0.018338879940911965, 'dropout_rate': 0.43054028653519594}. Best is trial 0 with value: 2787.516357421875.\n",
      "[I 2023-12-26 21:48:02,201] Trial 3 finished with value: 2787.47314453125 and parameters: {'hidden_size': 52, 'learning_rate': 1.4426729513064302e-05, 'dropout_rate': 0.44865381760510564}. Best is trial 3 with value: 2787.47314453125.\n",
      "[I 2023-12-26 21:48:16,572] Trial 4 finished with value: 2788.461669921875 and parameters: {'hidden_size': 35, 'learning_rate': 2.448231087737894e-05, 'dropout_rate': 0.44389613425603475}. Best is trial 3 with value: 2787.47314453125.\n",
      "[I 2023-12-26 21:48:31,351] Trial 5 pruned. \n",
      "[I 2023-12-26 21:48:32,036] Trial 6 pruned. \n",
      "[I 2023-12-26 21:48:35,119] Trial 7 pruned. \n",
      "[I 2023-12-26 21:53:40,442] Trial 8 finished with value: 2789.635986328125 and parameters: {'hidden_size': 1276, 'learning_rate': 0.00018622881587450221, 'dropout_rate': 0.2621497604056816}. Best is trial 3 with value: 2787.47314453125.\n",
      "[I 2023-12-26 21:53:40,863] Trial 9 pruned. \n",
      "[I 2023-12-26 21:53:43,069] Trial 10 pruned. \n",
      "[I 2023-12-26 21:53:43,467] Trial 11 pruned. \n",
      "[I 2023-12-26 21:54:00,346] Trial 12 finished with value: 2788.296630859375 and parameters: {'hidden_size': 84, 'learning_rate': 1.2706357798349639e-05, 'dropout_rate': 0.20118651644227917}. Best is trial 3 with value: 2787.47314453125.\n",
      "[I 2023-12-26 21:54:00,740] Trial 13 pruned. \n",
      "[I 2023-12-26 21:54:01,269] Trial 14 pruned. \n",
      "[I 2023-12-26 21:54:01,638] Trial 15 pruned. \n",
      "[I 2023-12-26 21:54:03,889] Trial 16 pruned. \n",
      "[I 2023-12-26 21:54:04,644] Trial 17 pruned. \n",
      "[I 2023-12-26 21:54:14,423] Trial 18 pruned. \n",
      "[I 2023-12-26 21:54:27,787] Trial 19 finished with value: 2788.64453125 and parameters: {'hidden_size': 7, 'learning_rate': 0.00041177868539142814, 'dropout_rate': 0.2521251384556469}. Best is trial 3 with value: 2787.47314453125.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500119059 eval_loss: 2787.47314453125\n",
      "500119059  Best hyperparameters: {'hidden_size': 52, 'learning_rate': 1.4426729513064302e-05, 'dropout_rate': 0.44865381760510564}\n",
      "epoch: 1, loss: 0.3038349588243957\n",
      "epoch: 2, loss: 0.2715599414149712\n",
      "epoch: 3, loss: 0.2586920060880521\n",
      "epoch: 4, loss: 0.24863527130082017\n",
      "epoch: 5, loss: 0.24001203542396451\n",
      "epoch: 6, loss: 0.23319167080766035\n",
      "epoch: 7, loss: 0.2271730578382602\n",
      "epoch: 8, loss: 0.22224681152203113\n",
      "epoch: 9, loss: 0.2181908471890145\n",
      "epoch: 10, loss: 0.21479944341261767\n",
      "epoch: 11, loss: 0.21143036690682016\n",
      "epoch: 12, loss: 0.20855448514639188\n",
      "epoch: 13, loss: 0.20576747824124225\n",
      "epoch: 14, loss: 0.20320563918763015\n",
      "epoch: 15, loss: 0.20080071873637811\n",
      "epoch: 16, loss: 0.19857599023228956\n",
      "epoch: 17, loss: 0.19651231090383797\n",
      "epoch: 18, loss: 0.1948678814277807\n",
      "epoch: 19, loss: 0.1937041381799842\n",
      "epoch: 20, loss: 0.19312238188730277\n",
      "epoch: 21, loss: 0.19288365252927128\n",
      "epoch: 22, loss: 0.1927760601641828\n",
      "epoch: 23, loss: 0.192698997372264\n",
      "epoch: 24, loss: 0.19266285188171045\n",
      "epoch: 25, loss: 0.19263101609693564\n",
      "epoch: 26, loss: 0.1926142728739592\n",
      "epoch: 27, loss: 0.19260584170333586\n",
      "epoch: 28, loss: 0.1925982240502955\n",
      "epoch: 29, loss: 0.192593428034849\n",
      "epoch: 30, loss: 0.19259088265386134\n",
      "epoch: 31, loss: 0.19258750500531305\n",
      "epoch: 32, loss: 0.1925864602856819\n",
      "epoch: 33, loss: 0.19258590966437084\n",
      "epoch: 34, loss: 0.19258536680570656\n",
      "epoch: 35, loss: 0.19258381422433554\n",
      "epoch: 36, loss: 0.1925834311110603\n",
      "epoch: 37, loss: 0.1925834666996102\n",
      "epoch: 38, loss: 0.1925826907209909\n",
      "epoch: 39, loss: 0.19258279512013857\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-26 21:54:47,795] A new study created in RDB with name: 500119069\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 40, loss: 0.1925825920560597\n",
      "500119059 done\n",
      "get data loader for 500119069\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-26 21:55:04,756] Trial 0 finished with value: 3561.4873046875 and parameters: {'hidden_size': 89, 'learning_rate': 0.011689462645289144, 'dropout_rate': 0.46263108707921474}. Best is trial 0 with value: 3561.4873046875.\n",
      "[I 2023-12-26 21:55:18,548] Trial 1 finished with value: 3495.05029296875 and parameters: {'hidden_size': 15, 'learning_rate': 0.00035194664380895754, 'dropout_rate': 0.4464779931275333}. Best is trial 1 with value: 3495.05029296875.\n",
      "[I 2023-12-26 21:55:37,583] Trial 2 finished with value: 3043.6611328125 and parameters: {'hidden_size': 112, 'learning_rate': 0.00032724523442694426, 'dropout_rate': 0.23501656413477892}. Best is trial 2 with value: 3043.6611328125.\n",
      "[I 2023-12-26 21:55:50,936] Trial 3 finished with value: 8668.1396484375 and parameters: {'hidden_size': 5, 'learning_rate': 1.4960488834924383e-05, 'dropout_rate': 0.21033097123646827}. Best is trial 2 with value: 3043.6611328125.\n",
      "[I 2023-12-26 21:56:04,247] Trial 4 finished with value: 4744.9013671875 and parameters: {'hidden_size': 6, 'learning_rate': 0.0001295031012543588, 'dropout_rate': 0.3804697440440317}. Best is trial 2 with value: 3043.6611328125.\n",
      "[I 2023-12-26 21:56:05,601] Trial 5 pruned. \n",
      "[I 2023-12-26 22:03:28,037] Trial 6 finished with value: 3239.2744140625 and parameters: {'hidden_size': 1559, 'learning_rate': 1.0693874131205424e-05, 'dropout_rate': 0.4785552854216309}. Best is trial 2 with value: 3043.6611328125.\n",
      "[I 2023-12-26 22:03:28,402] Trial 7 pruned. \n",
      "[I 2023-12-26 22:13:47,082] Trial 8 finished with value: 3330.64892578125 and parameters: {'hidden_size': 1837, 'learning_rate': 0.0009450134150731394, 'dropout_rate': 0.44083154196074736}. Best is trial 2 with value: 3043.6611328125.\n",
      "[I 2023-12-26 22:13:47,448] Trial 9 pruned. \n",
      "[I 2023-12-26 22:14:08,895] Trial 10 finished with value: 3198.362548828125 and parameters: {'hidden_size': 176, 'learning_rate': 8.459408703082541e-05, 'dropout_rate': 0.20102030498279475}. Best is trial 2 with value: 3043.6611328125.\n",
      "[I 2023-12-26 22:14:29,982] Trial 11 finished with value: 3147.3740234375 and parameters: {'hidden_size': 154, 'learning_rate': 0.00011455827186483239, 'dropout_rate': 0.20297685239970456}. Best is trial 2 with value: 3043.6611328125.\n",
      "[I 2023-12-26 22:14:30,924] Trial 12 pruned. \n",
      "[I 2023-12-26 22:14:31,344] Trial 13 pruned. \n",
      "[I 2023-12-26 22:14:32,788] Trial 14 pruned. \n",
      "[I 2023-12-26 22:14:33,300] Trial 15 pruned. \n",
      "[I 2023-12-26 22:14:36,670] Trial 16 pruned. \n",
      "[I 2023-12-26 22:14:54,476] Trial 17 finished with value: 3156.01806640625 and parameters: {'hidden_size': 100, 'learning_rate': 0.0001667216754777282, 'dropout_rate': 0.20151793313427357}. Best is trial 2 with value: 3043.6611328125.\n",
      "[I 2023-12-26 22:15:09,021] Trial 18 finished with value: 2984.7578125 and parameters: {'hidden_size': 40, 'learning_rate': 0.0007622312276149493, 'dropout_rate': 0.24005496758210995}. Best is trial 18 with value: 2984.7578125.\n",
      "[I 2023-12-26 22:15:23,406] Trial 19 finished with value: 3097.512939453125 and parameters: {'hidden_size': 26, 'learning_rate': 0.002288054083727242, 'dropout_rate': 0.29462952206033516}. Best is trial 18 with value: 2984.7578125.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500119069 eval_loss: 2984.7578125\n",
      "500119069  Best hyperparameters: {'hidden_size': 40, 'learning_rate': 0.0007622312276149493, 'dropout_rate': 0.24005496758210995}\n",
      "epoch: 1, loss: 0.5901406243001396\n",
      "epoch: 2, loss: 0.3732312958017902\n",
      "epoch: 3, loss: 0.30701625458961174\n",
      "epoch: 4, loss: 0.29122329512399736\n",
      "epoch: 5, loss: 0.2859581648575282\n",
      "epoch: 6, loss: 0.28360406159834506\n",
      "epoch: 7, loss: 0.2810295885249998\n",
      "epoch: 8, loss: 0.2792215577698919\n",
      "epoch: 9, loss: 0.27695947516234964\n",
      "epoch: 10, loss: 0.27457013125506996\n",
      "epoch: 11, loss: 0.27283629052093933\n",
      "epoch: 12, loss: 0.26886162700141286\n",
      "epoch: 13, loss: 0.266990836456184\n",
      "epoch: 14, loss: 0.2646002691713719\n",
      "epoch: 15, loss: 0.26088114857777667\n",
      "epoch: 16, loss: 0.25931775694742254\n",
      "epoch: 17, loss: 0.25738622471820205\n",
      "epoch: 18, loss: 0.25520056543341896\n",
      "epoch: 19, loss: 0.25501558301648547\n",
      "epoch: 20, loss: 0.2532317280821359\n",
      "epoch: 21, loss: 0.2545983365380951\n",
      "epoch: 22, loss: 0.2525601695709412\n",
      "epoch: 23, loss: 0.25487227137176155\n",
      "epoch: 24, loss: 0.25415686253997877\n",
      "epoch: 25, loss: 0.2518712983289522\n",
      "epoch: 26, loss: 0.25332637060999247\n",
      "epoch: 27, loss: 0.2520236149040192\n",
      "epoch: 28, loss: 0.25065400489129736\n",
      "epoch: 29, loss: 0.2531152790441563\n",
      "epoch: 30, loss: 0.2511928620606817\n",
      "epoch: 31, loss: 0.25101729962214125\n",
      "epoch: 32, loss: 0.25043706260426507\n",
      "epoch: 33, loss: 0.2506639300008511\n",
      "epoch: 34, loss: 0.25143763441169864\n",
      "epoch: 35, loss: 0.2503345266218585\n",
      "epoch: 36, loss: 0.25117495016277774\n",
      "epoch: 37, loss: 0.2497110558742004\n",
      "epoch: 38, loss: 0.2506740036019064\n",
      "epoch: 39, loss: 0.25141120191019867\n",
      "epoch: 40, loss: 0.2513646396562899\n",
      "500119069 done\n"
     ]
    }
   ],
   "source": [
    "for station_id in available_station:\n",
    "    station = station_model(station_id)\n",
    "    station.get_data_loader()\n",
    "    station.get_best_param()\n",
    "    station.train()\n",
    "    print(f'{station_id} done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-26 22:15:41,357] Using an existing study with name '500101185' instead of creating a new one.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[500101185, 500105066]\n",
      "get data loader for 500101185\n",
      "500101185 eval_loss: 5286.84228515625\n",
      "500101185  Best hyperparameters: {'hidden_size': 356, 'learning_rate': 0.0007141888333661958, 'dropout_rate': 0.29781193900891495}\n",
      "epoch: 1, loss: 0.49009636689613506\n",
      "epoch: 2, loss: 0.4264851840673735\n",
      "epoch: 3, loss: 0.40807564536154994\n",
      "epoch: 4, loss: 0.3988940385283857\n",
      "epoch: 5, loss: 0.3935745941874172\n",
      "epoch: 6, loss: 0.3898067466252668\n",
      "epoch: 7, loss: 0.387160284473895\n",
      "epoch: 8, loss: 0.38495025503406954\n",
      "epoch: 9, loss: 0.3824755400759963\n",
      "epoch: 10, loss: 0.38162313460434044\n",
      "epoch: 11, loss: 0.3806985300099185\n",
      "epoch: 12, loss: 0.37921132009398545\n",
      "epoch: 13, loss: 0.3788986421005236\n",
      "epoch: 14, loss: 0.3792520542629063\n",
      "epoch: 15, loss: 0.3771976936602063\n",
      "epoch: 16, loss: 0.3771499465381928\n",
      "epoch: 17, loss: 0.377403168512333\n",
      "epoch: 18, loss: 0.3766448287072653\n",
      "epoch: 19, loss: 0.37613593387167626\n",
      "epoch: 20, loss: 0.3776716471581561\n",
      "epoch: 21, loss: 0.37592414139610963\n",
      "epoch: 22, loss: 0.37711072858189354\n",
      "epoch: 23, loss: 0.3756858914788773\n",
      "epoch: 24, loss: 0.3749210535376641\n",
      "epoch: 25, loss: 0.37422125825969965\n",
      "epoch: 26, loss: 0.3755835060213566\n",
      "epoch: 27, loss: 0.3737448405519855\n",
      "epoch: 28, loss: 0.374030539022843\n",
      "epoch: 29, loss: 0.3749258885783134\n",
      "epoch: 30, loss: 0.37447465081514225\n",
      "epoch: 31, loss: 0.3739667925464463\n",
      "epoch: 32, loss: 0.3734499375929712\n",
      "epoch: 33, loss: 0.37387094842854124\n",
      "epoch: 34, loss: 0.3731749292450084\n",
      "epoch: 35, loss: 0.3730345183402585\n",
      "epoch: 36, loss: 0.3736583778556787\n",
      "epoch: 37, loss: 0.373127791490276\n",
      "epoch: 38, loss: 0.37385772603145356\n",
      "epoch: 39, loss: 0.37270427450784305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-26 22:16:42,487] Using an existing study with name '500105066' instead of creating a new one.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 40, loss: 0.37378710716677255\n",
      "train <__main__.station_model object at 0x168e6ea90> done!\n",
      "get data loader for 500105066\n",
      "500105066 eval_loss: 5370.21337890625\n",
      "500105066  Best hyperparameters: {'hidden_size': 224, 'learning_rate': 0.00022976864738344293, 'dropout_rate': 0.20056603384926994}\n",
      "epoch: 1, loss: 0.43624664471203534\n",
      "epoch: 2, loss: 0.403627388244316\n",
      "epoch: 3, loss: 0.3988242933159307\n",
      "epoch: 4, loss: 0.39586047071436936\n",
      "epoch: 5, loss: 0.39096930966327326\n",
      "epoch: 6, loss: 0.39025656943962006\n",
      "epoch: 7, loss: 0.3876387327129304\n",
      "epoch: 8, loss: 0.38575300579503774\n",
      "epoch: 9, loss: 0.3845306245860421\n",
      "epoch: 10, loss: 0.38363710893594366\n",
      "epoch: 11, loss: 0.383116187797583\n",
      "epoch: 12, loss: 0.38275671957050944\n",
      "epoch: 13, loss: 0.3817312025690994\n",
      "epoch: 14, loss: 0.3801361272992472\n",
      "epoch: 15, loss: 0.3795787002923809\n",
      "epoch: 16, loss: 0.3797081894067362\n",
      "epoch: 17, loss: 0.3787905340835478\n",
      "epoch: 18, loss: 0.37868835050190097\n",
      "epoch: 19, loss: 0.37846252735684677\n",
      "epoch: 20, loss: 0.3776147579855528\n",
      "epoch: 21, loss: 0.3782380082324329\n",
      "epoch: 22, loss: 0.3772338917690214\n",
      "epoch: 23, loss: 0.3774331305663623\n",
      "epoch: 24, loss: 0.3770790488620079\n",
      "epoch: 25, loss: 0.37649599269006473\n",
      "epoch: 26, loss: 0.3765994867937311\n",
      "epoch: 27, loss: 0.3767533830999288\n",
      "epoch: 28, loss: 0.37579585312756775\n",
      "epoch: 29, loss: 0.3757979439711279\n",
      "epoch: 30, loss: 0.37620878484860765\n",
      "epoch: 31, loss: 0.37513044304872684\n",
      "epoch: 32, loss: 0.3754259882068967\n",
      "epoch: 33, loss: 0.37562003414460293\n",
      "epoch: 34, loss: 0.375249339387887\n",
      "epoch: 35, loss: 0.37520473451722564\n",
      "epoch: 36, loss: 0.3752954640729681\n",
      "epoch: 37, loss: 0.37508441187948455\n",
      "epoch: 38, loss: 0.3749530701841151\n",
      "epoch: 39, loss: 0.3747770077114954\n",
      "epoch: 40, loss: 0.374578847258919\n",
      "train <__main__.station_model object at 0x13f9c7a50> done!\n"
     ]
    }
   ],
   "source": [
    "# read loss.csv \n",
    "# train those station with loss avg > 0.35\n",
    "loss_df = pd.read_csv('loss.csv')\n",
    "# get station with loss avg > 0.35\n",
    "station_list = []\n",
    "for col in loss_df.columns:\n",
    "    # if last element > 0.35, train this station\n",
    "    if loss_df[col].iloc[-1] > 0.35:\n",
    "        col = float(col)\n",
    "        col = int(col)\n",
    "        station_list.append(col)\n",
    "print(station_list)\n",
    "\n",
    "# train those station\n",
    "for station_id in station_list:\n",
    "    station = station_model(station_id)\n",
    "    station.get_data_loader()\n",
    "    station.get_best_param()\n",
    "    station.train()\n",
    "    print(f'train {station} done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "IsADirectoryError",
     "evalue": "[Errno 21] Is a directory: 'models/'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIsADirectoryError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/yl/HTML/final/train.ipynb Cell 7\u001b[0m line \u001b[0;36m8\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/yl/HTML/final/train.ipynb#X10sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m models \u001b[39m=\u001b[39m []\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/yl/HTML/final/train.ipynb#X10sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mfor\u001b[39;00m file \u001b[39min\u001b[39;00m model_files:\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/yl/HTML/final/train.ipynb#X10sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     model \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mload(file)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/yl/HTML/final/train.ipynb#X10sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     models\u001b[39m.\u001b[39mappend(model)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yl/HTML/final/train.ipynb#X10sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mprint\u001b[39m(models)\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/html-qsiNAWFM-py3.11/lib/python3.11/site-packages/torch/serialization.py:986\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m    983\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m pickle_load_args\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m    984\u001b[0m     pickle_load_args[\u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m--> 986\u001b[0m \u001b[39mwith\u001b[39;00m _open_file_like(f, \u001b[39m'\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m opened_file:\n\u001b[1;32m    987\u001b[0m     \u001b[39mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m    988\u001b[0m         \u001b[39m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m    989\u001b[0m         \u001b[39m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m    990\u001b[0m         \u001b[39m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m    991\u001b[0m         orig_position \u001b[39m=\u001b[39m opened_file\u001b[39m.\u001b[39mtell()\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/html-qsiNAWFM-py3.11/lib/python3.11/site-packages/torch/serialization.py:435\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    434\u001b[0m     \u001b[39mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 435\u001b[0m         \u001b[39mreturn\u001b[39;00m _open_file(name_or_buffer, mode)\n\u001b[1;32m    436\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    437\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m mode:\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/html-qsiNAWFM-py3.11/lib/python3.11/site-packages/torch/serialization.py:416\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    415\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, name, mode):\n\u001b[0;32m--> 416\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39mopen\u001b[39;49m(name, mode))\n",
      "\u001b[0;31mIsADirectoryError\u001b[0m: [Errno 21] Is a directory: 'models/'"
     ]
    }
   ],
   "source": [
    "# get Eout for each station\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>500101001</th>\n",
       "      <th>500101013</th>\n",
       "      <th>500101025</th>\n",
       "      <th>500101035</th>\n",
       "      <th>500101093</th>\n",
       "      <th>500101185</th>\n",
       "      <th>500105066</th>\n",
       "      <th>500119049</th>\n",
       "      <th>500119059</th>\n",
       "      <th>500119069</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.358454</td>\n",
       "      <td>0.496978</td>\n",
       "      <td>0.238550</td>\n",
       "      <td>0.274097</td>\n",
       "      <td>0.287640</td>\n",
       "      <td>0.490096</td>\n",
       "      <td>0.436247</td>\n",
       "      <td>0.252419</td>\n",
       "      <td>0.303835</td>\n",
       "      <td>0.590141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.308635</td>\n",
       "      <td>0.473846</td>\n",
       "      <td>0.181518</td>\n",
       "      <td>0.228091</td>\n",
       "      <td>0.273880</td>\n",
       "      <td>0.426485</td>\n",
       "      <td>0.403627</td>\n",
       "      <td>0.212392</td>\n",
       "      <td>0.271560</td>\n",
       "      <td>0.373231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.300904</td>\n",
       "      <td>0.454955</td>\n",
       "      <td>0.174322</td>\n",
       "      <td>0.219362</td>\n",
       "      <td>0.269760</td>\n",
       "      <td>0.408076</td>\n",
       "      <td>0.398824</td>\n",
       "      <td>0.205252</td>\n",
       "      <td>0.258692</td>\n",
       "      <td>0.307016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.297149</td>\n",
       "      <td>0.441614</td>\n",
       "      <td>0.170558</td>\n",
       "      <td>0.214117</td>\n",
       "      <td>0.267881</td>\n",
       "      <td>0.398894</td>\n",
       "      <td>0.395860</td>\n",
       "      <td>0.202949</td>\n",
       "      <td>0.248635</td>\n",
       "      <td>0.291223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.294615</td>\n",
       "      <td>0.432606</td>\n",
       "      <td>0.168464</td>\n",
       "      <td>0.211219</td>\n",
       "      <td>0.266337</td>\n",
       "      <td>0.393575</td>\n",
       "      <td>0.390969</td>\n",
       "      <td>0.200847</td>\n",
       "      <td>0.240012</td>\n",
       "      <td>0.285958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.293744</td>\n",
       "      <td>0.429598</td>\n",
       "      <td>0.166457</td>\n",
       "      <td>0.209519</td>\n",
       "      <td>0.265453</td>\n",
       "      <td>0.389807</td>\n",
       "      <td>0.390257</td>\n",
       "      <td>0.200325</td>\n",
       "      <td>0.233192</td>\n",
       "      <td>0.283604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.292254</td>\n",
       "      <td>0.426047</td>\n",
       "      <td>0.165044</td>\n",
       "      <td>0.207997</td>\n",
       "      <td>0.264652</td>\n",
       "      <td>0.387160</td>\n",
       "      <td>0.387639</td>\n",
       "      <td>0.199145</td>\n",
       "      <td>0.227173</td>\n",
       "      <td>0.281030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.290828</td>\n",
       "      <td>0.424440</td>\n",
       "      <td>0.164444</td>\n",
       "      <td>0.207295</td>\n",
       "      <td>0.263007</td>\n",
       "      <td>0.384950</td>\n",
       "      <td>0.385753</td>\n",
       "      <td>0.198478</td>\n",
       "      <td>0.222247</td>\n",
       "      <td>0.279222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.291058</td>\n",
       "      <td>0.421728</td>\n",
       "      <td>0.163466</td>\n",
       "      <td>0.206011</td>\n",
       "      <td>0.263070</td>\n",
       "      <td>0.382476</td>\n",
       "      <td>0.384531</td>\n",
       "      <td>0.197904</td>\n",
       "      <td>0.218191</td>\n",
       "      <td>0.276959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.290285</td>\n",
       "      <td>0.420428</td>\n",
       "      <td>0.162087</td>\n",
       "      <td>0.204745</td>\n",
       "      <td>0.262026</td>\n",
       "      <td>0.381623</td>\n",
       "      <td>0.383637</td>\n",
       "      <td>0.197369</td>\n",
       "      <td>0.214799</td>\n",
       "      <td>0.274570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.290243</td>\n",
       "      <td>0.417680</td>\n",
       "      <td>0.162093</td>\n",
       "      <td>0.205376</td>\n",
       "      <td>0.262410</td>\n",
       "      <td>0.380699</td>\n",
       "      <td>0.383116</td>\n",
       "      <td>0.197099</td>\n",
       "      <td>0.211430</td>\n",
       "      <td>0.272836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.289394</td>\n",
       "      <td>0.418286</td>\n",
       "      <td>0.161749</td>\n",
       "      <td>0.205323</td>\n",
       "      <td>0.261096</td>\n",
       "      <td>0.379211</td>\n",
       "      <td>0.382757</td>\n",
       "      <td>0.196573</td>\n",
       "      <td>0.208554</td>\n",
       "      <td>0.268862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.288964</td>\n",
       "      <td>0.417550</td>\n",
       "      <td>0.161035</td>\n",
       "      <td>0.204004</td>\n",
       "      <td>0.260576</td>\n",
       "      <td>0.378899</td>\n",
       "      <td>0.381731</td>\n",
       "      <td>0.196078</td>\n",
       "      <td>0.205767</td>\n",
       "      <td>0.266991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.289872</td>\n",
       "      <td>0.415027</td>\n",
       "      <td>0.160975</td>\n",
       "      <td>0.203782</td>\n",
       "      <td>0.260093</td>\n",
       "      <td>0.379252</td>\n",
       "      <td>0.380136</td>\n",
       "      <td>0.195824</td>\n",
       "      <td>0.203206</td>\n",
       "      <td>0.264600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.289617</td>\n",
       "      <td>0.413375</td>\n",
       "      <td>0.160364</td>\n",
       "      <td>0.203904</td>\n",
       "      <td>0.260111</td>\n",
       "      <td>0.377198</td>\n",
       "      <td>0.379579</td>\n",
       "      <td>0.194927</td>\n",
       "      <td>0.200801</td>\n",
       "      <td>0.260881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.288391</td>\n",
       "      <td>0.413180</td>\n",
       "      <td>0.160221</td>\n",
       "      <td>0.203656</td>\n",
       "      <td>0.260098</td>\n",
       "      <td>0.377150</td>\n",
       "      <td>0.379708</td>\n",
       "      <td>0.194846</td>\n",
       "      <td>0.198576</td>\n",
       "      <td>0.259318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.286588</td>\n",
       "      <td>0.412907</td>\n",
       "      <td>0.159848</td>\n",
       "      <td>0.203485</td>\n",
       "      <td>0.259699</td>\n",
       "      <td>0.377403</td>\n",
       "      <td>0.378791</td>\n",
       "      <td>0.194812</td>\n",
       "      <td>0.196512</td>\n",
       "      <td>0.257386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.288033</td>\n",
       "      <td>0.412067</td>\n",
       "      <td>0.159826</td>\n",
       "      <td>0.203220</td>\n",
       "      <td>0.258505</td>\n",
       "      <td>0.376645</td>\n",
       "      <td>0.378688</td>\n",
       "      <td>0.193697</td>\n",
       "      <td>0.194868</td>\n",
       "      <td>0.255201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.287525</td>\n",
       "      <td>0.411283</td>\n",
       "      <td>0.159212</td>\n",
       "      <td>0.202849</td>\n",
       "      <td>0.258628</td>\n",
       "      <td>0.376136</td>\n",
       "      <td>0.378463</td>\n",
       "      <td>0.192907</td>\n",
       "      <td>0.193704</td>\n",
       "      <td>0.255016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.286816</td>\n",
       "      <td>0.410245</td>\n",
       "      <td>0.159666</td>\n",
       "      <td>0.203068</td>\n",
       "      <td>0.257924</td>\n",
       "      <td>0.377672</td>\n",
       "      <td>0.377615</td>\n",
       "      <td>0.192207</td>\n",
       "      <td>0.193122</td>\n",
       "      <td>0.253232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.202732</td>\n",
       "      <td>0.258201</td>\n",
       "      <td>0.375924</td>\n",
       "      <td>0.378238</td>\n",
       "      <td>0.192318</td>\n",
       "      <td>0.192884</td>\n",
       "      <td>0.254598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.201771</td>\n",
       "      <td>0.257605</td>\n",
       "      <td>0.377111</td>\n",
       "      <td>0.377234</td>\n",
       "      <td>0.191810</td>\n",
       "      <td>0.192776</td>\n",
       "      <td>0.252560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.202171</td>\n",
       "      <td>0.257030</td>\n",
       "      <td>0.375686</td>\n",
       "      <td>0.377433</td>\n",
       "      <td>0.191850</td>\n",
       "      <td>0.192699</td>\n",
       "      <td>0.254872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.201849</td>\n",
       "      <td>0.257321</td>\n",
       "      <td>0.374921</td>\n",
       "      <td>0.377079</td>\n",
       "      <td>0.191319</td>\n",
       "      <td>0.192663</td>\n",
       "      <td>0.254157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.201754</td>\n",
       "      <td>0.256322</td>\n",
       "      <td>0.374221</td>\n",
       "      <td>0.376496</td>\n",
       "      <td>0.191444</td>\n",
       "      <td>0.192631</td>\n",
       "      <td>0.251871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.201575</td>\n",
       "      <td>0.255983</td>\n",
       "      <td>0.375584</td>\n",
       "      <td>0.376599</td>\n",
       "      <td>0.191202</td>\n",
       "      <td>0.192614</td>\n",
       "      <td>0.253326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.201141</td>\n",
       "      <td>0.256413</td>\n",
       "      <td>0.373745</td>\n",
       "      <td>0.376753</td>\n",
       "      <td>0.190889</td>\n",
       "      <td>0.192606</td>\n",
       "      <td>0.252024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.201617</td>\n",
       "      <td>0.255638</td>\n",
       "      <td>0.374031</td>\n",
       "      <td>0.375796</td>\n",
       "      <td>0.190742</td>\n",
       "      <td>0.192598</td>\n",
       "      <td>0.250654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.201715</td>\n",
       "      <td>0.255951</td>\n",
       "      <td>0.374926</td>\n",
       "      <td>0.375798</td>\n",
       "      <td>0.190626</td>\n",
       "      <td>0.192593</td>\n",
       "      <td>0.253115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.201294</td>\n",
       "      <td>0.255215</td>\n",
       "      <td>0.374475</td>\n",
       "      <td>0.376209</td>\n",
       "      <td>0.190547</td>\n",
       "      <td>0.192591</td>\n",
       "      <td>0.251193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.200766</td>\n",
       "      <td>0.255301</td>\n",
       "      <td>0.373967</td>\n",
       "      <td>0.375130</td>\n",
       "      <td>0.190815</td>\n",
       "      <td>0.192588</td>\n",
       "      <td>0.251017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.201345</td>\n",
       "      <td>0.255500</td>\n",
       "      <td>0.373450</td>\n",
       "      <td>0.375426</td>\n",
       "      <td>0.190484</td>\n",
       "      <td>0.192586</td>\n",
       "      <td>0.250437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.201410</td>\n",
       "      <td>0.255163</td>\n",
       "      <td>0.373871</td>\n",
       "      <td>0.375620</td>\n",
       "      <td>0.190572</td>\n",
       "      <td>0.192586</td>\n",
       "      <td>0.250664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.200746</td>\n",
       "      <td>0.254821</td>\n",
       "      <td>0.373175</td>\n",
       "      <td>0.375249</td>\n",
       "      <td>0.190356</td>\n",
       "      <td>0.192585</td>\n",
       "      <td>0.251438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.201057</td>\n",
       "      <td>0.254729</td>\n",
       "      <td>0.373035</td>\n",
       "      <td>0.375205</td>\n",
       "      <td>0.190230</td>\n",
       "      <td>0.192584</td>\n",
       "      <td>0.250335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.201099</td>\n",
       "      <td>0.254702</td>\n",
       "      <td>0.373658</td>\n",
       "      <td>0.375295</td>\n",
       "      <td>0.190188</td>\n",
       "      <td>0.192583</td>\n",
       "      <td>0.251175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.201219</td>\n",
       "      <td>0.254471</td>\n",
       "      <td>0.373128</td>\n",
       "      <td>0.375084</td>\n",
       "      <td>0.189780</td>\n",
       "      <td>0.192583</td>\n",
       "      <td>0.249711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.201056</td>\n",
       "      <td>0.254725</td>\n",
       "      <td>0.373858</td>\n",
       "      <td>0.374953</td>\n",
       "      <td>0.189908</td>\n",
       "      <td>0.192583</td>\n",
       "      <td>0.250674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.200649</td>\n",
       "      <td>0.253882</td>\n",
       "      <td>0.372704</td>\n",
       "      <td>0.374777</td>\n",
       "      <td>0.189746</td>\n",
       "      <td>0.192583</td>\n",
       "      <td>0.251411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.200838</td>\n",
       "      <td>0.254066</td>\n",
       "      <td>0.373787</td>\n",
       "      <td>0.374579</td>\n",
       "      <td>0.190186</td>\n",
       "      <td>0.192583</td>\n",
       "      <td>0.251365</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     1  500101001  500101013  500101025  500101035  500101093  500101185  \\\n",
       "0  NaN   0.358454   0.496978   0.238550   0.274097   0.287640   0.490096   \n",
       "1  NaN   0.308635   0.473846   0.181518   0.228091   0.273880   0.426485   \n",
       "2  NaN   0.300904   0.454955   0.174322   0.219362   0.269760   0.408076   \n",
       "3  NaN   0.297149   0.441614   0.170558   0.214117   0.267881   0.398894   \n",
       "4  NaN   0.294615   0.432606   0.168464   0.211219   0.266337   0.393575   \n",
       "5  NaN   0.293744   0.429598   0.166457   0.209519   0.265453   0.389807   \n",
       "6  NaN   0.292254   0.426047   0.165044   0.207997   0.264652   0.387160   \n",
       "7  NaN   0.290828   0.424440   0.164444   0.207295   0.263007   0.384950   \n",
       "8  NaN   0.291058   0.421728   0.163466   0.206011   0.263070   0.382476   \n",
       "9  NaN   0.290285   0.420428   0.162087   0.204745   0.262026   0.381623   \n",
       "10 NaN   0.290243   0.417680   0.162093   0.205376   0.262410   0.380699   \n",
       "11 NaN   0.289394   0.418286   0.161749   0.205323   0.261096   0.379211   \n",
       "12 NaN   0.288964   0.417550   0.161035   0.204004   0.260576   0.378899   \n",
       "13 NaN   0.289872   0.415027   0.160975   0.203782   0.260093   0.379252   \n",
       "14 NaN   0.289617   0.413375   0.160364   0.203904   0.260111   0.377198   \n",
       "15 NaN   0.288391   0.413180   0.160221   0.203656   0.260098   0.377150   \n",
       "16 NaN   0.286588   0.412907   0.159848   0.203485   0.259699   0.377403   \n",
       "17 NaN   0.288033   0.412067   0.159826   0.203220   0.258505   0.376645   \n",
       "18 NaN   0.287525   0.411283   0.159212   0.202849   0.258628   0.376136   \n",
       "19 NaN   0.286816   0.410245   0.159666   0.203068   0.257924   0.377672   \n",
       "20 NaN        NaN        NaN        NaN   0.202732   0.258201   0.375924   \n",
       "21 NaN        NaN        NaN        NaN   0.201771   0.257605   0.377111   \n",
       "22 NaN        NaN        NaN        NaN   0.202171   0.257030   0.375686   \n",
       "23 NaN        NaN        NaN        NaN   0.201849   0.257321   0.374921   \n",
       "24 NaN        NaN        NaN        NaN   0.201754   0.256322   0.374221   \n",
       "25 NaN        NaN        NaN        NaN   0.201575   0.255983   0.375584   \n",
       "26 NaN        NaN        NaN        NaN   0.201141   0.256413   0.373745   \n",
       "27 NaN        NaN        NaN        NaN   0.201617   0.255638   0.374031   \n",
       "28 NaN        NaN        NaN        NaN   0.201715   0.255951   0.374926   \n",
       "29 NaN        NaN        NaN        NaN   0.201294   0.255215   0.374475   \n",
       "30 NaN        NaN        NaN        NaN   0.200766   0.255301   0.373967   \n",
       "31 NaN        NaN        NaN        NaN   0.201345   0.255500   0.373450   \n",
       "32 NaN        NaN        NaN        NaN   0.201410   0.255163   0.373871   \n",
       "33 NaN        NaN        NaN        NaN   0.200746   0.254821   0.373175   \n",
       "34 NaN        NaN        NaN        NaN   0.201057   0.254729   0.373035   \n",
       "35 NaN        NaN        NaN        NaN   0.201099   0.254702   0.373658   \n",
       "36 NaN        NaN        NaN        NaN   0.201219   0.254471   0.373128   \n",
       "37 NaN        NaN        NaN        NaN   0.201056   0.254725   0.373858   \n",
       "38 NaN        NaN        NaN        NaN   0.200649   0.253882   0.372704   \n",
       "39 NaN        NaN        NaN        NaN   0.200838   0.254066   0.373787   \n",
       "\n",
       "    500105066  500119049  500119059  500119069  \n",
       "0    0.436247   0.252419   0.303835   0.590141  \n",
       "1    0.403627   0.212392   0.271560   0.373231  \n",
       "2    0.398824   0.205252   0.258692   0.307016  \n",
       "3    0.395860   0.202949   0.248635   0.291223  \n",
       "4    0.390969   0.200847   0.240012   0.285958  \n",
       "5    0.390257   0.200325   0.233192   0.283604  \n",
       "6    0.387639   0.199145   0.227173   0.281030  \n",
       "7    0.385753   0.198478   0.222247   0.279222  \n",
       "8    0.384531   0.197904   0.218191   0.276959  \n",
       "9    0.383637   0.197369   0.214799   0.274570  \n",
       "10   0.383116   0.197099   0.211430   0.272836  \n",
       "11   0.382757   0.196573   0.208554   0.268862  \n",
       "12   0.381731   0.196078   0.205767   0.266991  \n",
       "13   0.380136   0.195824   0.203206   0.264600  \n",
       "14   0.379579   0.194927   0.200801   0.260881  \n",
       "15   0.379708   0.194846   0.198576   0.259318  \n",
       "16   0.378791   0.194812   0.196512   0.257386  \n",
       "17   0.378688   0.193697   0.194868   0.255201  \n",
       "18   0.378463   0.192907   0.193704   0.255016  \n",
       "19   0.377615   0.192207   0.193122   0.253232  \n",
       "20   0.378238   0.192318   0.192884   0.254598  \n",
       "21   0.377234   0.191810   0.192776   0.252560  \n",
       "22   0.377433   0.191850   0.192699   0.254872  \n",
       "23   0.377079   0.191319   0.192663   0.254157  \n",
       "24   0.376496   0.191444   0.192631   0.251871  \n",
       "25   0.376599   0.191202   0.192614   0.253326  \n",
       "26   0.376753   0.190889   0.192606   0.252024  \n",
       "27   0.375796   0.190742   0.192598   0.250654  \n",
       "28   0.375798   0.190626   0.192593   0.253115  \n",
       "29   0.376209   0.190547   0.192591   0.251193  \n",
       "30   0.375130   0.190815   0.192588   0.251017  \n",
       "31   0.375426   0.190484   0.192586   0.250437  \n",
       "32   0.375620   0.190572   0.192586   0.250664  \n",
       "33   0.375249   0.190356   0.192585   0.251438  \n",
       "34   0.375205   0.190230   0.192584   0.250335  \n",
       "35   0.375295   0.190188   0.192583   0.251175  \n",
       "36   0.375084   0.189780   0.192583   0.249711  \n",
       "37   0.374953   0.189908   0.192583   0.250674  \n",
       "38   0.374777   0.189746   0.192583   0.251411  \n",
       "39   0.374579   0.190186   0.192583   0.251365  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('loss.csv')\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "html-qsiNAWFM-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
