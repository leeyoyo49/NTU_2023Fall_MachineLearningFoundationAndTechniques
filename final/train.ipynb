{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import random_split, TensorDataset\n",
    "\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "import optuna\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_loss_in_csv(model_num, loss):\n",
    "    df = pd.read_csv('loss.csv')\n",
    "    df = df.reindex(range(len(loss)))\n",
    "    df[f\"{model_num}\"] = loss\n",
    "    df.to_csv('loss.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the gradient boosting regression model with PyTorch\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, dropout_rate):\n",
    "        super(Net, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout_rate)\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3= nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc4= nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc5= nn.Linear(hidden_size, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.dropout(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc3(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc4(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc5(out)\n",
    "        return out\n",
    "    \n",
    "class CustomLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomLoss, self).__init__()\n",
    "        \n",
    "    def forward(self, outputs, labels):\n",
    "        # Define your custom loss function here\n",
    "        # This is a simple example, replace it with your own function\n",
    "        custom_loss = 3*(torch.abs(labels - outputs))*(torch.abs(labels - 1/3) + torch.abs(labels -2/3))\n",
    "        return custom_loss\n",
    "    \n",
    "\n",
    "    \n",
    "def objective(trial):\n",
    "    # Sample hyperparameters\n",
    "    input_size = 8\n",
    "    hidden_size = trial.suggest_int('hidden_size', 3, 1000, log=True)  # Single hidden size for all layers\n",
    "    output_size = 1\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-1, log=True)\n",
    "    dropout_rate = trial.suggest_float('dropout_rate', 0.0, 0.3)\n",
    "\n",
    "    # Instantiate the model with sampled hyperparameters\n",
    "    model = Net(input_size, hidden_size, output_size, dropout_rate)\n",
    "\n",
    "    # Define the loss function and optimizer\n",
    "    criterion = CustomLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(100):\n",
    "        training_loss = 0.0\n",
    "        eval_loss = 0.0\n",
    "\n",
    "        # Iterate over the DataLoader for training data\n",
    "        for i, data in enumerate(train_loader):\n",
    "            inputs, labels = data\n",
    "            inputs = inputs.float()\n",
    "            labels = labels.float()\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            outputs = outputs.reshape(-1)\n",
    "            loss = criterion(outputs, labels).sum()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            training_loss += loss.item()\n",
    "\n",
    "        model.eval()\n",
    "        outloss = 0\n",
    "        # Iterate over the DataLoader for test data\n",
    "        with torch.no_grad():\n",
    "            for i, data in enumerate(test_loader):\n",
    "                inputs, labels = data\n",
    "                inputs = inputs.float()\n",
    "                labels = labels.float()\n",
    "                outputs = model(inputs).reshape(-1)\n",
    "                loss = criterion(outputs, labels).sum()\n",
    "                eval_loss += loss\n",
    "\n",
    "        # Optuna logs the running loss for each epoch\n",
    "        trial.report(eval_loss, epoch)\n",
    "\n",
    "        # Handle pruning based on the intermediate value\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "    # Return the running loss as the objective value to minimize\n",
    "    return eval_loss\n",
    "\n",
    "\n",
    "class station_model():\n",
    "    def __init__(self, station):\n",
    "        self.station = station\n",
    "        self.epoch_cnt = 20\n",
    "        self.trial_cnt = 100\n",
    "\n",
    "    def get_data_loader(self):\n",
    "        df = pd.read_parquet(f'parquets/{self.station}.parquet')\n",
    "        TOT = df['tot'].iloc[0]\n",
    "        df['sbi'] = df['sbi']/TOT\n",
    "        df['time'] = df['time']/1440\n",
    "\n",
    "        # x is dataset without 'sbi', y is 'sbi'\n",
    "        X = df.drop(['tot', 'sbi','bemp' ,'act', 'tot', 'station'], axis=1)\n",
    "        y = df['sbi']\n",
    "\n",
    "        X = X.to_numpy()\n",
    "        y = y.to_numpy()\n",
    "\n",
    "        X = torch.from_numpy(X)\n",
    "        y = torch.from_numpy(y)\n",
    "        dataset = TensorDataset(X, y)\n",
    "\n",
    "        # get train, test loader\n",
    "        self.all_data_loader = torch.utils.data.DataLoader(dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "        # split train, test\n",
    "        train_size = int(0.8 * len(dataset))\n",
    "        test_size = len(dataset) - train_size\n",
    "        train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "        # get train, test loader\n",
    "        self.train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "        self.test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=128, shuffle=True)\n",
    "        print(f'get data loader for {self.station}')\n",
    "\n",
    "    def objective(self,trial):\n",
    "        # Sample hyperparameters\n",
    "        input_size = 8\n",
    "        hidden_size = trial.suggest_int('hidden_size', 3, 2000, log=True)  # Single hidden size for all layers\n",
    "        output_size = 1\n",
    "        learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-1, log=True)\n",
    "        dropout_rate = trial.suggest_float('dropout_rate', 0.2, 0.5)\n",
    "\n",
    "        # Instantiate the model with sampled hyperparameters\n",
    "        model = Net(input_size, hidden_size, output_size, dropout_rate)\n",
    "\n",
    "        # Define the loss function and optimizer\n",
    "        criterion = CustomLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "        # Training loop\n",
    "        for epoch in range(self.epoch_cnt):\n",
    "            training_loss = 0.0\n",
    "            eval_loss = 0.0\n",
    "            # Iterate over the DataLoader for training data\n",
    "            for i, data in enumerate(self.train_loader):\n",
    "                inputs, labels = data\n",
    "                inputs = inputs.float()\n",
    "                labels = labels.float()\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                outputs = outputs.reshape(-1)\n",
    "                loss = criterion(outputs, labels).sum()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                training_loss += loss.item()\n",
    "\n",
    "            model.eval()\n",
    "            # Iterate over the DataLoader for test data\n",
    "            with torch.no_grad():\n",
    "                for i, data in enumerate(self.test_loader):\n",
    "                    inputs, labels = data\n",
    "                    inputs = inputs.float()\n",
    "                    labels = labels.float()\n",
    "                    outputs = model(inputs).reshape(-1)\n",
    "                    loss = criterion(outputs, labels).sum()\n",
    "                    eval_loss += loss\n",
    "\n",
    "            # Optuna logs the running loss for each epoch\n",
    "            trial.report(eval_loss, epoch)\n",
    "            # Handle pruning based on the intermediate value\n",
    "            if trial.should_prune():\n",
    "                raise optuna.exceptions.TrialPruned()\n",
    "        # Return the running loss as the objective value to minimize\n",
    "        return eval_loss      \n",
    "\n",
    "\n",
    "    def get_best_param(self):\n",
    "        # Create an Optuna Study\n",
    "        study = optuna.create_study(direction='minimize', storage='sqlite:///db.sqlite3', study_name=f'{self.station}', load_if_exists=True)\n",
    "        num_trials = len(study.trials)\n",
    "\n",
    "        study.optimize(self.objective, n_trials=self.trial_cnt)\n",
    "        # Run the optimization process\n",
    "\n",
    "        print(f'{self.station} eval_loss: {study.best_trial.value}')\n",
    "\n",
    "        # Access the best hyperparameters\n",
    "        best_params = study.best_trial.params\n",
    "\n",
    "        self.best_params = best_params\n",
    "\n",
    "        print(self.station,' Best hyperparameters:', best_params)\n",
    "\n",
    "    def train(self):\n",
    "        # Instantiate the final model with the best hyperparameters\n",
    "        final_model = Net(8, self.best_params['hidden_size'], 1, self.best_params['dropout_rate'])\n",
    "\n",
    "        # ... rest of your training code for the final model\n",
    "        criterion = CustomLoss()\n",
    "        optimizer = optim.Adam(final_model.parameters(), lr=self.best_params['learning_rate'])\n",
    "\n",
    "        # train with whole dataset\n",
    "        running_loss = 0.0\n",
    "        loss_list = []\n",
    "        for epoch in range(self.epoch_cnt):\n",
    "            for i, data in enumerate(self.all_data_loader):\n",
    "                inputs, labels = data\n",
    "                inputs = inputs.float()\n",
    "                labels = labels.float()\n",
    "                optimizer.zero_grad()\n",
    "                outputs = final_model(inputs)\n",
    "                outputs = outputs.squeeze()\n",
    "                loss = criterion(outputs, labels).sum()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item()/128\n",
    "            print(f'epoch: {epoch+1}, loss: {running_loss/self.all_data_loader.__len__()}')\n",
    "            loss_list.append(running_loss/self.all_data_loader.__len__())\n",
    "            running_loss = 0.0\n",
    "        save_loss_in_csv(self.station, loss_list)\n",
    "        \n",
    "        final_model.eval()\n",
    "        # save model\n",
    "        torch.save(final_model, f'models/{self.station}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['500101040' '500101041' '500101042' '500101091' '500101092' '500101093'\n",
      " '500101094' '500101114' '500101115' '500101123' '500101166' '500101175'\n",
      " '500101176' '500101181' '500101184' '500101185' '500101188' '500101189'\n",
      " '500101190' '500101191' '500101193' '500101199' '500101209' '500101216'\n",
      " '500101219' '500105066' '500106002' '500106003' '500106004' '500119043'\n",
      " '500119044' '500119045' '500119046' '500119047' '500119048']\n"
     ]
    }
   ],
   "source": [
    "available_station = np.loadtxt('html.2023.final.data/sno_test_set.txt', dtype='str')\n",
    "# available_station = available_station[:35]\n",
    "folder_path = 'models/'  # replace with your actual folder path\n",
    "trained_models = os.listdir(folder_path)\n",
    "available_station = np.setdiff1d(available_station, trained_models)\n",
    "available_station = available_station[:35]\n",
    "print(available_station)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'parquets/500101040.parquet'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/yl/HTML/final/train.ipynb Cell 5\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/yl/HTML/final/train.ipynb#W4sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfor\u001b[39;00m station_id \u001b[39min\u001b[39;00m available_station:\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/yl/HTML/final/train.ipynb#W4sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     station \u001b[39m=\u001b[39m station_model(station_id)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/yl/HTML/final/train.ipynb#W4sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     station\u001b[39m.\u001b[39;49mget_data_loader()\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/yl/HTML/final/train.ipynb#W4sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     station\u001b[39m.\u001b[39mget_best_param()\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/yl/HTML/final/train.ipynb#W4sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     station\u001b[39m.\u001b[39mtrain()\n",
      "\u001b[1;32m/Users/yl/HTML/final/train.ipynb Cell 5\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/yl/HTML/final/train.ipynb#W4sZmlsZQ%3D%3D?line=103'>104</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_data_loader\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/yl/HTML/final/train.ipynb#W4sZmlsZQ%3D%3D?line=104'>105</a>\u001b[0m     df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_parquet(\u001b[39mf\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mparquets/\u001b[39;49m\u001b[39m{\u001b[39;49;00m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstation\u001b[39m}\u001b[39;49;00m\u001b[39m.parquet\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/yl/HTML/final/train.ipynb#W4sZmlsZQ%3D%3D?line=105'>106</a>\u001b[0m     TOT \u001b[39m=\u001b[39m df[\u001b[39m'\u001b[39m\u001b[39mtot\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39miloc[\u001b[39m0\u001b[39m]\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/yl/HTML/final/train.ipynb#W4sZmlsZQ%3D%3D?line=106'>107</a>\u001b[0m     df[\u001b[39m'\u001b[39m\u001b[39msbi\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m df[\u001b[39m'\u001b[39m\u001b[39msbi\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m/\u001b[39mTOT\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/html-qsiNAWFM-py3.11/lib/python3.11/site-packages/pandas/io/parquet.py:670\u001b[0m, in \u001b[0;36mread_parquet\u001b[0;34m(path, engine, columns, storage_options, use_nullable_dtypes, dtype_backend, filesystem, filters, **kwargs)\u001b[0m\n\u001b[1;32m    667\u001b[0m     use_nullable_dtypes \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    668\u001b[0m check_dtype_backend(dtype_backend)\n\u001b[0;32m--> 670\u001b[0m \u001b[39mreturn\u001b[39;00m impl\u001b[39m.\u001b[39;49mread(\n\u001b[1;32m    671\u001b[0m     path,\n\u001b[1;32m    672\u001b[0m     columns\u001b[39m=\u001b[39;49mcolumns,\n\u001b[1;32m    673\u001b[0m     filters\u001b[39m=\u001b[39;49mfilters,\n\u001b[1;32m    674\u001b[0m     storage_options\u001b[39m=\u001b[39;49mstorage_options,\n\u001b[1;32m    675\u001b[0m     use_nullable_dtypes\u001b[39m=\u001b[39;49muse_nullable_dtypes,\n\u001b[1;32m    676\u001b[0m     dtype_backend\u001b[39m=\u001b[39;49mdtype_backend,\n\u001b[1;32m    677\u001b[0m     filesystem\u001b[39m=\u001b[39;49mfilesystem,\n\u001b[1;32m    678\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    679\u001b[0m )\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/html-qsiNAWFM-py3.11/lib/python3.11/site-packages/pandas/io/parquet.py:265\u001b[0m, in \u001b[0;36mPyArrowImpl.read\u001b[0;34m(self, path, columns, filters, use_nullable_dtypes, dtype_backend, storage_options, filesystem, **kwargs)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[39mif\u001b[39;00m manager \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39marray\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    263\u001b[0m     to_pandas_kwargs[\u001b[39m\"\u001b[39m\u001b[39msplit_blocks\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m  \u001b[39m# type: ignore[assignment]\u001b[39;00m\n\u001b[0;32m--> 265\u001b[0m path_or_handle, handles, filesystem \u001b[39m=\u001b[39m _get_path_or_handle(\n\u001b[1;32m    266\u001b[0m     path,\n\u001b[1;32m    267\u001b[0m     filesystem,\n\u001b[1;32m    268\u001b[0m     storage_options\u001b[39m=\u001b[39;49mstorage_options,\n\u001b[1;32m    269\u001b[0m     mode\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    270\u001b[0m )\n\u001b[1;32m    271\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    272\u001b[0m     pa_table \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi\u001b[39m.\u001b[39mparquet\u001b[39m.\u001b[39mread_table(\n\u001b[1;32m    273\u001b[0m         path_or_handle,\n\u001b[1;32m    274\u001b[0m         columns\u001b[39m=\u001b[39mcolumns,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    277\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m    278\u001b[0m     )\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/html-qsiNAWFM-py3.11/lib/python3.11/site-packages/pandas/io/parquet.py:139\u001b[0m, in \u001b[0;36m_get_path_or_handle\u001b[0;34m(path, fs, storage_options, mode, is_dir)\u001b[0m\n\u001b[1;32m    129\u001b[0m handles \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    131\u001b[0m     \u001b[39mnot\u001b[39;00m fs\n\u001b[1;32m    132\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_dir\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[39m# fsspec resources can also point to directories\u001b[39;00m\n\u001b[1;32m    138\u001b[0m     \u001b[39m# this branch is used for example when reading from non-fsspec URLs\u001b[39;00m\n\u001b[0;32m--> 139\u001b[0m     handles \u001b[39m=\u001b[39m get_handle(\n\u001b[1;32m    140\u001b[0m         path_or_handle, mode, is_text\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, storage_options\u001b[39m=\u001b[39;49mstorage_options\n\u001b[1;32m    141\u001b[0m     )\n\u001b[1;32m    142\u001b[0m     fs \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    143\u001b[0m     path_or_handle \u001b[39m=\u001b[39m handles\u001b[39m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/html-qsiNAWFM-py3.11/lib/python3.11/site-packages/pandas/io/common.py:872\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    863\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(\n\u001b[1;32m    864\u001b[0m             handle,\n\u001b[1;32m    865\u001b[0m             ioargs\u001b[39m.\u001b[39mmode,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    868\u001b[0m             newline\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    869\u001b[0m         )\n\u001b[1;32m    870\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    871\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[0;32m--> 872\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(handle, ioargs\u001b[39m.\u001b[39;49mmode)\n\u001b[1;32m    873\u001b[0m     handles\u001b[39m.\u001b[39mappend(handle)\n\u001b[1;32m    875\u001b[0m \u001b[39m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'parquets/500101040.parquet'"
     ]
    }
   ],
   "source": [
    "for station_id in available_station:\n",
    "    station = station_model(station_id)\n",
    "    station.get_data_loader()\n",
    "    station.get_best_param()\n",
    "    station.train()\n",
    "    print(f'{station_id} done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-16 19:49:42,506] Using an existing study with name '500101003' instead of creating a new one.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[500101003, 500101004, 500101008, 500101009, 500101010, 500101013, 500101014, 500101018, 500101026]\n",
      "get data loader for 500101003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-16 19:49:43,738] Trial 480 pruned. \n",
      "[I 2023-12-16 19:49:44,220] Trial 481 pruned. \n",
      "[I 2023-12-16 19:49:44,846] Trial 482 pruned. \n",
      "[I 2023-12-16 19:49:45,446] Trial 483 pruned. \n",
      "[I 2023-12-16 19:49:46,630] Trial 484 pruned. \n",
      "[I 2023-12-16 19:49:47,081] Trial 485 pruned. \n",
      "[I 2023-12-16 19:49:47,495] Trial 486 pruned. \n",
      "[I 2023-12-16 19:49:48,450] Trial 487 pruned. \n",
      "[I 2023-12-16 19:49:52,147] Trial 488 pruned. \n",
      "[I 2023-12-16 19:49:52,550] Trial 489 pruned. \n",
      "[I 2023-12-16 19:49:53,494] Trial 490 pruned. \n",
      "[I 2023-12-16 19:50:03,845] Trial 491 pruned. \n",
      "[I 2023-12-16 19:50:08,623] Trial 492 pruned. \n",
      "[I 2023-12-16 19:50:09,093] Trial 493 pruned. \n",
      "[I 2023-12-16 19:50:09,835] Trial 494 pruned. \n",
      "[I 2023-12-16 19:50:13,371] Trial 495 pruned. \n",
      "[I 2023-12-16 19:50:13,951] Trial 496 pruned. \n",
      "[I 2023-12-16 19:50:16,351] Trial 497 pruned. \n",
      "[I 2023-12-16 19:50:17,238] Trial 498 pruned. \n",
      "[I 2023-12-16 19:50:18,279] Trial 499 pruned. \n",
      "[I 2023-12-16 19:50:18,702] Trial 500 pruned. \n",
      "[I 2023-12-16 19:50:21,103] Trial 501 pruned. \n",
      "[I 2023-12-16 19:50:24,285] Trial 502 pruned. \n",
      "[I 2023-12-16 19:50:24,822] Trial 503 pruned. \n",
      "[I 2023-12-16 19:50:25,229] Trial 504 pruned. \n",
      "[I 2023-12-16 19:50:25,712] Trial 505 pruned. \n",
      "[I 2023-12-16 19:50:28,216] Trial 506 pruned. \n",
      "[I 2023-12-16 19:50:28,803] Trial 507 pruned. \n",
      "[I 2023-12-16 19:50:29,264] Trial 508 pruned. \n",
      "[I 2023-12-16 19:50:30,451] Trial 509 pruned. \n",
      "[I 2023-12-16 19:50:30,867] Trial 510 pruned. \n",
      "[I 2023-12-16 19:50:31,655] Trial 511 pruned. \n",
      "[I 2023-12-16 19:50:32,798] Trial 512 pruned. \n",
      "[I 2023-12-16 19:50:35,033] Trial 513 pruned. \n",
      "[I 2023-12-16 19:50:35,978] Trial 514 pruned. \n",
      "[I 2023-12-16 19:50:36,698] Trial 515 pruned. \n",
      "[I 2023-12-16 19:50:38,347] Trial 516 pruned. \n",
      "[I 2023-12-16 19:50:39,309] Trial 517 pruned. \n",
      "[I 2023-12-16 19:50:40,125] Trial 518 pruned. \n",
      "[I 2023-12-16 19:50:45,850] Trial 519 pruned. \n",
      "[I 2023-12-16 19:50:46,301] Trial 520 pruned. \n",
      "[I 2023-12-16 19:50:46,775] Trial 521 pruned. \n",
      "[I 2023-12-16 19:50:48,243] Trial 522 pruned. \n",
      "[I 2023-12-16 19:50:48,663] Trial 523 pruned. \n",
      "[I 2023-12-16 19:50:52,383] Trial 524 pruned. \n",
      "[I 2023-12-16 19:50:55,556] Trial 525 pruned. \n",
      "[I 2023-12-16 19:50:56,240] Trial 526 pruned. \n",
      "[I 2023-12-16 19:50:58,580] Trial 527 pruned. \n",
      "[I 2023-12-16 19:51:06,848] Trial 528 pruned. \n",
      "[I 2023-12-16 19:51:07,407] Trial 529 pruned. \n",
      "[I 2023-12-16 19:51:08,346] Trial 530 pruned. \n",
      "[I 2023-12-16 19:51:08,902] Trial 531 pruned. \n",
      "[I 2023-12-16 19:51:12,923] Trial 532 pruned. \n",
      "[I 2023-12-16 19:51:15,672] Trial 533 pruned. \n",
      "[I 2023-12-16 19:51:16,143] Trial 534 pruned. \n",
      "[I 2023-12-16 19:51:16,792] Trial 535 pruned. \n",
      "[I 2023-12-16 19:51:18,451] Trial 536 pruned. \n",
      "[I 2023-12-16 19:51:18,860] Trial 537 pruned. \n",
      "[I 2023-12-16 19:51:19,294] Trial 538 pruned. \n",
      "[I 2023-12-16 19:51:19,708] Trial 539 pruned. \n",
      "[I 2023-12-16 19:51:20,401] Trial 540 pruned. \n",
      "[I 2023-12-16 19:51:20,898] Trial 541 pruned. \n",
      "[I 2023-12-16 19:51:21,570] Trial 542 pruned. \n",
      "[I 2023-12-16 19:51:22,153] Trial 543 pruned. \n",
      "[I 2023-12-16 19:51:23,567] Trial 544 pruned. \n",
      "[I 2023-12-16 19:51:23,973] Trial 545 pruned. \n",
      "[I 2023-12-16 19:51:38,018] Trial 546 pruned. \n",
      "[I 2023-12-16 19:51:38,506] Trial 547 pruned. \n",
      "[I 2023-12-16 19:51:38,949] Trial 548 pruned. \n",
      "[I 2023-12-16 19:51:39,872] Trial 549 pruned. \n",
      "[I 2023-12-16 19:51:40,293] Trial 550 pruned. \n",
      "[I 2023-12-16 19:51:49,839] Trial 551 pruned. \n",
      "[I 2023-12-16 19:51:50,252] Trial 552 pruned. \n",
      "[I 2023-12-16 19:51:56,893] Trial 553 pruned. \n",
      "[I 2023-12-16 19:51:57,344] Trial 554 pruned. \n",
      "[I 2023-12-16 19:51:58,194] Trial 555 pruned. \n",
      "[I 2023-12-16 19:51:59,382] Trial 556 pruned. \n",
      "[I 2023-12-16 19:52:01,246] Trial 557 pruned. \n",
      "[I 2023-12-16 19:52:03,186] Trial 558 pruned. \n",
      "[I 2023-12-16 19:52:05,844] Trial 559 pruned. \n",
      "[I 2023-12-16 19:52:06,757] Trial 560 pruned. \n",
      "[I 2023-12-16 19:52:08,380] Trial 561 pruned. \n",
      "[I 2023-12-16 19:52:09,425] Trial 562 pruned. \n",
      "[I 2023-12-16 19:52:09,908] Trial 563 pruned. \n",
      "[I 2023-12-16 19:52:11,220] Trial 564 pruned. \n",
      "[I 2023-12-16 19:52:11,774] Trial 565 pruned. \n",
      "[I 2023-12-16 19:52:13,428] Trial 566 pruned. \n",
      "[I 2023-12-16 19:52:13,831] Trial 567 pruned. \n",
      "[I 2023-12-16 19:52:14,265] Trial 568 pruned. \n",
      "[I 2023-12-16 19:52:14,949] Trial 569 pruned. \n",
      "[I 2023-12-16 19:52:15,412] Trial 570 pruned. \n",
      "[I 2023-12-16 19:52:16,157] Trial 571 pruned. \n",
      "[I 2023-12-16 19:52:17,204] Trial 572 pruned. \n",
      "[I 2023-12-16 19:52:17,734] Trial 573 pruned. \n",
      "[I 2023-12-16 19:52:18,345] Trial 574 pruned. \n",
      "[I 2023-12-16 19:52:18,885] Trial 575 pruned. \n",
      "[I 2023-12-16 19:52:21,000] Trial 576 pruned. \n",
      "[I 2023-12-16 19:52:34,553] Trial 577 pruned. \n",
      "[I 2023-12-16 19:52:34,983] Trial 578 pruned. \n",
      "[I 2023-12-16 19:52:35,451] Trial 579 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500101003 eval_loss: 5882.21875\n",
      "500101003  Best hyperparameters: {'hidden_size': 151, 'learning_rate': 0.0012323738201189078, 'dropout_rate': 0.21124150624483878}\n",
      "epoch: 1, loss: 0.4422799083621973\n",
      "epoch: 2, loss: 0.3917129118428151\n",
      "epoch: 3, loss: 0.38498967683692864\n",
      "epoch: 4, loss: 0.3819094836532081\n",
      "epoch: 5, loss: 0.37844545575108823\n",
      "epoch: 6, loss: 0.37605048927336615\n",
      "epoch: 7, loss: 0.3756995624097044\n",
      "epoch: 8, loss: 0.37441762488442126\n",
      "epoch: 9, loss: 0.3731591708474929\n",
      "epoch: 10, loss: 0.37282656375908746\n",
      "epoch: 11, loss: 0.3721342783078529\n",
      "epoch: 12, loss: 0.37245812614309304\n",
      "epoch: 13, loss: 0.37093430537264094\n",
      "epoch: 14, loss: 0.37052025021156393\n",
      "epoch: 15, loss: 0.37078560557255735\n",
      "epoch: 16, loss: 0.36949656532900366\n",
      "epoch: 17, loss: 0.3685296755705484\n",
      "epoch: 18, loss: 0.36904708921999235\n",
      "epoch: 19, loss: 0.36899127021184697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-16 19:52:53,285] Using an existing study with name '500101004' instead of creating a new one.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 20, loss: 0.36886378369809636\n",
      "train <__main__.station_model object at 0x17d74a010> done!\n",
      "get data loader for 500101004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-16 19:52:53,805] Trial 305 pruned. \n",
      "[I 2023-12-16 19:52:54,750] Trial 306 pruned. \n",
      "[I 2023-12-16 19:52:55,277] Trial 307 pruned. \n",
      "[I 2023-12-16 19:52:56,084] Trial 308 pruned. \n",
      "[I 2023-12-16 19:52:56,492] Trial 309 pruned. \n",
      "[I 2023-12-16 19:52:57,212] Trial 310 pruned. \n",
      "[I 2023-12-16 19:53:00,003] Trial 311 pruned. \n",
      "[I 2023-12-16 19:53:04,707] Trial 312 pruned. \n",
      "[I 2023-12-16 19:53:06,125] Trial 313 pruned. \n",
      "[I 2023-12-16 19:53:07,202] Trial 314 pruned. \n",
      "[I 2023-12-16 19:53:07,763] Trial 315 pruned. \n",
      "[I 2023-12-16 19:53:10,257] Trial 316 pruned. \n",
      "[I 2023-12-16 19:53:10,716] Trial 317 pruned. \n",
      "[I 2023-12-16 19:53:12,219] Trial 318 pruned. \n",
      "[I 2023-12-16 19:53:12,765] Trial 319 pruned. \n",
      "[I 2023-12-16 19:53:23,314] Trial 320 pruned. \n",
      "[I 2023-12-16 19:53:23,771] Trial 321 pruned. \n",
      "[I 2023-12-16 19:53:24,235] Trial 322 pruned. \n",
      "[I 2023-12-16 19:53:42,776] Trial 323 pruned. \n",
      "[I 2023-12-16 19:53:43,654] Trial 324 pruned. \n",
      "[I 2023-12-16 19:53:44,136] Trial 325 pruned. \n",
      "[I 2023-12-16 19:53:44,587] Trial 326 pruned. \n",
      "[I 2023-12-16 19:53:45,215] Trial 327 pruned. \n",
      "[I 2023-12-16 19:53:58,641] Trial 328 pruned. \n",
      "[I 2023-12-16 19:53:59,081] Trial 329 pruned. \n",
      "[I 2023-12-16 19:53:59,506] Trial 330 pruned. \n",
      "[I 2023-12-16 19:54:00,310] Trial 331 pruned. \n",
      "[I 2023-12-16 19:54:01,329] Trial 332 pruned. \n",
      "[I 2023-12-16 19:54:21,664] Trial 333 pruned. \n",
      "[I 2023-12-16 19:54:25,350] Trial 334 pruned. \n",
      "[I 2023-12-16 19:54:28,485] Trial 335 pruned. \n",
      "[I 2023-12-16 19:54:37,949] Trial 336 pruned. \n",
      "[I 2023-12-16 19:54:38,654] Trial 337 pruned. \n",
      "[I 2023-12-16 19:54:39,221] Trial 338 pruned. \n",
      "[I 2023-12-16 19:54:52,954] Trial 339 pruned. \n",
      "[I 2023-12-16 19:54:55,975] Trial 340 pruned. \n",
      "[I 2023-12-16 19:54:56,516] Trial 341 pruned. \n",
      "[I 2023-12-16 19:54:57,512] Trial 342 pruned. \n",
      "[I 2023-12-16 19:54:57,961] Trial 343 pruned. \n",
      "[I 2023-12-16 19:54:58,372] Trial 344 pruned. \n",
      "[I 2023-12-16 19:54:58,969] Trial 345 pruned. \n",
      "[I 2023-12-16 19:54:59,411] Trial 346 pruned. \n",
      "[I 2023-12-16 19:54:59,826] Trial 347 pruned. \n",
      "[I 2023-12-16 19:55:01,116] Trial 348 pruned. \n",
      "[I 2023-12-16 19:55:01,544] Trial 349 pruned. \n",
      "[I 2023-12-16 19:55:02,018] Trial 350 pruned. \n",
      "[I 2023-12-16 19:55:03,668] Trial 351 pruned. \n",
      "[I 2023-12-16 19:55:04,956] Trial 352 pruned. \n",
      "[I 2023-12-16 19:55:05,363] Trial 353 pruned. \n",
      "[I 2023-12-16 19:55:05,919] Trial 354 pruned. \n",
      "[I 2023-12-16 19:55:06,378] Trial 355 pruned. \n",
      "[I 2023-12-16 19:55:08,554] Trial 356 pruned. \n",
      "[I 2023-12-16 19:55:16,842] Trial 357 pruned. \n",
      "[I 2023-12-16 19:55:17,330] Trial 358 pruned. \n",
      "[I 2023-12-16 19:55:28,989] Trial 359 pruned. \n",
      "[I 2023-12-16 19:55:33,609] Trial 360 pruned. \n",
      "[I 2023-12-16 19:55:34,065] Trial 361 pruned. \n",
      "[I 2023-12-16 19:55:34,905] Trial 362 pruned. \n",
      "[I 2023-12-16 19:55:35,313] Trial 363 pruned. \n",
      "[I 2023-12-16 19:55:35,720] Trial 364 pruned. \n",
      "[I 2023-12-16 19:55:36,160] Trial 365 pruned. \n",
      "[I 2023-12-16 19:55:38,844] Trial 366 pruned. \n",
      "[I 2023-12-16 19:55:40,226] Trial 367 pruned. \n",
      "[I 2023-12-16 19:55:41,163] Trial 368 pruned. \n",
      "[I 2023-12-16 19:55:47,466] Trial 369 pruned. \n",
      "[I 2023-12-16 19:56:01,857] Trial 370 pruned. \n",
      "[I 2023-12-16 19:56:02,306] Trial 371 pruned. \n",
      "[I 2023-12-16 19:56:05,434] Trial 372 pruned. \n",
      "[I 2023-12-16 19:56:06,043] Trial 373 pruned. \n",
      "[I 2023-12-16 19:56:06,494] Trial 374 pruned. \n",
      "[I 2023-12-16 19:56:07,488] Trial 375 pruned. \n",
      "[I 2023-12-16 19:56:09,429] Trial 376 pruned. \n",
      "[I 2023-12-16 19:56:10,906] Trial 377 pruned. \n",
      "[I 2023-12-16 19:56:11,316] Trial 378 pruned. \n",
      "[I 2023-12-16 19:56:29,505] Trial 379 pruned. \n",
      "[I 2023-12-16 19:56:30,181] Trial 380 pruned. \n",
      "[I 2023-12-16 19:56:30,722] Trial 381 pruned. \n",
      "[I 2023-12-16 19:56:31,255] Trial 382 pruned. \n",
      "[I 2023-12-16 19:56:32,069] Trial 383 pruned. \n",
      "[I 2023-12-16 19:56:33,225] Trial 384 pruned. \n",
      "[I 2023-12-16 19:56:33,688] Trial 385 pruned. \n",
      "[I 2023-12-16 19:56:42,639] Trial 386 pruned. \n",
      "[I 2023-12-16 19:56:45,390] Trial 387 pruned. \n",
      "[I 2023-12-16 19:56:45,841] Trial 388 pruned. \n",
      "[I 2023-12-16 19:56:46,314] Trial 389 pruned. \n",
      "[I 2023-12-16 19:56:50,634] Trial 390 pruned. \n",
      "[I 2023-12-16 19:56:51,042] Trial 391 pruned. \n",
      "[I 2023-12-16 19:57:12,923] Trial 392 pruned. \n",
      "[I 2023-12-16 19:57:13,663] Trial 393 pruned. \n",
      "[I 2023-12-16 19:57:14,130] Trial 394 pruned. \n",
      "[I 2023-12-16 19:57:14,584] Trial 395 pruned. \n",
      "[I 2023-12-16 19:57:21,585] Trial 396 pruned. \n",
      "[I 2023-12-16 19:57:22,228] Trial 397 pruned. \n",
      "[I 2023-12-16 19:57:23,229] Trial 398 pruned. \n",
      "[I 2023-12-16 19:57:24,658] Trial 399 pruned. \n",
      "[I 2023-12-16 19:57:25,104] Trial 400 pruned. \n",
      "[I 2023-12-16 19:57:25,543] Trial 401 pruned. \n",
      "[I 2023-12-16 19:57:27,289] Trial 402 pruned. \n",
      "[I 2023-12-16 19:57:30,734] Trial 403 pruned. \n",
      "[I 2023-12-16 19:57:32,811] Trial 404 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500101004 eval_loss: 5998.4501953125\n",
      "500101004  Best hyperparameters: {'hidden_size': 718, 'learning_rate': 0.0004477829369950199, 'dropout_rate': 0.23214310395372975}\n",
      "epoch: 1, loss: 0.4568528639964568\n",
      "epoch: 2, loss: 0.4357847798823591\n",
      "epoch: 3, loss: 0.4287550973721399\n",
      "epoch: 4, loss: 0.42287967674333043\n",
      "epoch: 5, loss: 0.4163445786967716\n",
      "epoch: 6, loss: 0.41079830610913926\n",
      "epoch: 7, loss: 0.4088242112871029\n",
      "epoch: 8, loss: 0.40509997443106377\n",
      "epoch: 9, loss: 0.40097026129114144\n",
      "epoch: 10, loss: 0.3978843303512484\n",
      "epoch: 11, loss: 0.39535102223468943\n",
      "epoch: 12, loss: 0.3930795586486747\n",
      "epoch: 13, loss: 0.3895829824076159\n",
      "epoch: 14, loss: 0.3885398201003873\n",
      "epoch: 15, loss: 0.3860111709678515\n",
      "epoch: 16, loss: 0.3853596924926361\n",
      "epoch: 17, loss: 0.3842843703330013\n",
      "epoch: 18, loss: 0.38427938687225993\n",
      "epoch: 19, loss: 0.3826360616434034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-16 19:58:57,807] Using an existing study with name '500101008' instead of creating a new one.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 20, loss: 0.3829298801776331\n",
      "train <__main__.station_model object at 0x17d137b50> done!\n",
      "get data loader for 500101008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-16 19:59:00,130] Trial 305 pruned. \n",
      "[I 2023-12-16 19:59:05,514] Trial 306 pruned. \n",
      "[I 2023-12-16 19:59:16,012] Trial 307 pruned. \n",
      "[I 2023-12-16 19:59:16,786] Trial 308 pruned. \n",
      "[I 2023-12-16 19:59:17,777] Trial 309 pruned. \n",
      "[I 2023-12-16 19:59:28,751] Trial 310 pruned. \n",
      "[I 2023-12-16 19:59:30,168] Trial 311 pruned. \n",
      "[I 2023-12-16 19:59:34,293] Trial 312 pruned. \n",
      "[I 2023-12-16 19:59:37,383] Trial 313 pruned. \n",
      "[I 2023-12-16 20:02:07,229] Trial 314 finished with value: 7214.47802734375 and parameters: {'hidden_size': 1155, 'learning_rate': 0.001389819137217496, 'dropout_rate': 0.4624270997036185}. Best is trial 57 with value: 7155.44873046875.\n",
      "[I 2023-12-16 20:02:16,115] Trial 315 pruned. \n",
      "[I 2023-12-16 20:02:16,927] Trial 316 pruned. \n",
      "[I 2023-12-16 20:02:34,231] Trial 317 pruned. \n",
      "[I 2023-12-16 20:02:35,475] Trial 318 pruned. \n",
      "[I 2023-12-16 20:02:37,078] Trial 319 pruned. \n",
      "[I 2023-12-16 20:03:07,210] Trial 320 pruned. \n",
      "[I 2023-12-16 20:05:19,759] Trial 321 finished with value: 7376.57177734375 and parameters: {'hidden_size': 1079, 'learning_rate': 0.001650947662213198, 'dropout_rate': 0.45208304065676297}. Best is trial 57 with value: 7155.44873046875.\n",
      "[I 2023-12-16 20:05:21,971] Trial 322 pruned. \n",
      "[I 2023-12-16 20:05:34,212] Trial 323 pruned. \n",
      "[I 2023-12-16 20:05:56,751] Trial 324 finished with value: 7218.5205078125 and parameters: {'hidden_size': 360, 'learning_rate': 0.0013569360135991355, 'dropout_rate': 0.356769388697162}. Best is trial 57 with value: 7155.44873046875.\n",
      "[I 2023-12-16 20:06:03,780] Trial 325 pruned. \n",
      "[I 2023-12-16 20:06:05,053] Trial 326 pruned. \n",
      "[I 2023-12-16 20:06:26,936] Trial 327 finished with value: 7232.05517578125 and parameters: {'hidden_size': 340, 'learning_rate': 0.0013939138904510407, 'dropout_rate': 0.49575595350387364}. Best is trial 57 with value: 7155.44873046875.\n",
      "[I 2023-12-16 20:06:52,663] Trial 328 finished with value: 7164.0673828125 and parameters: {'hidden_size': 398, 'learning_rate': 0.0011682156577689977, 'dropout_rate': 0.4896513871474153}. Best is trial 57 with value: 7155.44873046875.\n",
      "[I 2023-12-16 20:06:54,318] Trial 329 pruned. \n",
      "[I 2023-12-16 20:06:57,430] Trial 330 pruned. \n",
      "[I 2023-12-16 20:06:59,170] Trial 331 pruned. \n",
      "[I 2023-12-16 20:07:00,588] Trial 332 pruned. \n",
      "[I 2023-12-16 20:07:02,447] Trial 333 pruned. \n",
      "[I 2023-12-16 20:07:04,003] Trial 334 pruned. \n",
      "[I 2023-12-16 20:07:06,438] Trial 335 pruned. \n",
      "[I 2023-12-16 20:07:07,764] Trial 336 pruned. \n",
      "[I 2023-12-16 20:07:24,330] Trial 337 pruned. \n",
      "[I 2023-12-16 20:07:24,936] Trial 338 pruned. \n",
      "[I 2023-12-16 20:07:26,335] Trial 339 pruned. \n",
      "[I 2023-12-16 20:07:29,643] Trial 340 pruned. \n",
      "[I 2023-12-16 20:07:31,530] Trial 341 pruned. \n",
      "[I 2023-12-16 20:07:54,993] Trial 342 finished with value: 7241.068359375 and parameters: {'hidden_size': 358, 'learning_rate': 0.0018630471767805753, 'dropout_rate': 0.39519968807955735}. Best is trial 57 with value: 7155.44873046875.\n",
      "[I 2023-12-16 20:08:17,539] Trial 343 finished with value: 7168.50439453125 and parameters: {'hidden_size': 349, 'learning_rate': 0.0014367106052738546, 'dropout_rate': 0.3893970768031569}. Best is trial 57 with value: 7155.44873046875.\n",
      "[I 2023-12-16 20:08:20,366] Trial 344 pruned. \n",
      "[I 2023-12-16 20:08:22,124] Trial 345 pruned. \n",
      "[I 2023-12-16 20:08:44,299] Trial 346 finished with value: 7152.4580078125 and parameters: {'hidden_size': 350, 'learning_rate': 0.0011527949270604273, 'dropout_rate': 0.38883900448608577}. Best is trial 346 with value: 7152.4580078125.\n",
      "[I 2023-12-16 20:08:45,694] Trial 347 pruned. \n",
      "[I 2023-12-16 20:08:47,159] Trial 348 pruned. \n",
      "[I 2023-12-16 20:09:09,795] Trial 349 finished with value: 7249.4072265625 and parameters: {'hidden_size': 351, 'learning_rate': 0.0009580257721926777, 'dropout_rate': 0.39297611377527936}. Best is trial 346 with value: 7152.4580078125.\n",
      "[I 2023-12-16 20:09:11,569] Trial 350 pruned. \n",
      "[I 2023-12-16 20:09:14,106] Trial 351 pruned. \n",
      "[I 2023-12-16 20:09:15,854] Trial 352 pruned. \n",
      "[I 2023-12-16 20:09:18,126] Trial 353 pruned. \n",
      "[I 2023-12-16 20:09:18,735] Trial 354 pruned. \n",
      "[I 2023-12-16 20:09:31,885] Trial 355 pruned. \n",
      "[I 2023-12-16 20:09:54,133] Trial 356 pruned. \n",
      "[I 2023-12-16 20:09:55,489] Trial 357 pruned. \n",
      "[I 2023-12-16 20:09:57,352] Trial 358 pruned. \n",
      "[I 2023-12-16 20:10:09,903] Trial 359 pruned. \n",
      "[I 2023-12-16 20:10:32,802] Trial 360 finished with value: 7270.16845703125 and parameters: {'hidden_size': 322, 'learning_rate': 0.0014235582787143369, 'dropout_rate': 0.45566406365397527}. Best is trial 346 with value: 7152.4580078125.\n",
      "[I 2023-12-16 20:10:33,695] Trial 361 pruned. \n",
      "[I 2023-12-16 20:10:35,758] Trial 362 pruned. \n",
      "[I 2023-12-16 20:10:50,479] Trial 363 finished with value: 7258.34326171875 and parameters: {'hidden_size': 222, 'learning_rate': 0.0017764540629175859, 'dropout_rate': 0.3462712406287369}. Best is trial 346 with value: 7152.4580078125.\n",
      "[I 2023-12-16 20:10:52,093] Trial 364 pruned. \n",
      "[I 2023-12-16 20:10:52,765] Trial 365 pruned. \n",
      "[I 2023-12-16 20:10:53,639] Trial 366 pruned. \n",
      "[I 2023-12-16 20:10:55,965] Trial 367 pruned. \n",
      "[I 2023-12-16 20:10:56,425] Trial 368 pruned. \n",
      "[I 2023-12-16 20:11:13,074] Trial 369 pruned. \n",
      "[I 2023-12-16 20:11:14,781] Trial 370 pruned. \n",
      "[I 2023-12-16 20:11:16,872] Trial 371 pruned. \n",
      "[I 2023-12-16 20:11:17,666] Trial 372 pruned. \n",
      "[I 2023-12-16 20:11:19,158] Trial 373 pruned. \n",
      "[I 2023-12-16 20:11:21,057] Trial 374 pruned. \n",
      "[I 2023-12-16 20:11:36,090] Trial 375 pruned. \n",
      "[I 2023-12-16 20:11:55,386] Trial 376 pruned. \n",
      "[I 2023-12-16 20:11:57,123] Trial 377 pruned. \n",
      "[I 2023-12-16 20:11:57,873] Trial 378 pruned. \n",
      "[I 2023-12-16 20:11:59,557] Trial 379 pruned. \n",
      "[I 2023-12-16 20:12:01,823] Trial 380 pruned. \n",
      "[I 2023-12-16 20:12:11,470] Trial 381 pruned. \n",
      "[I 2023-12-16 20:12:32,883] Trial 382 finished with value: 7324.0126953125 and parameters: {'hidden_size': 312, 'learning_rate': 0.0014712234999780145, 'dropout_rate': 0.3416219504226385}. Best is trial 346 with value: 7152.4580078125.\n",
      "[I 2023-12-16 20:12:46,676] Trial 383 finished with value: 7282.9599609375 and parameters: {'hidden_size': 184, 'learning_rate': 0.001680784984790047, 'dropout_rate': 0.4441712833637118}. Best is trial 346 with value: 7152.4580078125.\n",
      "[I 2023-12-16 20:12:47,233] Trial 384 pruned. \n",
      "[I 2023-12-16 20:12:49,537] Trial 385 pruned. \n",
      "[I 2023-12-16 20:12:50,202] Trial 386 pruned. \n",
      "[I 2023-12-16 20:12:51,721] Trial 387 pruned. \n",
      "[I 2023-12-16 20:13:03,480] Trial 388 pruned. \n",
      "[I 2023-12-16 20:13:04,542] Trial 389 pruned. \n",
      "[I 2023-12-16 20:13:11,020] Trial 390 pruned. \n",
      "[I 2023-12-16 20:13:23,616] Trial 391 finished with value: 7233.5537109375 and parameters: {'hidden_size': 157, 'learning_rate': 0.002414814640312608, 'dropout_rate': 0.4113462291469023}. Best is trial 346 with value: 7152.4580078125.\n",
      "[I 2023-12-16 20:13:35,774] Trial 392 finished with value: 7156.47412109375 and parameters: {'hidden_size': 160, 'learning_rate': 0.0025209335254701057, 'dropout_rate': 0.4095085990783396}. Best is trial 346 with value: 7152.4580078125.\n",
      "[I 2023-12-16 20:13:38,594] Trial 393 pruned. \n",
      "[I 2023-12-16 20:13:41,020] Trial 394 pruned. \n",
      "[I 2023-12-16 20:13:53,233] Trial 395 pruned. \n",
      "[I 2023-12-16 20:13:54,051] Trial 396 pruned. \n",
      "[I 2023-12-16 20:13:54,815] Trial 397 pruned. \n",
      "[I 2023-12-16 20:14:08,938] Trial 398 finished with value: 7239.08740234375 and parameters: {'hidden_size': 182, 'learning_rate': 0.0022430034359648982, 'dropout_rate': 0.47123985043246813}. Best is trial 346 with value: 7152.4580078125.\n",
      "[I 2023-12-16 20:14:22,653] Trial 399 finished with value: 7218.48828125 and parameters: {'hidden_size': 182, 'learning_rate': 0.002094652568148418, 'dropout_rate': 0.3216186993786504}. Best is trial 346 with value: 7152.4580078125.\n",
      "[I 2023-12-16 20:14:24,285] Trial 400 pruned. \n",
      "[I 2023-12-16 20:14:25,664] Trial 401 pruned. \n",
      "[I 2023-12-16 20:14:26,626] Trial 402 pruned. \n",
      "[I 2023-12-16 20:14:27,550] Trial 403 pruned. \n",
      "[I 2023-12-16 20:14:28,958] Trial 404 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500101008 eval_loss: 7152.4580078125\n",
      "500101008  Best hyperparameters: {'hidden_size': 350, 'learning_rate': 0.0011527949270604273, 'dropout_rate': 0.38883900448608577}\n",
      "epoch: 1, loss: 0.5863623348026793\n",
      "epoch: 2, loss: 0.48226090593665255\n",
      "epoch: 3, loss: 0.46875045472140764\n",
      "epoch: 4, loss: 0.46230423598807324\n",
      "epoch: 5, loss: 0.45905279796317694\n",
      "epoch: 6, loss: 0.4579665236523428\n",
      "epoch: 7, loss: 0.4563540199808046\n",
      "epoch: 8, loss: 0.453884843562703\n",
      "epoch: 9, loss: 0.4543899132520365\n",
      "epoch: 10, loss: 0.4549838260917462\n",
      "epoch: 11, loss: 0.45083322662573594\n",
      "epoch: 12, loss: 0.4523303858699842\n",
      "epoch: 13, loss: 0.4498910681874144\n",
      "epoch: 14, loss: 0.4508940954868251\n",
      "epoch: 15, loss: 0.45001195047505843\n",
      "epoch: 16, loss: 0.44947422427289624\n",
      "epoch: 17, loss: 0.45066361985594977\n",
      "epoch: 18, loss: 0.4511280492689814\n",
      "epoch: 19, loss: 0.4497517281707775\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-16 20:15:04,390] Using an existing study with name '500101009' instead of creating a new one.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 20, loss: 0.44855751366906577\n",
      "train <__main__.station_model object at 0x17aad53d0> done!\n",
      "get data loader for 500101009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-16 20:15:05,185] Trial 305 pruned. \n",
      "[I 2023-12-16 20:15:05,987] Trial 306 pruned. \n",
      "[I 2023-12-16 20:15:08,318] Trial 307 pruned. \n",
      "[I 2023-12-16 20:15:09,418] Trial 308 pruned. \n",
      "[I 2023-12-16 20:15:10,276] Trial 309 pruned. \n",
      "[I 2023-12-16 20:15:12,378] Trial 310 pruned. \n",
      "[I 2023-12-16 20:15:13,965] Trial 311 pruned. \n",
      "[I 2023-12-16 20:15:14,946] Trial 312 pruned. \n",
      "[I 2023-12-16 20:15:15,688] Trial 313 pruned. \n",
      "[I 2023-12-16 20:15:17,478] Trial 314 pruned. \n",
      "[I 2023-12-16 20:15:18,084] Trial 315 pruned. \n",
      "[I 2023-12-16 20:15:18,864] Trial 316 pruned. \n",
      "[I 2023-12-16 20:15:20,366] Trial 317 pruned. \n",
      "[I 2023-12-16 20:15:21,187] Trial 318 pruned. \n",
      "[I 2023-12-16 20:15:21,858] Trial 319 pruned. \n",
      "[I 2023-12-16 20:15:28,566] Trial 320 pruned. \n",
      "[I 2023-12-16 20:15:29,338] Trial 321 pruned. \n",
      "[I 2023-12-16 20:15:29,878] Trial 322 pruned. \n",
      "[I 2023-12-16 20:15:32,528] Trial 323 pruned. \n",
      "[I 2023-12-16 20:15:34,466] Trial 324 pruned. \n",
      "[I 2023-12-16 20:15:36,444] Trial 325 pruned. \n",
      "[I 2023-12-16 20:15:37,346] Trial 326 pruned. \n",
      "[I 2023-12-16 20:15:38,760] Trial 327 pruned. \n",
      "[I 2023-12-16 20:15:39,460] Trial 328 pruned. \n",
      "[I 2023-12-16 20:15:40,211] Trial 329 pruned. \n",
      "[I 2023-12-16 20:15:40,862] Trial 330 pruned. \n",
      "[I 2023-12-16 20:15:41,770] Trial 331 pruned. \n",
      "[I 2023-12-16 20:15:42,298] Trial 332 pruned. \n",
      "[I 2023-12-16 20:15:43,684] Trial 333 pruned. \n",
      "[I 2023-12-16 20:15:44,286] Trial 334 pruned. \n",
      "[I 2023-12-16 20:15:45,688] Trial 335 pruned. \n",
      "[I 2023-12-16 20:15:48,268] Trial 336 pruned. \n",
      "[I 2023-12-16 20:15:50,311] Trial 337 pruned. \n",
      "[I 2023-12-16 20:15:51,957] Trial 338 pruned. \n",
      "[I 2023-12-16 20:15:53,736] Trial 339 pruned. \n",
      "[I 2023-12-16 20:15:54,420] Trial 340 pruned. \n",
      "[I 2023-12-16 20:15:55,904] Trial 341 pruned. \n",
      "[I 2023-12-16 20:15:56,536] Trial 342 pruned. \n",
      "[I 2023-12-16 20:15:57,980] Trial 343 pruned. \n",
      "[I 2023-12-16 20:15:59,640] Trial 344 pruned. \n",
      "[I 2023-12-16 20:16:02,002] Trial 345 pruned. \n",
      "[I 2023-12-16 20:16:02,721] Trial 346 pruned. \n",
      "[I 2023-12-16 20:16:04,421] Trial 347 pruned. \n",
      "[I 2023-12-16 20:16:05,462] Trial 348 pruned. \n",
      "[I 2023-12-16 20:16:06,252] Trial 349 pruned. \n",
      "[I 2023-12-16 20:16:07,015] Trial 350 pruned. \n",
      "[I 2023-12-16 20:16:08,154] Trial 351 pruned. \n",
      "[I 2023-12-16 20:16:09,724] Trial 352 pruned. \n",
      "[I 2023-12-16 20:16:11,032] Trial 353 pruned. \n",
      "[I 2023-12-16 20:16:11,710] Trial 354 pruned. \n",
      "[I 2023-12-16 20:16:13,141] Trial 355 pruned. \n",
      "[I 2023-12-16 20:16:14,116] Trial 356 pruned. \n",
      "[I 2023-12-16 20:16:16,515] Trial 357 pruned. \n",
      "[I 2023-12-16 20:16:17,366] Trial 358 pruned. \n",
      "[I 2023-12-16 20:16:17,967] Trial 359 pruned. \n",
      "[I 2023-12-16 20:16:28,788] Trial 360 pruned. \n",
      "[I 2023-12-16 20:16:41,155] Trial 361 pruned. \n",
      "[I 2023-12-16 20:16:41,908] Trial 362 pruned. \n",
      "[I 2023-12-16 20:16:43,599] Trial 363 pruned. \n",
      "[I 2023-12-16 20:16:44,011] Trial 364 pruned. \n",
      "[I 2023-12-16 20:16:44,720] Trial 365 pruned. \n",
      "[I 2023-12-16 20:16:45,513] Trial 366 pruned. \n",
      "[I 2023-12-16 20:16:48,850] Trial 367 pruned. \n",
      "[I 2023-12-16 20:16:52,138] Trial 368 pruned. \n",
      "[I 2023-12-16 20:16:52,565] Trial 369 pruned. \n",
      "[I 2023-12-16 20:16:53,012] Trial 370 pruned. \n",
      "[I 2023-12-16 20:16:53,479] Trial 371 pruned. \n",
      "[I 2023-12-16 20:16:54,331] Trial 372 pruned. \n",
      "[I 2023-12-16 20:16:55,421] Trial 373 pruned. \n",
      "[I 2023-12-16 20:16:55,836] Trial 374 pruned. \n",
      "[I 2023-12-16 20:16:56,488] Trial 375 pruned. \n",
      "[I 2023-12-16 20:16:59,159] Trial 376 pruned. \n",
      "[I 2023-12-16 20:17:00,866] Trial 377 pruned. \n",
      "[I 2023-12-16 20:17:02,324] Trial 378 pruned. \n",
      "[I 2023-12-16 20:17:03,147] Trial 379 pruned. \n",
      "[I 2023-12-16 20:17:03,908] Trial 380 pruned. \n",
      "[I 2023-12-16 20:17:10,930] Trial 381 pruned. \n",
      "[I 2023-12-16 20:17:12,152] Trial 382 pruned. \n",
      "[I 2023-12-16 20:17:14,535] Trial 383 pruned. \n",
      "[I 2023-12-16 20:17:15,604] Trial 384 pruned. \n",
      "[I 2023-12-16 20:17:17,354] Trial 385 pruned. \n",
      "[I 2023-12-16 20:17:18,138] Trial 386 pruned. \n",
      "[I 2023-12-16 20:17:19,003] Trial 387 pruned. \n",
      "[I 2023-12-16 20:17:19,613] Trial 388 pruned. \n",
      "[I 2023-12-16 20:17:22,187] Trial 389 pruned. \n",
      "[I 2023-12-16 20:17:22,933] Trial 390 pruned. \n",
      "[I 2023-12-16 20:17:23,628] Trial 391 pruned. \n",
      "[I 2023-12-16 20:17:24,579] Trial 392 pruned. \n",
      "[I 2023-12-16 20:17:25,364] Trial 393 pruned. \n",
      "[I 2023-12-16 20:17:25,946] Trial 394 pruned. \n",
      "[I 2023-12-16 20:17:28,202] Trial 395 pruned. \n",
      "[I 2023-12-16 20:17:29,303] Trial 396 pruned. \n",
      "[I 2023-12-16 20:17:29,781] Trial 397 pruned. \n",
      "[I 2023-12-16 20:17:30,461] Trial 398 pruned. \n",
      "[I 2023-12-16 20:17:31,320] Trial 399 pruned. \n",
      "[I 2023-12-16 20:17:44,708] Trial 400 pruned. \n",
      "[I 2023-12-16 20:17:46,090] Trial 401 pruned. \n",
      "[I 2023-12-16 20:17:47,881] Trial 402 pruned. \n",
      "[I 2023-12-16 20:17:50,432] Trial 403 pruned. \n",
      "[I 2023-12-16 20:17:52,423] Trial 404 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500101009 eval_loss: 6478.22412109375\n",
      "500101009  Best hyperparameters: {'hidden_size': 135, 'learning_rate': 0.001848136147566704, 'dropout_rate': 0.2366009454820926}\n",
      "epoch: 1, loss: 0.5416398172240092\n",
      "epoch: 2, loss: 0.4642186750563741\n",
      "epoch: 3, loss: 0.4428506373999525\n",
      "epoch: 4, loss: 0.43874701228256857\n",
      "epoch: 5, loss: 0.4353274445008728\n",
      "epoch: 6, loss: 0.4310490517549745\n",
      "epoch: 7, loss: 0.4302962900421738\n",
      "epoch: 8, loss: 0.4299610522357946\n",
      "epoch: 9, loss: 0.4269188819158131\n",
      "epoch: 10, loss: 0.4272589091082324\n",
      "epoch: 11, loss: 0.4244975200875313\n",
      "epoch: 12, loss: 0.42512374932050345\n",
      "epoch: 13, loss: 0.4253724007045521\n",
      "epoch: 14, loss: 0.4229559459657511\n",
      "epoch: 15, loss: 0.42365946943224103\n",
      "epoch: 16, loss: 0.4235999215152648\n",
      "epoch: 17, loss: 0.42144868034043464\n",
      "epoch: 18, loss: 0.4219239732843359\n",
      "epoch: 19, loss: 0.4199219849943935\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-16 20:18:10,542] Using an existing study with name '500101010' instead of creating a new one.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 20, loss: 0.42108154438469747\n",
      "train <__main__.station_model object at 0x177bc8e50> done!\n",
      "get data loader for 500101010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-16 20:18:12,582] Trial 305 pruned. \n",
      "[I 2023-12-16 20:18:22,193] Trial 306 pruned. \n",
      "[I 2023-12-16 20:18:24,420] Trial 307 pruned. \n",
      "[I 2023-12-16 20:18:27,809] Trial 308 pruned. \n",
      "[I 2023-12-16 20:18:35,054] Trial 309 pruned. \n",
      "[I 2023-12-16 20:18:35,796] Trial 310 pruned. \n",
      "[I 2023-12-16 20:18:49,171] Trial 311 pruned. \n",
      "[I 2023-12-16 20:19:01,572] Trial 312 pruned. \n",
      "[I 2023-12-16 20:19:02,036] Trial 313 pruned. \n",
      "[I 2023-12-16 20:19:02,766] Trial 314 pruned. \n",
      "[I 2023-12-16 20:19:15,401] Trial 315 pruned. \n",
      "[I 2023-12-16 20:19:19,730] Trial 316 pruned. \n",
      "[I 2023-12-16 20:19:21,083] Trial 317 pruned. \n",
      "[I 2023-12-16 20:19:26,028] Trial 318 pruned. \n",
      "[I 2023-12-16 20:19:27,520] Trial 319 pruned. \n",
      "[I 2023-12-16 20:19:37,976] Trial 320 pruned. \n",
      "[I 2023-12-16 20:19:38,848] Trial 321 pruned. \n",
      "[I 2023-12-16 20:19:39,394] Trial 322 pruned. \n",
      "[I 2023-12-16 20:19:40,380] Trial 323 pruned. \n",
      "[I 2023-12-16 20:19:40,813] Trial 324 pruned. \n",
      "[I 2023-12-16 20:19:42,789] Trial 325 pruned. \n",
      "[I 2023-12-16 20:19:43,643] Trial 326 pruned. \n",
      "[I 2023-12-16 20:19:44,298] Trial 327 pruned. \n",
      "[I 2023-12-16 20:19:50,302] Trial 328 pruned. \n",
      "[I 2023-12-16 20:19:53,388] Trial 329 pruned. \n",
      "[I 2023-12-16 20:19:56,326] Trial 330 pruned. \n",
      "[I 2023-12-16 20:19:57,042] Trial 331 pruned. \n",
      "[I 2023-12-16 20:19:57,489] Trial 332 pruned. \n",
      "[I 2023-12-16 20:19:59,063] Trial 333 pruned. \n",
      "[I 2023-12-16 20:20:02,897] Trial 334 pruned. \n",
      "[I 2023-12-16 20:20:04,300] Trial 335 pruned. \n",
      "[I 2023-12-16 20:20:06,951] Trial 336 pruned. \n",
      "[I 2023-12-16 20:20:08,835] Trial 337 pruned. \n",
      "[I 2023-12-16 20:20:09,804] Trial 338 pruned. \n",
      "[I 2023-12-16 20:20:10,497] Trial 339 pruned. \n",
      "[I 2023-12-16 20:20:11,477] Trial 340 pruned. \n",
      "[I 2023-12-16 20:20:13,271] Trial 341 pruned. \n",
      "[I 2023-12-16 20:20:15,274] Trial 342 pruned. \n",
      "[I 2023-12-16 20:20:20,125] Trial 343 pruned. \n",
      "[I 2023-12-16 20:20:23,685] Trial 344 pruned. \n",
      "[I 2023-12-16 20:20:25,146] Trial 345 pruned. \n",
      "[I 2023-12-16 20:20:26,809] Trial 346 pruned. \n",
      "[I 2023-12-16 20:20:27,542] Trial 347 pruned. \n",
      "[I 2023-12-16 20:20:28,169] Trial 348 pruned. \n",
      "[I 2023-12-16 20:20:30,749] Trial 349 pruned. \n",
      "[I 2023-12-16 20:20:46,331] Trial 350 pruned. \n",
      "[I 2023-12-16 20:20:47,198] Trial 351 pruned. \n",
      "[I 2023-12-16 20:20:51,752] Trial 352 pruned. \n",
      "[I 2023-12-16 20:21:18,069] Trial 353 pruned. \n",
      "[I 2023-12-16 20:21:19,651] Trial 354 pruned. \n",
      "[I 2023-12-16 20:21:34,665] Trial 355 pruned. \n",
      "[I 2023-12-16 20:21:38,512] Trial 356 pruned. \n",
      "[I 2023-12-16 20:21:41,722] Trial 357 pruned. \n",
      "[I 2023-12-16 20:21:45,218] Trial 358 pruned. \n",
      "[I 2023-12-16 20:21:46,821] Trial 359 pruned. \n",
      "[I 2023-12-16 20:21:49,539] Trial 360 pruned. \n",
      "[I 2023-12-16 20:21:50,722] Trial 361 pruned. \n",
      "[I 2023-12-16 20:21:57,317] Trial 362 pruned. \n",
      "[I 2023-12-16 20:22:05,878] Trial 363 pruned. \n",
      "[I 2023-12-16 20:22:06,803] Trial 364 pruned. \n",
      "[I 2023-12-16 20:22:08,155] Trial 365 pruned. \n",
      "[I 2023-12-16 20:22:13,765] Trial 366 pruned. \n",
      "[I 2023-12-16 20:22:14,333] Trial 367 pruned. \n",
      "[I 2023-12-16 20:22:15,069] Trial 368 pruned. \n",
      "[I 2023-12-16 20:22:18,271] Trial 369 pruned. \n",
      "[I 2023-12-16 20:22:20,351] Trial 370 pruned. \n",
      "[I 2023-12-16 20:22:22,302] Trial 371 pruned. \n",
      "[I 2023-12-16 20:22:30,875] Trial 372 pruned. \n",
      "[I 2023-12-16 20:22:50,169] Trial 373 pruned. \n",
      "[I 2023-12-16 20:22:55,176] Trial 374 pruned. \n",
      "[I 2023-12-16 20:22:57,309] Trial 375 pruned. \n",
      "[I 2023-12-16 20:22:59,472] Trial 376 pruned. \n",
      "[I 2023-12-16 20:23:16,065] Trial 377 pruned. \n",
      "[I 2023-12-16 20:23:16,643] Trial 378 pruned. \n",
      "[I 2023-12-16 20:23:22,192] Trial 379 pruned. \n",
      "[I 2023-12-16 20:23:23,677] Trial 380 pruned. \n",
      "[I 2023-12-16 20:23:28,790] Trial 381 pruned. \n",
      "[I 2023-12-16 20:23:35,068] Trial 382 pruned. \n",
      "[I 2023-12-16 20:23:36,151] Trial 383 pruned. \n",
      "[I 2023-12-16 20:24:14,551] Trial 384 pruned. \n",
      "[I 2023-12-16 20:24:15,151] Trial 385 pruned. \n",
      "[I 2023-12-16 20:24:15,798] Trial 386 pruned. \n",
      "[I 2023-12-16 20:24:16,456] Trial 387 pruned. \n",
      "[I 2023-12-16 20:24:20,481] Trial 388 pruned. \n",
      "[I 2023-12-16 20:24:22,067] Trial 389 pruned. \n",
      "[I 2023-12-16 20:24:28,022] Trial 390 pruned. \n",
      "[I 2023-12-16 20:24:28,836] Trial 391 pruned. \n",
      "[I 2023-12-16 20:24:29,370] Trial 392 pruned. \n",
      "[I 2023-12-16 20:24:29,956] Trial 393 pruned. \n",
      "[I 2023-12-16 20:24:30,500] Trial 394 pruned. \n",
      "[I 2023-12-16 20:24:37,884] Trial 395 pruned. \n",
      "[I 2023-12-16 20:24:41,165] Trial 396 pruned. \n",
      "[I 2023-12-16 20:24:44,934] Trial 397 pruned. \n",
      "[I 2023-12-16 20:24:45,458] Trial 398 pruned. \n",
      "[I 2023-12-16 20:24:48,362] Trial 399 pruned. \n",
      "[I 2023-12-16 20:25:09,360] Trial 400 pruned. \n",
      "[I 2023-12-16 20:25:16,351] Trial 401 pruned. \n",
      "[I 2023-12-16 20:25:20,579] Trial 402 pruned. \n",
      "[I 2023-12-16 20:25:40,519] Trial 403 pruned. \n",
      "[I 2023-12-16 20:25:42,689] Trial 404 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500101010 eval_loss: 6424.9736328125\n",
      "500101010  Best hyperparameters: {'hidden_size': 705, 'learning_rate': 0.0008842352648663706, 'dropout_rate': 0.265999554807392}\n",
      "epoch: 1, loss: 0.6063752268549184\n",
      "epoch: 2, loss: 0.4858998987334886\n",
      "epoch: 3, loss: 0.4524546780737277\n",
      "epoch: 4, loss: 0.4437112414234544\n",
      "epoch: 5, loss: 0.4369695064573806\n",
      "epoch: 6, loss: 0.4340178151565799\n",
      "epoch: 7, loss: 0.43204610148616146\n",
      "epoch: 8, loss: 0.4288789834911467\n",
      "epoch: 9, loss: 0.42810103387135007\n",
      "epoch: 10, loss: 0.4251355486577812\n",
      "epoch: 11, loss: 0.42541149224900554\n",
      "epoch: 12, loss: 0.42353632326938806\n",
      "epoch: 13, loss: 0.4209040120401354\n",
      "epoch: 14, loss: 0.4199781498533209\n",
      "epoch: 15, loss: 0.42024032668200734\n",
      "epoch: 16, loss: 0.4190035750513523\n",
      "epoch: 17, loss: 0.417499955648031\n",
      "epoch: 18, loss: 0.41630500096541184\n",
      "epoch: 19, loss: 0.41894007007820394\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-16 20:27:07,244] Using an existing study with name '500101013' instead of creating a new one.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 20, loss: 0.41686733055798\n",
      "train <__main__.station_model object at 0x108d60190> done!\n",
      "get data loader for 500101013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-16 20:27:10,479] Trial 275 pruned. \n",
      "[I 2023-12-16 20:27:13,045] Trial 276 pruned. \n",
      "[I 2023-12-16 20:27:14,099] Trial 277 pruned. \n",
      "[I 2023-12-16 20:27:14,918] Trial 278 pruned. \n",
      "[I 2023-12-16 20:27:16,376] Trial 279 pruned. \n",
      "[I 2023-12-16 20:27:17,055] Trial 280 pruned. \n",
      "[I 2023-12-16 20:27:19,051] Trial 281 pruned. \n",
      "[I 2023-12-16 20:27:20,365] Trial 282 pruned. \n",
      "[I 2023-12-16 20:27:21,276] Trial 283 pruned. \n",
      "[I 2023-12-16 20:27:22,835] Trial 284 pruned. \n",
      "[I 2023-12-16 20:27:24,169] Trial 285 pruned. \n",
      "[I 2023-12-16 20:27:24,891] Trial 286 pruned. \n",
      "[I 2023-12-16 20:27:26,602] Trial 287 pruned. \n",
      "[I 2023-12-16 20:27:27,723] Trial 288 pruned. \n",
      "[I 2023-12-16 20:27:28,203] Trial 289 pruned. \n",
      "[I 2023-12-16 20:27:29,743] Trial 290 pruned. \n",
      "[I 2023-12-16 20:27:31,979] Trial 291 pruned. \n",
      "[I 2023-12-16 20:27:32,814] Trial 292 pruned. \n",
      "[I 2023-12-16 20:27:34,207] Trial 293 pruned. \n",
      "[I 2023-12-16 20:27:37,724] Trial 294 pruned. \n",
      "[I 2023-12-16 20:27:38,750] Trial 295 pruned. \n",
      "[I 2023-12-16 20:27:41,847] Trial 296 pruned. \n",
      "[I 2023-12-16 20:27:43,406] Trial 297 pruned. \n",
      "[I 2023-12-16 20:27:44,216] Trial 298 pruned. \n",
      "[I 2023-12-16 20:27:45,269] Trial 299 pruned. \n",
      "[I 2023-12-16 20:27:47,598] Trial 300 pruned. \n",
      "[I 2023-12-16 20:27:49,662] Trial 301 pruned. \n",
      "[I 2023-12-16 20:27:52,332] Trial 302 pruned. \n",
      "[I 2023-12-16 20:27:53,828] Trial 303 pruned. \n",
      "[I 2023-12-16 20:27:54,996] Trial 304 pruned. \n",
      "[I 2023-12-16 20:27:56,036] Trial 305 pruned. \n",
      "[I 2023-12-16 20:27:57,721] Trial 306 pruned. \n",
      "[I 2023-12-16 20:27:58,331] Trial 307 pruned. \n",
      "[I 2023-12-16 20:27:59,629] Trial 308 pruned. \n",
      "[I 2023-12-16 20:28:00,624] Trial 309 pruned. \n",
      "[I 2023-12-16 20:28:02,067] Trial 310 pruned. \n",
      "[I 2023-12-16 20:28:05,438] Trial 311 pruned. \n",
      "[I 2023-12-16 20:28:07,739] Trial 312 pruned. \n",
      "[I 2023-12-16 20:28:08,488] Trial 313 pruned. \n",
      "[I 2023-12-16 20:28:09,999] Trial 314 pruned. \n",
      "[I 2023-12-16 20:28:11,691] Trial 315 pruned. \n",
      "[I 2023-12-16 20:28:12,725] Trial 316 pruned. \n",
      "[I 2023-12-16 20:28:17,130] Trial 317 pruned. \n",
      "[I 2023-12-16 20:28:18,479] Trial 318 pruned. \n",
      "[I 2023-12-16 20:28:20,523] Trial 319 pruned. \n",
      "[I 2023-12-16 20:28:34,644] Trial 320 pruned. \n",
      "[I 2023-12-16 20:28:36,397] Trial 321 pruned. \n",
      "[I 2023-12-16 20:28:37,192] Trial 322 pruned. \n",
      "[I 2023-12-16 20:28:37,794] Trial 323 pruned. \n",
      "[I 2023-12-16 20:28:39,072] Trial 324 pruned. \n",
      "[I 2023-12-16 20:28:40,571] Trial 325 pruned. \n",
      "[I 2023-12-16 20:28:41,789] Trial 326 pruned. \n",
      "[I 2023-12-16 20:28:45,314] Trial 327 pruned. \n",
      "[I 2023-12-16 20:28:46,092] Trial 328 pruned. \n",
      "[I 2023-12-16 20:28:46,654] Trial 329 pruned. \n",
      "[I 2023-12-16 20:28:48,585] Trial 330 pruned. \n",
      "[I 2023-12-16 20:28:50,266] Trial 331 pruned. \n",
      "[I 2023-12-16 20:28:51,762] Trial 332 pruned. \n",
      "[I 2023-12-16 20:28:52,814] Trial 333 pruned. \n",
      "[I 2023-12-16 20:28:53,441] Trial 334 pruned. \n",
      "[I 2023-12-16 20:28:54,729] Trial 335 pruned. \n",
      "[I 2023-12-16 20:28:55,681] Trial 336 pruned. \n",
      "[I 2023-12-16 20:28:57,407] Trial 337 pruned. \n",
      "[I 2023-12-16 20:28:58,106] Trial 338 pruned. \n",
      "[I 2023-12-16 20:29:00,345] Trial 339 pruned. \n",
      "[I 2023-12-16 20:29:02,041] Trial 340 pruned. \n",
      "[I 2023-12-16 20:29:05,468] Trial 341 pruned. \n",
      "[I 2023-12-16 20:29:07,934] Trial 342 pruned. \n",
      "[I 2023-12-16 20:29:09,180] Trial 343 pruned. \n",
      "[I 2023-12-16 20:29:09,993] Trial 344 pruned. \n",
      "[I 2023-12-16 20:29:10,983] Trial 345 pruned. \n",
      "[I 2023-12-16 20:29:11,407] Trial 346 pruned. \n",
      "[I 2023-12-16 20:29:11,815] Trial 347 pruned. \n",
      "[I 2023-12-16 20:29:13,613] Trial 348 pruned. \n",
      "[I 2023-12-16 20:29:15,066] Trial 349 pruned. \n",
      "[I 2023-12-16 20:29:15,997] Trial 350 pruned. \n",
      "[I 2023-12-16 20:29:21,311] Trial 351 pruned. \n",
      "[I 2023-12-16 20:29:22,775] Trial 352 pruned. \n",
      "[I 2023-12-16 20:29:25,157] Trial 353 pruned. \n",
      "[I 2023-12-16 20:29:26,769] Trial 354 pruned. \n",
      "[I 2023-12-16 20:29:28,848] Trial 355 pruned. \n",
      "[I 2023-12-16 20:29:36,738] Trial 356 pruned. \n",
      "[I 2023-12-16 20:29:38,085] Trial 357 pruned. \n",
      "[I 2023-12-16 20:29:39,167] Trial 358 pruned. \n",
      "[I 2023-12-16 20:29:40,716] Trial 359 pruned. \n",
      "[I 2023-12-16 20:29:41,223] Trial 360 pruned. \n",
      "[I 2023-12-16 20:29:41,966] Trial 361 pruned. \n",
      "[I 2023-12-16 20:29:52,441] Trial 362 pruned. \n",
      "[I 2023-12-16 20:30:12,521] Trial 363 pruned. \n",
      "[I 2023-12-16 20:30:13,418] Trial 364 pruned. \n",
      "[I 2023-12-16 20:30:14,917] Trial 365 pruned. \n",
      "[I 2023-12-16 20:30:17,749] Trial 366 pruned. \n",
      "[I 2023-12-16 20:30:18,775] Trial 367 pruned. \n",
      "[I 2023-12-16 20:30:20,888] Trial 368 pruned. \n",
      "[I 2023-12-16 20:30:21,677] Trial 369 pruned. \n",
      "[I 2023-12-16 20:30:22,947] Trial 370 pruned. \n",
      "[I 2023-12-16 20:30:23,416] Trial 371 pruned. \n",
      "[I 2023-12-16 20:30:25,179] Trial 372 pruned. \n",
      "[I 2023-12-16 20:30:26,677] Trial 373 pruned. \n",
      "[I 2023-12-16 20:30:29,591] Trial 374 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500101013 eval_loss: 6597.4921875\n",
      "500101013  Best hyperparameters: {'hidden_size': 208, 'learning_rate': 0.0004360857458312358, 'dropout_rate': 0.42189096967136797}\n",
      "epoch: 1, loss: 0.5229004816816583\n",
      "epoch: 2, loss: 0.4970358508115081\n",
      "epoch: 3, loss: 0.4817516222303989\n",
      "epoch: 4, loss: 0.4712020037176023\n",
      "epoch: 5, loss: 0.46201272556055367\n",
      "epoch: 6, loss: 0.4549996849728206\n",
      "epoch: 7, loss: 0.44485574178566223\n",
      "epoch: 8, loss: 0.4383763374246623\n",
      "epoch: 9, loss: 0.43422749613834544\n",
      "epoch: 10, loss: 0.43219584831195357\n",
      "epoch: 11, loss: 0.4306867764096814\n",
      "epoch: 12, loss: 0.4274881176684237\n",
      "epoch: 13, loss: 0.4267234447135465\n",
      "epoch: 14, loss: 0.42468561474823846\n",
      "epoch: 15, loss: 0.42418387496273624\n",
      "epoch: 16, loss: 0.4240719155263757\n",
      "epoch: 17, loss: 0.42402127322208466\n",
      "epoch: 18, loss: 0.42220901465254135\n",
      "epoch: 19, loss: 0.42128923200464174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-16 20:30:50,979] Using an existing study with name '500101014' instead of creating a new one.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 20, loss: 0.42208235547912965\n",
      "train <__main__.station_model object at 0x17d901cd0> done!\n",
      "get data loader for 500101014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-16 20:30:53,403] Trial 205 pruned. \n",
      "[I 2023-12-16 20:30:54,875] Trial 206 pruned. \n",
      "[I 2023-12-16 20:31:05,309] Trial 207 pruned. \n",
      "[I 2023-12-16 20:31:08,989] Trial 208 pruned. \n",
      "[I 2023-12-16 20:31:09,809] Trial 209 pruned. \n",
      "[I 2023-12-16 20:32:53,375] Trial 210 pruned. \n",
      "[I 2023-12-16 20:33:24,018] Trial 211 pruned. \n",
      "[I 2023-12-16 20:33:26,327] Trial 212 pruned. \n",
      "[I 2023-12-16 20:33:28,330] Trial 213 pruned. \n",
      "[I 2023-12-16 20:33:31,720] Trial 214 pruned. \n",
      "[I 2023-12-16 20:33:34,379] Trial 215 pruned. \n",
      "[I 2023-12-16 20:33:51,380] Trial 216 pruned. \n",
      "[I 2023-12-16 20:34:02,425] Trial 217 pruned. \n",
      "[I 2023-12-16 20:34:04,292] Trial 218 pruned. \n",
      "[I 2023-12-16 20:34:12,775] Trial 219 pruned. \n",
      "[I 2023-12-16 20:34:15,224] Trial 220 pruned. \n",
      "[I 2023-12-16 20:34:16,592] Trial 221 pruned. \n",
      "[I 2023-12-16 20:34:17,597] Trial 222 pruned. \n",
      "[I 2023-12-16 20:34:18,634] Trial 223 pruned. \n",
      "[I 2023-12-16 20:34:20,115] Trial 224 pruned. \n",
      "[I 2023-12-16 20:34:21,573] Trial 225 pruned. \n",
      "[I 2023-12-16 20:34:22,019] Trial 226 pruned. \n",
      "[I 2023-12-16 20:34:25,567] Trial 227 pruned. \n",
      "[I 2023-12-16 20:34:27,726] Trial 228 pruned. \n",
      "[I 2023-12-16 20:35:19,157] Trial 229 pruned. \n",
      "[I 2023-12-16 20:35:19,869] Trial 230 pruned. \n",
      "[I 2023-12-16 20:35:38,665] Trial 231 pruned. \n",
      "[I 2023-12-16 20:35:40,752] Trial 232 pruned. \n",
      "[I 2023-12-16 20:35:41,476] Trial 233 pruned. \n",
      "[I 2023-12-16 20:36:26,943] Trial 234 pruned. \n",
      "[I 2023-12-16 20:39:08,196] Trial 235 pruned. \n",
      "[I 2023-12-16 20:39:20,682] Trial 236 pruned. \n",
      "[I 2023-12-16 20:39:22,708] Trial 237 pruned. \n",
      "[I 2023-12-16 20:39:24,452] Trial 238 pruned. \n",
      "[I 2023-12-16 20:39:43,096] Trial 239 pruned. \n",
      "[I 2023-12-16 20:39:45,084] Trial 240 pruned. \n",
      "[I 2023-12-16 20:39:45,966] Trial 241 pruned. \n",
      "[I 2023-12-16 20:39:46,470] Trial 242 pruned. \n",
      "[I 2023-12-16 20:39:46,985] Trial 243 pruned. \n",
      "[I 2023-12-16 20:39:51,182] Trial 244 pruned. \n",
      "[I 2023-12-16 20:39:51,628] Trial 245 pruned. \n",
      "[I 2023-12-16 20:39:52,062] Trial 246 pruned. \n",
      "[I 2023-12-16 20:39:52,728] Trial 247 pruned. \n",
      "[I 2023-12-16 20:39:53,453] Trial 248 pruned. \n",
      "[I 2023-12-16 20:39:53,886] Trial 249 pruned. \n",
      "[I 2023-12-16 20:39:54,468] Trial 250 pruned. \n",
      "[I 2023-12-16 20:39:55,135] Trial 251 pruned. \n",
      "[I 2023-12-16 20:39:56,637] Trial 252 pruned. \n",
      "[I 2023-12-16 20:39:57,712] Trial 253 pruned. \n",
      "[I 2023-12-16 20:39:59,613] Trial 254 pruned. \n",
      "[I 2023-12-16 20:40:00,075] Trial 255 pruned. \n",
      "[I 2023-12-16 20:40:10,757] Trial 256 pruned. \n",
      "[I 2023-12-16 20:40:11,231] Trial 257 pruned. \n",
      "[I 2023-12-16 20:40:12,574] Trial 258 pruned. \n",
      "[I 2023-12-16 20:40:14,312] Trial 259 pruned. \n",
      "[I 2023-12-16 20:40:19,168] Trial 260 pruned. \n",
      "[I 2023-12-16 20:40:19,791] Trial 261 pruned. \n",
      "[I 2023-12-16 20:40:20,631] Trial 262 pruned. \n",
      "[I 2023-12-16 20:40:21,213] Trial 263 pruned. \n",
      "[I 2023-12-16 20:40:26,092] Trial 264 pruned. \n",
      "[I 2023-12-16 20:40:27,099] Trial 265 pruned. \n",
      "[I 2023-12-16 20:40:27,764] Trial 266 pruned. \n",
      "[I 2023-12-16 20:40:28,252] Trial 267 pruned. \n",
      "[I 2023-12-16 20:40:55,213] Trial 268 pruned. \n",
      "[I 2023-12-16 20:40:56,161] Trial 269 pruned. \n",
      "[I 2023-12-16 20:40:57,606] Trial 270 pruned. \n",
      "[I 2023-12-16 20:41:04,380] Trial 271 pruned. \n",
      "[I 2023-12-16 20:41:07,016] Trial 272 pruned. \n",
      "[I 2023-12-16 20:41:07,497] Trial 273 pruned. \n",
      "[I 2023-12-16 20:41:33,347] Trial 274 pruned. \n",
      "[I 2023-12-16 20:41:37,181] Trial 275 pruned. \n",
      "[I 2023-12-16 20:41:37,877] Trial 276 pruned. \n",
      "[I 2023-12-16 20:41:46,129] Trial 277 pruned. \n",
      "[I 2023-12-16 20:41:47,637] Trial 278 pruned. \n",
      "[I 2023-12-16 20:41:51,153] Trial 279 pruned. \n",
      "[I 2023-12-16 20:41:51,589] Trial 280 pruned. \n",
      "[I 2023-12-16 20:41:52,308] Trial 281 pruned. \n",
      "[I 2023-12-16 20:41:52,808] Trial 282 pruned. \n",
      "[I 2023-12-16 20:42:15,237] Trial 283 pruned. \n",
      "[I 2023-12-16 20:42:15,801] Trial 284 pruned. \n",
      "[I 2023-12-16 20:42:16,258] Trial 285 pruned. \n",
      "[I 2023-12-16 20:42:16,794] Trial 286 pruned. \n",
      "[I 2023-12-16 20:42:18,836] Trial 287 pruned. \n",
      "[I 2023-12-16 20:42:19,918] Trial 288 pruned. \n",
      "[I 2023-12-16 20:42:20,413] Trial 289 pruned. \n",
      "[I 2023-12-16 20:43:14,829] Trial 290 pruned. \n",
      "[I 2023-12-16 20:43:15,534] Trial 291 pruned. \n",
      "[I 2023-12-16 20:43:18,037] Trial 292 pruned. \n",
      "[I 2023-12-16 20:43:29,788] Trial 293 pruned. \n",
      "[I 2023-12-16 20:43:30,319] Trial 294 pruned. \n",
      "[I 2023-12-16 20:43:30,894] Trial 295 pruned. \n",
      "[I 2023-12-16 20:43:34,232] Trial 296 pruned. \n",
      "[I 2023-12-16 20:43:49,570] Trial 297 pruned. \n",
      "[I 2023-12-16 20:43:50,240] Trial 298 pruned. \n",
      "[I 2023-12-16 20:43:52,009] Trial 299 pruned. \n",
      "[I 2023-12-16 20:43:52,811] Trial 300 pruned. \n",
      "[I 2023-12-16 20:44:24,392] Trial 301 pruned. \n",
      "[I 2023-12-16 20:45:52,401] Trial 302 pruned. \n",
      "[I 2023-12-16 20:46:18,645] Trial 303 pruned. \n",
      "[I 2023-12-16 20:46:19,422] Trial 304 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500101014 eval_loss: 6963.8759765625\n",
      "500101014  Best hyperparameters: {'hidden_size': 1403, 'learning_rate': 0.000548578883021796, 'dropout_rate': 0.3757852508886575}\n",
      "epoch: 1, loss: 0.520043669108352\n",
      "epoch: 2, loss: 0.4911984387521111\n",
      "epoch: 3, loss: 0.4712107867047438\n",
      "epoch: 4, loss: 0.46268681031096937\n",
      "epoch: 5, loss: 0.45896633992008135\n",
      "epoch: 6, loss: 0.4556743778839011\n",
      "epoch: 7, loss: 0.4533985036890251\n",
      "epoch: 8, loss: 0.45114303111670423\n",
      "epoch: 9, loss: 0.4490272852944752\n",
      "epoch: 10, loss: 0.44916375997289454\n",
      "epoch: 11, loss: 0.4471449413720299\n",
      "epoch: 12, loss: 0.4481903981434274\n",
      "epoch: 13, loss: 0.444318360737546\n",
      "epoch: 14, loss: 0.4442888596748335\n",
      "epoch: 15, loss: 0.44394611644798815\n",
      "epoch: 16, loss: 0.444127734969644\n",
      "epoch: 17, loss: 0.4428003267644937\n",
      "epoch: 18, loss: 0.4423726347596757\n",
      "epoch: 19, loss: 0.442309977422113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-16 20:51:24,387] Using an existing study with name '500101018' instead of creating a new one.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 20, loss: 0.4411779221591187\n",
      "train <__main__.station_model object at 0x11deb6a90> done!\n",
      "get data loader for 500101018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-16 20:51:24,991] Trial 205 pruned. \n",
      "[I 2023-12-16 20:51:33,878] Trial 206 pruned. \n",
      "[I 2023-12-16 20:51:40,391] Trial 207 pruned. \n",
      "[I 2023-12-16 20:51:41,443] Trial 208 pruned. \n",
      "[I 2023-12-16 20:51:44,979] Trial 209 pruned. \n",
      "[I 2023-12-16 20:51:47,364] Trial 210 pruned. \n",
      "[I 2023-12-16 20:51:48,009] Trial 211 pruned. \n",
      "[I 2023-12-16 20:51:48,776] Trial 212 pruned. \n",
      "[I 2023-12-16 20:51:55,932] Trial 213 pruned. \n",
      "[I 2023-12-16 20:51:57,287] Trial 214 pruned. \n",
      "[I 2023-12-16 20:51:58,921] Trial 215 pruned. \n",
      "[I 2023-12-16 20:52:00,018] Trial 216 pruned. \n",
      "[I 2023-12-16 20:52:14,362] Trial 217 pruned. \n",
      "[I 2023-12-16 20:52:37,528] Trial 218 pruned. \n",
      "[I 2023-12-16 20:52:39,040] Trial 219 pruned. \n",
      "[I 2023-12-16 20:52:40,039] Trial 220 pruned. \n",
      "[I 2023-12-16 20:52:40,915] Trial 221 pruned. \n",
      "[I 2023-12-16 20:52:41,541] Trial 222 pruned. \n",
      "[I 2023-12-16 20:52:42,566] Trial 223 pruned. \n",
      "[I 2023-12-16 20:52:43,231] Trial 224 pruned. \n",
      "[I 2023-12-16 20:52:43,836] Trial 225 pruned. \n",
      "[I 2023-12-16 20:52:44,645] Trial 226 pruned. \n",
      "[I 2023-12-16 20:52:46,378] Trial 227 pruned. \n",
      "[I 2023-12-16 20:52:47,087] Trial 228 pruned. \n",
      "[I 2023-12-16 20:52:48,399] Trial 229 pruned. \n",
      "[I 2023-12-16 20:52:49,922] Trial 230 pruned. \n",
      "[I 2023-12-16 20:52:50,785] Trial 231 pruned. \n",
      "[I 2023-12-16 20:52:51,701] Trial 232 pruned. \n",
      "[I 2023-12-16 20:52:52,504] Trial 233 pruned. \n",
      "[I 2023-12-16 20:52:53,615] Trial 234 pruned. \n",
      "[I 2023-12-16 20:52:54,285] Trial 235 pruned. \n",
      "[I 2023-12-16 20:52:55,251] Trial 236 pruned. \n",
      "[I 2023-12-16 20:53:04,081] Trial 237 pruned. \n",
      "[I 2023-12-16 20:53:08,832] Trial 238 pruned. \n",
      "[I 2023-12-16 20:53:25,917] Trial 239 pruned. \n",
      "[I 2023-12-16 20:53:27,398] Trial 240 pruned. \n",
      "[I 2023-12-16 20:53:28,339] Trial 241 pruned. \n",
      "[I 2023-12-16 20:53:29,380] Trial 242 pruned. \n",
      "[I 2023-12-16 20:53:30,206] Trial 243 pruned. \n",
      "[I 2023-12-16 20:53:31,153] Trial 244 pruned. \n",
      "[I 2023-12-16 20:53:32,455] Trial 245 pruned. \n",
      "[I 2023-12-16 20:53:33,187] Trial 246 pruned. \n",
      "[I 2023-12-16 20:53:33,617] Trial 247 pruned. \n",
      "[I 2023-12-16 20:53:38,785] Trial 248 pruned. \n",
      "[I 2023-12-16 20:53:39,425] Trial 249 pruned. \n",
      "[I 2023-12-16 20:53:41,531] Trial 250 pruned. \n",
      "[I 2023-12-16 20:53:44,998] Trial 251 pruned. \n",
      "[I 2023-12-16 20:53:58,789] Trial 252 pruned. \n",
      "[I 2023-12-16 20:53:59,609] Trial 253 pruned. \n",
      "[I 2023-12-16 20:54:00,761] Trial 254 pruned. \n",
      "[I 2023-12-16 20:54:01,648] Trial 255 pruned. \n",
      "[I 2023-12-16 20:54:02,621] Trial 256 pruned. \n",
      "[I 2023-12-16 20:54:04,160] Trial 257 pruned. \n",
      "[I 2023-12-16 20:54:08,832] Trial 258 pruned. \n",
      "[I 2023-12-16 20:54:10,319] Trial 259 pruned. \n",
      "[I 2023-12-16 20:54:11,183] Trial 260 pruned. \n",
      "[I 2023-12-16 20:54:13,760] Trial 261 pruned. \n",
      "[I 2023-12-16 20:54:20,142] Trial 262 pruned. \n",
      "[I 2023-12-16 20:54:20,864] Trial 263 pruned. \n",
      "[I 2023-12-16 20:54:21,455] Trial 264 pruned. \n",
      "[I 2023-12-16 20:54:22,000] Trial 265 pruned. \n",
      "[I 2023-12-16 20:54:22,525] Trial 266 pruned. \n",
      "[I 2023-12-16 20:54:23,523] Trial 267 pruned. \n",
      "[I 2023-12-16 20:54:24,530] Trial 268 pruned. \n",
      "[I 2023-12-16 20:54:44,126] Trial 269 pruned. \n",
      "[I 2023-12-16 20:54:44,856] Trial 270 pruned. \n",
      "[I 2023-12-16 20:54:46,333] Trial 271 pruned. \n",
      "[I 2023-12-16 20:54:49,187] Trial 272 pruned. \n",
      "[I 2023-12-16 20:54:50,498] Trial 273 pruned. \n",
      "[I 2023-12-16 20:54:51,127] Trial 274 pruned. \n",
      "[I 2023-12-16 20:54:51,891] Trial 275 pruned. \n",
      "[I 2023-12-16 20:54:52,743] Trial 276 pruned. \n",
      "[I 2023-12-16 20:55:00,768] Trial 277 pruned. \n",
      "[I 2023-12-16 20:55:02,448] Trial 278 pruned. \n",
      "[I 2023-12-16 20:55:04,441] Trial 279 pruned. \n",
      "[I 2023-12-16 20:55:06,066] Trial 280 pruned. \n",
      "[I 2023-12-16 20:55:13,711] Trial 281 pruned. \n",
      "[I 2023-12-16 20:55:14,205] Trial 282 pruned. \n",
      "[I 2023-12-16 20:55:14,701] Trial 283 pruned. \n",
      "[I 2023-12-16 20:55:16,922] Trial 284 pruned. \n",
      "[I 2023-12-16 20:55:27,643] Trial 285 pruned. \n",
      "[I 2023-12-16 20:55:30,349] Trial 286 pruned. \n",
      "[I 2023-12-16 20:55:31,666] Trial 287 pruned. \n",
      "[I 2023-12-16 20:55:33,402] Trial 288 pruned. \n",
      "[I 2023-12-16 20:55:34,024] Trial 289 pruned. \n",
      "[I 2023-12-16 20:55:34,518] Trial 290 pruned. \n",
      "[I 2023-12-16 20:55:36,892] Trial 291 pruned. \n",
      "[I 2023-12-16 20:55:37,564] Trial 292 pruned. \n",
      "[I 2023-12-16 20:55:38,577] Trial 293 pruned. \n",
      "[I 2023-12-16 20:55:39,154] Trial 294 pruned. \n",
      "[I 2023-12-16 20:55:40,692] Trial 295 pruned. \n",
      "[I 2023-12-16 20:55:41,240] Trial 296 pruned. \n",
      "[I 2023-12-16 20:55:53,916] Trial 297 pruned. \n",
      "[I 2023-12-16 20:55:55,000] Trial 298 pruned. \n",
      "[I 2023-12-16 20:56:04,648] Trial 299 pruned. \n",
      "[I 2023-12-16 20:56:26,866] Trial 300 pruned. \n",
      "[I 2023-12-16 20:56:28,383] Trial 301 pruned. \n",
      "[I 2023-12-16 20:56:29,376] Trial 302 pruned. \n",
      "[I 2023-12-16 20:56:30,765] Trial 303 pruned. \n",
      "[I 2023-12-16 20:56:36,276] Trial 304 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500101018 eval_loss: 7757.27099609375\n",
      "500101018  Best hyperparameters: {'hidden_size': 82, 'learning_rate': 0.0029400660933418497, 'dropout_rate': 0.45560509156989987}\n",
      "epoch: 1, loss: 0.6850093115418925\n",
      "epoch: 2, loss: 0.5929863170396149\n",
      "epoch: 3, loss: 0.5777424343750668\n",
      "epoch: 4, loss: 0.57404629137721\n",
      "epoch: 5, loss: 0.5700123504051435\n",
      "epoch: 6, loss: 0.5679855663582211\n",
      "epoch: 7, loss: 0.5655943642329666\n",
      "epoch: 8, loss: 0.5631518961797113\n",
      "epoch: 9, loss: 0.562383515937296\n",
      "epoch: 10, loss: 0.5617428755058962\n",
      "epoch: 11, loss: 0.5581722158157628\n",
      "epoch: 12, loss: 0.558258004025816\n",
      "epoch: 13, loss: 0.5557566205121094\n",
      "epoch: 14, loss: 0.5553607414300985\n",
      "epoch: 15, loss: 0.5547742116999662\n",
      "epoch: 16, loss: 0.5542949286835945\n",
      "epoch: 17, loss: 0.5563584535100341\n",
      "epoch: 18, loss: 0.5530253764281265\n",
      "epoch: 19, loss: 0.5520224917348514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-16 20:56:49,440] Using an existing study with name '500101026' instead of creating a new one.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 20, loss: 0.5522589449789009\n",
      "train <__main__.station_model object at 0x17d7b1b90> done!\n",
      "get data loader for 500101026\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-16 20:57:10,327] Trial 205 finished with value: 5896.7060546875 and parameters: {'hidden_size': 312, 'learning_rate': 0.0006529623101316045, 'dropout_rate': 0.26235623582136314}. Best is trial 205 with value: 5896.7060546875.\n",
      "[I 2023-12-16 20:57:30,668] Trial 206 finished with value: 5977.5166015625 and parameters: {'hidden_size': 312, 'learning_rate': 0.0006087445879657995, 'dropout_rate': 0.2621260238078149}. Best is trial 205 with value: 5896.7060546875.\n",
      "[I 2023-12-16 20:57:33,104] Trial 207 pruned. \n",
      "[I 2023-12-16 20:57:35,873] Trial 208 pruned. \n",
      "[I 2023-12-16 20:57:43,313] Trial 209 pruned. \n",
      "[I 2023-12-16 20:58:02,809] Trial 210 finished with value: 5994.60107421875 and parameters: {'hidden_size': 295, 'learning_rate': 0.0007618085831777872, 'dropout_rate': 0.25793649396277607}. Best is trial 205 with value: 5896.7060546875.\n",
      "[I 2023-12-16 20:58:22,560] Trial 211 finished with value: 5911.04248046875 and parameters: {'hidden_size': 299, 'learning_rate': 0.0007253623141595271, 'dropout_rate': 0.25624757139950227}. Best is trial 205 with value: 5896.7060546875.\n",
      "[I 2023-12-16 20:58:41,774] Trial 212 finished with value: 5876.42431640625 and parameters: {'hidden_size': 296, 'learning_rate': 0.000790644409945961, 'dropout_rate': 0.25751223484625235}. Best is trial 212 with value: 5876.42431640625.\n",
      "[I 2023-12-16 20:59:01,120] Trial 213 finished with value: 6019.603515625 and parameters: {'hidden_size': 284, 'learning_rate': 0.0007189792288299015, 'dropout_rate': 0.2576317499437701}. Best is trial 212 with value: 5876.42431640625.\n",
      "[I 2023-12-16 20:59:21,863] Trial 214 finished with value: 5927.2470703125 and parameters: {'hidden_size': 303, 'learning_rate': 0.0008423436802617691, 'dropout_rate': 0.26336676909425166}. Best is trial 212 with value: 5876.42431640625.\n",
      "[I 2023-12-16 20:59:45,135] Trial 215 finished with value: 5944.68408203125 and parameters: {'hidden_size': 341, 'learning_rate': 0.000937946988659433, 'dropout_rate': 0.2637312521304238}. Best is trial 212 with value: 5876.42431640625.\n",
      "[I 2023-12-16 20:59:46,676] Trial 216 pruned. \n",
      "[I 2023-12-16 21:00:06,926] Trial 217 finished with value: 5928.9150390625 and parameters: {'hidden_size': 304, 'learning_rate': 0.0008765689791023043, 'dropout_rate': 0.2742571284328239}. Best is trial 212 with value: 5876.42431640625.\n",
      "[I 2023-12-16 21:00:26,484] Trial 218 finished with value: 5919.3369140625 and parameters: {'hidden_size': 281, 'learning_rate': 0.0010404006857383946, 'dropout_rate': 0.27617170024147325}. Best is trial 212 with value: 5876.42431640625.\n",
      "[I 2023-12-16 21:00:28,630] Trial 219 pruned. \n",
      "[I 2023-12-16 21:00:31,043] Trial 220 pruned. \n",
      "[I 2023-12-16 21:00:50,946] Trial 221 finished with value: 6031.7119140625 and parameters: {'hidden_size': 297, 'learning_rate': 0.0008501043283549444, 'dropout_rate': 0.2754286346825034}. Best is trial 212 with value: 5876.42431640625.\n",
      "[I 2023-12-16 21:01:09,269] Trial 222 finished with value: 5934.107421875 and parameters: {'hidden_size': 272, 'learning_rate': 0.0010212301721160913, 'dropout_rate': 0.26502840036094916}. Best is trial 212 with value: 5876.42431640625.\n",
      "[I 2023-12-16 21:01:10,312] Trial 223 pruned. \n",
      "[I 2023-12-16 21:01:29,311] Trial 224 finished with value: 5892.3251953125 and parameters: {'hidden_size': 271, 'learning_rate': 0.0010637172370226843, 'dropout_rate': 0.26689716672155356}. Best is trial 212 with value: 5876.42431640625.\n",
      "[I 2023-12-16 21:01:31,572] Trial 225 pruned. \n",
      "[I 2023-12-16 21:01:46,861] Trial 226 finished with value: 5901.07861328125 and parameters: {'hidden_size': 227, 'learning_rate': 0.001321605846355446, 'dropout_rate': 0.28623800237198804}. Best is trial 212 with value: 5876.42431640625.\n",
      "[I 2023-12-16 21:01:47,884] Trial 227 pruned. \n",
      "[I 2023-12-16 21:02:02,695] Trial 228 finished with value: 5961.10107421875 and parameters: {'hidden_size': 212, 'learning_rate': 0.0016231975091398135, 'dropout_rate': 0.2741653611238564}. Best is trial 212 with value: 5876.42431640625.\n",
      "[I 2023-12-16 21:02:03,633] Trial 229 pruned. \n",
      "[I 2023-12-16 21:02:18,980] Trial 230 finished with value: 5951.03466796875 and parameters: {'hidden_size': 225, 'learning_rate': 0.001692988324735087, 'dropout_rate': 0.2927657871585406}. Best is trial 212 with value: 5876.42431640625.\n",
      "[I 2023-12-16 21:02:33,602] Trial 231 finished with value: 6047.19140625 and parameters: {'hidden_size': 216, 'learning_rate': 0.0015227062740882946, 'dropout_rate': 0.2943096371030204}. Best is trial 212 with value: 5876.42431640625.\n",
      "[I 2023-12-16 21:02:35,586] Trial 232 pruned. \n",
      "[I 2023-12-16 21:02:50,520] Trial 233 finished with value: 5919.0244140625 and parameters: {'hidden_size': 205, 'learning_rate': 0.0011716431756198485, 'dropout_rate': 0.2990977924417399}. Best is trial 212 with value: 5876.42431640625.\n",
      "[I 2023-12-16 21:03:03,889] Trial 234 finished with value: 5950.068359375 and parameters: {'hidden_size': 192, 'learning_rate': 0.001685052219403041, 'dropout_rate': 0.29030044721653936}. Best is trial 212 with value: 5876.42431640625.\n",
      "[I 2023-12-16 21:03:06,940] Trial 235 pruned. \n",
      "[I 2023-12-16 21:03:07,950] Trial 236 pruned. \n",
      "[I 2023-12-16 21:03:08,803] Trial 237 pruned. \n",
      "[I 2023-12-16 21:03:09,872] Trial 238 pruned. \n",
      "[I 2023-12-16 21:03:30,117] Trial 239 finished with value: 6013.32421875 and parameters: {'hidden_size': 277, 'learning_rate': 0.0015734610180471779, 'dropout_rate': 0.2734707307146822}. Best is trial 212 with value: 5876.42431640625.\n",
      "[I 2023-12-16 21:03:44,346] Trial 240 finished with value: 5982.22998046875 and parameters: {'hidden_size': 198, 'learning_rate': 0.0011532490760805547, 'dropout_rate': 0.29075272705772504}. Best is trial 212 with value: 5876.42431640625.\n",
      "[I 2023-12-16 21:03:59,432] Trial 241 finished with value: 6098.8388671875 and parameters: {'hidden_size': 209, 'learning_rate': 0.0012625067053994223, 'dropout_rate': 0.29915289382238214}. Best is trial 212 with value: 5876.42431640625.\n",
      "[I 2023-12-16 21:04:11,984] Trial 242 finished with value: 6029.4794921875 and parameters: {'hidden_size': 166, 'learning_rate': 0.0023110834490951324, 'dropout_rate': 0.29205919256325646}. Best is trial 212 with value: 5876.42431640625.\n",
      "[I 2023-12-16 21:04:13,275] Trial 243 pruned. \n",
      "[I 2023-12-16 21:04:26,428] Trial 244 finished with value: 5999.1611328125 and parameters: {'hidden_size': 192, 'learning_rate': 0.0015914458995825917, 'dropout_rate': 0.313253211363613}. Best is trial 212 with value: 5876.42431640625.\n",
      "[I 2023-12-16 21:04:28,969] Trial 245 pruned. \n",
      "[I 2023-12-16 21:04:49,218] Trial 246 finished with value: 5904.06884765625 and parameters: {'hidden_size': 277, 'learning_rate': 0.0012839876418735418, 'dropout_rate': 0.2750027765005711}. Best is trial 212 with value: 5876.42431640625.\n",
      "[I 2023-12-16 21:04:53,440] Trial 247 pruned. \n",
      "[I 2023-12-16 21:05:07,939] Trial 248 finished with value: 5977.275390625 and parameters: {'hidden_size': 224, 'learning_rate': 0.001255507874814484, 'dropout_rate': 0.28014227961084043}. Best is trial 212 with value: 5876.42431640625.\n",
      "[I 2023-12-16 21:05:29,428] Trial 249 finished with value: 5937.0302734375 and parameters: {'hidden_size': 311, 'learning_rate': 0.0016412492620176142, 'dropout_rate': 0.2721054917441749}. Best is trial 212 with value: 5876.42431640625.\n",
      "[I 2023-12-16 21:05:34,135] Trial 250 pruned. \n",
      "[I 2023-12-16 21:05:50,650] Trial 251 finished with value: 5987.19482421875 and parameters: {'hidden_size': 234, 'learning_rate': 0.0018048939294563373, 'dropout_rate': 0.26962226489429425}. Best is trial 212 with value: 5876.42431640625.\n",
      "[I 2023-12-16 21:05:54,929] Trial 252 pruned. \n",
      "[I 2023-12-16 21:05:57,399] Trial 253 pruned. \n",
      "[I 2023-12-16 21:05:58,427] Trial 254 pruned. \n",
      "[I 2023-12-16 21:06:00,626] Trial 255 pruned. \n",
      "[I 2023-12-16 21:06:03,055] Trial 256 pruned. \n",
      "[I 2023-12-16 21:06:23,198] Trial 257 finished with value: 5899.7294921875 and parameters: {'hidden_size': 280, 'learning_rate': 0.0009813619441719765, 'dropout_rate': 0.2866185321995583}. Best is trial 212 with value: 5876.42431640625.\n",
      "[I 2023-12-16 21:06:25,111] Trial 258 pruned. \n",
      "[I 2023-12-16 21:06:25,955] Trial 259 pruned. \n",
      "[I 2023-12-16 21:06:27,711] Trial 260 pruned. \n",
      "[I 2023-12-16 21:06:29,092] Trial 261 pruned. \n",
      "[I 2023-12-16 21:06:29,543] Trial 262 pruned. \n",
      "[I 2023-12-16 21:06:31,139] Trial 263 pruned. \n",
      "[I 2023-12-16 21:06:31,602] Trial 264 pruned. \n",
      "[I 2023-12-16 21:06:33,203] Trial 265 pruned. \n",
      "[I 2023-12-16 21:06:34,423] Trial 266 pruned. \n",
      "[I 2023-12-16 21:06:35,982] Trial 267 pruned. \n",
      "[I 2023-12-16 21:06:37,395] Trial 268 pruned. \n",
      "[I 2023-12-16 21:06:38,911] Trial 269 pruned. \n",
      "[I 2023-12-16 21:06:54,242] Trial 270 finished with value: 5950.83447265625 and parameters: {'hidden_size': 195, 'learning_rate': 0.0010062440097822944, 'dropout_rate': 0.26794235287047957}. Best is trial 212 with value: 5876.42431640625.\n",
      "[I 2023-12-16 21:06:55,183] Trial 271 pruned. \n",
      "[I 2023-12-16 21:06:55,976] Trial 272 pruned. \n",
      "[I 2023-12-16 21:06:56,856] Trial 273 pruned. \n",
      "[I 2023-12-16 21:07:00,348] Trial 274 pruned. \n",
      "[I 2023-12-16 21:07:02,952] Trial 275 pruned. \n",
      "[I 2023-12-16 21:07:05,769] Trial 276 pruned. \n",
      "[I 2023-12-16 21:07:06,814] Trial 277 pruned. \n",
      "[I 2023-12-16 21:07:07,903] Trial 278 pruned. \n",
      "[I 2023-12-16 21:07:09,335] Trial 279 pruned. \n",
      "[I 2023-12-16 21:07:10,166] Trial 280 pruned. \n",
      "[I 2023-12-16 21:07:11,824] Trial 281 pruned. \n",
      "[I 2023-12-16 21:07:13,088] Trial 282 pruned. \n",
      "[I 2023-12-16 21:07:14,605] Trial 283 pruned. \n",
      "[I 2023-12-16 21:07:58,031] Trial 284 pruned. \n",
      "[I 2023-12-16 21:08:01,569] Trial 285 pruned. \n",
      "[I 2023-12-16 21:08:02,105] Trial 286 pruned. \n",
      "[I 2023-12-16 21:08:03,101] Trial 287 pruned. \n",
      "[I 2023-12-16 21:08:04,159] Trial 288 pruned. \n",
      "[I 2023-12-16 21:08:25,855] Trial 289 finished with value: 5896.67236328125 and parameters: {'hidden_size': 309, 'learning_rate': 0.0009934108755050737, 'dropout_rate': 0.26015363338258063}. Best is trial 212 with value: 5876.42431640625.\n",
      "[I 2023-12-16 21:08:27,292] Trial 290 pruned. \n",
      "[I 2023-12-16 21:08:30,071] Trial 291 pruned. \n",
      "[I 2023-12-16 21:08:49,362] Trial 292 finished with value: 5968.6904296875 and parameters: {'hidden_size': 266, 'learning_rate': 0.0013878083244987954, 'dropout_rate': 0.25973715281175264}. Best is trial 212 with value: 5876.42431640625.\n",
      "[I 2023-12-16 21:08:50,594] Trial 293 pruned. \n",
      "[I 2023-12-16 21:09:05,609] Trial 294 finished with value: 6009.4560546875 and parameters: {'hidden_size': 219, 'learning_rate': 0.001954043578770355, 'dropout_rate': 0.26259448754606457}. Best is trial 212 with value: 5876.42431640625.\n",
      "[I 2023-12-16 21:09:06,162] Trial 295 pruned. \n",
      "[I 2023-12-16 21:09:18,044] Trial 296 finished with value: 5977.5869140625 and parameters: {'hidden_size': 160, 'learning_rate': 0.001277010975195645, 'dropout_rate': 0.27200496159071463}. Best is trial 212 with value: 5876.42431640625.\n",
      "[I 2023-12-16 21:09:37,576] Trial 297 finished with value: 5935.7060546875 and parameters: {'hidden_size': 288, 'learning_rate': 0.0016452839769111403, 'dropout_rate': 0.2603107158743083}. Best is trial 212 with value: 5876.42431640625.\n",
      "[I 2023-12-16 21:09:58,862] Trial 298 finished with value: 5987.3125 and parameters: {'hidden_size': 310, 'learning_rate': 0.0017637265959373973, 'dropout_rate': 0.2655036182872009}. Best is trial 212 with value: 5876.42431640625.\n",
      "[I 2023-12-16 21:09:59,936] Trial 299 pruned. \n",
      "[I 2023-12-16 21:10:07,102] Trial 300 pruned. \n",
      "[I 2023-12-16 21:10:30,444] Trial 301 finished with value: 6045.484375 and parameters: {'hidden_size': 357, 'learning_rate': 0.0010930526722518095, 'dropout_rate': 0.30653687753929876}. Best is trial 212 with value: 5876.42431640625.\n",
      "[I 2023-12-16 21:10:50,352] Trial 302 finished with value: 5952.65185546875 and parameters: {'hidden_size': 287, 'learning_rate': 0.0014964335343601838, 'dropout_rate': 0.26921012434515373}. Best is trial 212 with value: 5876.42431640625.\n",
      "[I 2023-12-16 21:10:51,709] Trial 303 pruned. \n",
      "[I 2023-12-16 21:10:53,067] Trial 304 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500101026 eval_loss: 5876.42431640625\n",
      "500101026  Best hyperparameters: {'hidden_size': 296, 'learning_rate': 0.000790644409945961, 'dropout_rate': 0.25751223484625235}\n",
      "epoch: 1, loss: 0.5025382466940139\n",
      "epoch: 2, loss: 0.40982631053410323\n",
      "epoch: 3, loss: 0.39145812098931404\n",
      "epoch: 4, loss: 0.38460197876211744\n",
      "epoch: 5, loss: 0.38022502319575435\n",
      "epoch: 6, loss: 0.3792253107641617\n",
      "epoch: 7, loss: 0.3751290423028609\n",
      "epoch: 8, loss: 0.3747187431221814\n",
      "epoch: 9, loss: 0.3744614759114533\n",
      "epoch: 10, loss: 0.37257213951235985\n",
      "epoch: 11, loss: 0.37267235285556155\n",
      "epoch: 12, loss: 0.3728525435627855\n",
      "epoch: 13, loss: 0.3715907512611933\n",
      "epoch: 14, loss: 0.3705204709757507\n",
      "epoch: 15, loss: 0.36951398065097374\n",
      "epoch: 16, loss: 0.3706374197164452\n",
      "epoch: 17, loss: 0.36892028267566973\n",
      "epoch: 18, loss: 0.36983089296211846\n",
      "epoch: 19, loss: 0.3698523592984874\n",
      "epoch: 20, loss: 0.36788912998515194\n",
      "train <__main__.station_model object at 0x17d608350> done!\n"
     ]
    }
   ],
   "source": [
    "# read loss.csv \n",
    "# train those station with loss avg > 0.35\n",
    "loss_df = pd.read_csv('loss.csv')\n",
    "# get station with loss avg > 0.35\n",
    "station_list = []\n",
    "for col in loss_df.columns:\n",
    "    # if last element > 0.35, train this station\n",
    "    if loss_df[col].iloc[-1] > 0.35:\n",
    "        col = float(col)\n",
    "        col = int(col)\n",
    "        station_list.append(col)\n",
    "print(station_list)\n",
    "\n",
    "# train those station\n",
    "for station_id in station_list:\n",
    "    station = station_model(station_id)\n",
    "    station.get_data_loader()\n",
    "    station.get_best_param()\n",
    "    station.train()\n",
    "    print(f'train {station} done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>500101003.0</th>\n",
       "      <th>500101004.0</th>\n",
       "      <th>500101005.0</th>\n",
       "      <th>500101006.0</th>\n",
       "      <th>500101007.0</th>\n",
       "      <th>500101008.0</th>\n",
       "      <th>500101009.0</th>\n",
       "      <th>500101010.0</th>\n",
       "      <th>500101013.0</th>\n",
       "      <th>500101014.0</th>\n",
       "      <th>...</th>\n",
       "      <th>500101010</th>\n",
       "      <th>500101013</th>\n",
       "      <th>500101014</th>\n",
       "      <th>500101018</th>\n",
       "      <th>500101019</th>\n",
       "      <th>500101024</th>\n",
       "      <th>500101026</th>\n",
       "      <th>500101031</th>\n",
       "      <th>500101037</th>\n",
       "      <th>500101039</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.444689</td>\n",
       "      <td>0.538689</td>\n",
       "      <td>0.331459</td>\n",
       "      <td>0.396226</td>\n",
       "      <td>0.372812</td>\n",
       "      <td>0.613214</td>\n",
       "      <td>0.561268</td>\n",
       "      <td>0.710761</td>\n",
       "      <td>0.525923</td>\n",
       "      <td>0.526660</td>\n",
       "      <td>...</td>\n",
       "      <td>0.606375</td>\n",
       "      <td>0.522900</td>\n",
       "      <td>0.520044</td>\n",
       "      <td>0.685009</td>\n",
       "      <td>0.392406</td>\n",
       "      <td>0.291617</td>\n",
       "      <td>0.502538</td>\n",
       "      <td>0.386127</td>\n",
       "      <td>0.339212</td>\n",
       "      <td>0.428888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.395261</td>\n",
       "      <td>0.476579</td>\n",
       "      <td>0.316708</td>\n",
       "      <td>0.385969</td>\n",
       "      <td>0.350236</td>\n",
       "      <td>0.511558</td>\n",
       "      <td>0.501418</td>\n",
       "      <td>0.664368</td>\n",
       "      <td>0.500104</td>\n",
       "      <td>0.512044</td>\n",
       "      <td>...</td>\n",
       "      <td>0.485900</td>\n",
       "      <td>0.497036</td>\n",
       "      <td>0.491198</td>\n",
       "      <td>0.592986</td>\n",
       "      <td>0.372061</td>\n",
       "      <td>0.214250</td>\n",
       "      <td>0.409826</td>\n",
       "      <td>0.359999</td>\n",
       "      <td>0.304312</td>\n",
       "      <td>0.420668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.387346</td>\n",
       "      <td>0.467370</td>\n",
       "      <td>0.313265</td>\n",
       "      <td>0.383831</td>\n",
       "      <td>0.347216</td>\n",
       "      <td>0.486738</td>\n",
       "      <td>0.486223</td>\n",
       "      <td>0.618513</td>\n",
       "      <td>0.484265</td>\n",
       "      <td>0.502888</td>\n",
       "      <td>...</td>\n",
       "      <td>0.452455</td>\n",
       "      <td>0.481752</td>\n",
       "      <td>0.471211</td>\n",
       "      <td>0.577742</td>\n",
       "      <td>0.365425</td>\n",
       "      <td>0.205954</td>\n",
       "      <td>0.391458</td>\n",
       "      <td>0.351646</td>\n",
       "      <td>0.292062</td>\n",
       "      <td>0.414385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.382817</td>\n",
       "      <td>0.462352</td>\n",
       "      <td>0.311306</td>\n",
       "      <td>0.381893</td>\n",
       "      <td>0.346180</td>\n",
       "      <td>0.479111</td>\n",
       "      <td>0.476797</td>\n",
       "      <td>0.592754</td>\n",
       "      <td>0.473701</td>\n",
       "      <td>0.488586</td>\n",
       "      <td>...</td>\n",
       "      <td>0.443711</td>\n",
       "      <td>0.471202</td>\n",
       "      <td>0.462687</td>\n",
       "      <td>0.574046</td>\n",
       "      <td>0.362662</td>\n",
       "      <td>0.200216</td>\n",
       "      <td>0.384602</td>\n",
       "      <td>0.345904</td>\n",
       "      <td>0.284678</td>\n",
       "      <td>0.408163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.378458</td>\n",
       "      <td>0.458872</td>\n",
       "      <td>0.310363</td>\n",
       "      <td>0.380193</td>\n",
       "      <td>0.344546</td>\n",
       "      <td>0.472084</td>\n",
       "      <td>0.472706</td>\n",
       "      <td>0.572710</td>\n",
       "      <td>0.468697</td>\n",
       "      <td>0.480242</td>\n",
       "      <td>...</td>\n",
       "      <td>0.436970</td>\n",
       "      <td>0.462013</td>\n",
       "      <td>0.458966</td>\n",
       "      <td>0.570012</td>\n",
       "      <td>0.360111</td>\n",
       "      <td>0.197163</td>\n",
       "      <td>0.380225</td>\n",
       "      <td>0.342101</td>\n",
       "      <td>0.281267</td>\n",
       "      <td>0.398697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.377167</td>\n",
       "      <td>0.455809</td>\n",
       "      <td>0.308897</td>\n",
       "      <td>0.379075</td>\n",
       "      <td>0.343764</td>\n",
       "      <td>0.468867</td>\n",
       "      <td>0.466260</td>\n",
       "      <td>0.557047</td>\n",
       "      <td>0.463540</td>\n",
       "      <td>0.474583</td>\n",
       "      <td>...</td>\n",
       "      <td>0.434018</td>\n",
       "      <td>0.455000</td>\n",
       "      <td>0.455674</td>\n",
       "      <td>0.567986</td>\n",
       "      <td>0.357484</td>\n",
       "      <td>0.194032</td>\n",
       "      <td>0.379225</td>\n",
       "      <td>0.338014</td>\n",
       "      <td>0.277630</td>\n",
       "      <td>0.386378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.375860</td>\n",
       "      <td>0.454142</td>\n",
       "      <td>0.308208</td>\n",
       "      <td>0.378053</td>\n",
       "      <td>0.342959</td>\n",
       "      <td>0.466838</td>\n",
       "      <td>0.464686</td>\n",
       "      <td>0.535636</td>\n",
       "      <td>0.457769</td>\n",
       "      <td>0.471129</td>\n",
       "      <td>...</td>\n",
       "      <td>0.432046</td>\n",
       "      <td>0.444856</td>\n",
       "      <td>0.453399</td>\n",
       "      <td>0.565594</td>\n",
       "      <td>0.355343</td>\n",
       "      <td>0.192943</td>\n",
       "      <td>0.375129</td>\n",
       "      <td>0.334768</td>\n",
       "      <td>0.276315</td>\n",
       "      <td>0.371611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.375356</td>\n",
       "      <td>0.452423</td>\n",
       "      <td>0.307136</td>\n",
       "      <td>0.377295</td>\n",
       "      <td>0.342771</td>\n",
       "      <td>0.465613</td>\n",
       "      <td>0.465003</td>\n",
       "      <td>0.517334</td>\n",
       "      <td>0.453156</td>\n",
       "      <td>0.469102</td>\n",
       "      <td>...</td>\n",
       "      <td>0.428879</td>\n",
       "      <td>0.438376</td>\n",
       "      <td>0.451143</td>\n",
       "      <td>0.563152</td>\n",
       "      <td>0.353816</td>\n",
       "      <td>0.191459</td>\n",
       "      <td>0.374719</td>\n",
       "      <td>0.331472</td>\n",
       "      <td>0.276297</td>\n",
       "      <td>0.350956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.373972</td>\n",
       "      <td>0.449503</td>\n",
       "      <td>0.305809</td>\n",
       "      <td>0.376403</td>\n",
       "      <td>0.342191</td>\n",
       "      <td>0.464973</td>\n",
       "      <td>0.463659</td>\n",
       "      <td>0.503593</td>\n",
       "      <td>0.447606</td>\n",
       "      <td>0.465231</td>\n",
       "      <td>...</td>\n",
       "      <td>0.428101</td>\n",
       "      <td>0.434227</td>\n",
       "      <td>0.449027</td>\n",
       "      <td>0.562384</td>\n",
       "      <td>0.351643</td>\n",
       "      <td>0.189875</td>\n",
       "      <td>0.374461</td>\n",
       "      <td>0.330133</td>\n",
       "      <td>0.274712</td>\n",
       "      <td>0.335674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.373933</td>\n",
       "      <td>0.448108</td>\n",
       "      <td>0.304643</td>\n",
       "      <td>0.375276</td>\n",
       "      <td>0.342760</td>\n",
       "      <td>0.465499</td>\n",
       "      <td>0.463357</td>\n",
       "      <td>0.490896</td>\n",
       "      <td>0.439626</td>\n",
       "      <td>0.463448</td>\n",
       "      <td>...</td>\n",
       "      <td>0.425136</td>\n",
       "      <td>0.432196</td>\n",
       "      <td>0.449164</td>\n",
       "      <td>0.561743</td>\n",
       "      <td>0.350101</td>\n",
       "      <td>0.189323</td>\n",
       "      <td>0.372572</td>\n",
       "      <td>0.328008</td>\n",
       "      <td>0.273379</td>\n",
       "      <td>0.326247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.372924</td>\n",
       "      <td>0.446009</td>\n",
       "      <td>0.304494</td>\n",
       "      <td>0.374275</td>\n",
       "      <td>0.341847</td>\n",
       "      <td>0.462641</td>\n",
       "      <td>0.460793</td>\n",
       "      <td>0.482647</td>\n",
       "      <td>0.435532</td>\n",
       "      <td>0.461784</td>\n",
       "      <td>...</td>\n",
       "      <td>0.425411</td>\n",
       "      <td>0.430687</td>\n",
       "      <td>0.447145</td>\n",
       "      <td>0.558172</td>\n",
       "      <td>0.348667</td>\n",
       "      <td>0.187943</td>\n",
       "      <td>0.372672</td>\n",
       "      <td>0.326116</td>\n",
       "      <td>0.272581</td>\n",
       "      <td>0.321499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.374191</td>\n",
       "      <td>0.444187</td>\n",
       "      <td>0.303723</td>\n",
       "      <td>0.373221</td>\n",
       "      <td>0.341709</td>\n",
       "      <td>0.461365</td>\n",
       "      <td>0.461391</td>\n",
       "      <td>0.472874</td>\n",
       "      <td>0.430297</td>\n",
       "      <td>0.460860</td>\n",
       "      <td>...</td>\n",
       "      <td>0.423536</td>\n",
       "      <td>0.427488</td>\n",
       "      <td>0.448190</td>\n",
       "      <td>0.558258</td>\n",
       "      <td>0.347154</td>\n",
       "      <td>0.187684</td>\n",
       "      <td>0.372853</td>\n",
       "      <td>0.324072</td>\n",
       "      <td>0.273068</td>\n",
       "      <td>0.316094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.372002</td>\n",
       "      <td>0.442234</td>\n",
       "      <td>0.303263</td>\n",
       "      <td>0.372443</td>\n",
       "      <td>0.341372</td>\n",
       "      <td>0.459348</td>\n",
       "      <td>0.461053</td>\n",
       "      <td>0.463473</td>\n",
       "      <td>0.429538</td>\n",
       "      <td>0.459470</td>\n",
       "      <td>...</td>\n",
       "      <td>0.420904</td>\n",
       "      <td>0.426723</td>\n",
       "      <td>0.444318</td>\n",
       "      <td>0.555757</td>\n",
       "      <td>0.346279</td>\n",
       "      <td>0.187076</td>\n",
       "      <td>0.371591</td>\n",
       "      <td>0.324382</td>\n",
       "      <td>0.272557</td>\n",
       "      <td>0.313901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.372654</td>\n",
       "      <td>0.439408</td>\n",
       "      <td>0.302566</td>\n",
       "      <td>0.371369</td>\n",
       "      <td>0.341262</td>\n",
       "      <td>0.460870</td>\n",
       "      <td>0.460472</td>\n",
       "      <td>0.457777</td>\n",
       "      <td>0.426561</td>\n",
       "      <td>0.459008</td>\n",
       "      <td>...</td>\n",
       "      <td>0.419978</td>\n",
       "      <td>0.424686</td>\n",
       "      <td>0.444289</td>\n",
       "      <td>0.555361</td>\n",
       "      <td>0.345258</td>\n",
       "      <td>0.186514</td>\n",
       "      <td>0.370520</td>\n",
       "      <td>0.322408</td>\n",
       "      <td>0.272308</td>\n",
       "      <td>0.310457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.371947</td>\n",
       "      <td>0.438746</td>\n",
       "      <td>0.301860</td>\n",
       "      <td>0.371120</td>\n",
       "      <td>0.341056</td>\n",
       "      <td>0.458038</td>\n",
       "      <td>0.458195</td>\n",
       "      <td>0.451151</td>\n",
       "      <td>0.425953</td>\n",
       "      <td>0.457625</td>\n",
       "      <td>...</td>\n",
       "      <td>0.420240</td>\n",
       "      <td>0.424184</td>\n",
       "      <td>0.443946</td>\n",
       "      <td>0.554774</td>\n",
       "      <td>0.344576</td>\n",
       "      <td>0.186403</td>\n",
       "      <td>0.369514</td>\n",
       "      <td>0.323320</td>\n",
       "      <td>0.272278</td>\n",
       "      <td>0.308664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.419004</td>\n",
       "      <td>0.424072</td>\n",
       "      <td>0.444128</td>\n",
       "      <td>0.554295</td>\n",
       "      <td>0.344404</td>\n",
       "      <td>0.186044</td>\n",
       "      <td>0.370637</td>\n",
       "      <td>0.322822</td>\n",
       "      <td>0.271461</td>\n",
       "      <td>0.307074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.417500</td>\n",
       "      <td>0.424021</td>\n",
       "      <td>0.442800</td>\n",
       "      <td>0.556358</td>\n",
       "      <td>0.343953</td>\n",
       "      <td>0.185359</td>\n",
       "      <td>0.368920</td>\n",
       "      <td>0.321171</td>\n",
       "      <td>0.271334</td>\n",
       "      <td>0.305520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.416305</td>\n",
       "      <td>0.422209</td>\n",
       "      <td>0.442373</td>\n",
       "      <td>0.553025</td>\n",
       "      <td>0.343495</td>\n",
       "      <td>0.185524</td>\n",
       "      <td>0.369831</td>\n",
       "      <td>0.321217</td>\n",
       "      <td>0.271052</td>\n",
       "      <td>0.304176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.418940</td>\n",
       "      <td>0.421289</td>\n",
       "      <td>0.442310</td>\n",
       "      <td>0.552022</td>\n",
       "      <td>0.341981</td>\n",
       "      <td>0.184678</td>\n",
       "      <td>0.369852</td>\n",
       "      <td>0.320095</td>\n",
       "      <td>0.270930</td>\n",
       "      <td>0.304505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.416867</td>\n",
       "      <td>0.422082</td>\n",
       "      <td>0.441178</td>\n",
       "      <td>0.552259</td>\n",
       "      <td>0.342363</td>\n",
       "      <td>0.184280</td>\n",
       "      <td>0.367889</td>\n",
       "      <td>0.319105</td>\n",
       "      <td>0.269843</td>\n",
       "      <td>0.302825</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows  48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    500101003.0  500101004.0  500101005.0  500101006.0  500101007.0  \\\n",
       "0      0.444689     0.538689     0.331459     0.396226     0.372812   \n",
       "1      0.395261     0.476579     0.316708     0.385969     0.350236   \n",
       "2      0.387346     0.467370     0.313265     0.383831     0.347216   \n",
       "3      0.382817     0.462352     0.311306     0.381893     0.346180   \n",
       "4      0.378458     0.458872     0.310363     0.380193     0.344546   \n",
       "5      0.377167     0.455809     0.308897     0.379075     0.343764   \n",
       "6      0.375860     0.454142     0.308208     0.378053     0.342959   \n",
       "7      0.375356     0.452423     0.307136     0.377295     0.342771   \n",
       "8      0.373972     0.449503     0.305809     0.376403     0.342191   \n",
       "9      0.373933     0.448108     0.304643     0.375276     0.342760   \n",
       "10     0.372924     0.446009     0.304494     0.374275     0.341847   \n",
       "11     0.374191     0.444187     0.303723     0.373221     0.341709   \n",
       "12     0.372002     0.442234     0.303263     0.372443     0.341372   \n",
       "13     0.372654     0.439408     0.302566     0.371369     0.341262   \n",
       "14     0.371947     0.438746     0.301860     0.371120     0.341056   \n",
       "15          NaN          NaN          NaN          NaN          NaN   \n",
       "16          NaN          NaN          NaN          NaN          NaN   \n",
       "17          NaN          NaN          NaN          NaN          NaN   \n",
       "18          NaN          NaN          NaN          NaN          NaN   \n",
       "19          NaN          NaN          NaN          NaN          NaN   \n",
       "\n",
       "    500101008.0  500101009.0  500101010.0  500101013.0  500101014.0  ...  \\\n",
       "0      0.613214     0.561268     0.710761     0.525923     0.526660  ...   \n",
       "1      0.511558     0.501418     0.664368     0.500104     0.512044  ...   \n",
       "2      0.486738     0.486223     0.618513     0.484265     0.502888  ...   \n",
       "3      0.479111     0.476797     0.592754     0.473701     0.488586  ...   \n",
       "4      0.472084     0.472706     0.572710     0.468697     0.480242  ...   \n",
       "5      0.468867     0.466260     0.557047     0.463540     0.474583  ...   \n",
       "6      0.466838     0.464686     0.535636     0.457769     0.471129  ...   \n",
       "7      0.465613     0.465003     0.517334     0.453156     0.469102  ...   \n",
       "8      0.464973     0.463659     0.503593     0.447606     0.465231  ...   \n",
       "9      0.465499     0.463357     0.490896     0.439626     0.463448  ...   \n",
       "10     0.462641     0.460793     0.482647     0.435532     0.461784  ...   \n",
       "11     0.461365     0.461391     0.472874     0.430297     0.460860  ...   \n",
       "12     0.459348     0.461053     0.463473     0.429538     0.459470  ...   \n",
       "13     0.460870     0.460472     0.457777     0.426561     0.459008  ...   \n",
       "14     0.458038     0.458195     0.451151     0.425953     0.457625  ...   \n",
       "15          NaN          NaN          NaN          NaN          NaN  ...   \n",
       "16          NaN          NaN          NaN          NaN          NaN  ...   \n",
       "17          NaN          NaN          NaN          NaN          NaN  ...   \n",
       "18          NaN          NaN          NaN          NaN          NaN  ...   \n",
       "19          NaN          NaN          NaN          NaN          NaN  ...   \n",
       "\n",
       "    500101010  500101013  500101014  500101018  500101019  500101024  \\\n",
       "0    0.606375   0.522900   0.520044   0.685009   0.392406   0.291617   \n",
       "1    0.485900   0.497036   0.491198   0.592986   0.372061   0.214250   \n",
       "2    0.452455   0.481752   0.471211   0.577742   0.365425   0.205954   \n",
       "3    0.443711   0.471202   0.462687   0.574046   0.362662   0.200216   \n",
       "4    0.436970   0.462013   0.458966   0.570012   0.360111   0.197163   \n",
       "5    0.434018   0.455000   0.455674   0.567986   0.357484   0.194032   \n",
       "6    0.432046   0.444856   0.453399   0.565594   0.355343   0.192943   \n",
       "7    0.428879   0.438376   0.451143   0.563152   0.353816   0.191459   \n",
       "8    0.428101   0.434227   0.449027   0.562384   0.351643   0.189875   \n",
       "9    0.425136   0.432196   0.449164   0.561743   0.350101   0.189323   \n",
       "10   0.425411   0.430687   0.447145   0.558172   0.348667   0.187943   \n",
       "11   0.423536   0.427488   0.448190   0.558258   0.347154   0.187684   \n",
       "12   0.420904   0.426723   0.444318   0.555757   0.346279   0.187076   \n",
       "13   0.419978   0.424686   0.444289   0.555361   0.345258   0.186514   \n",
       "14   0.420240   0.424184   0.443946   0.554774   0.344576   0.186403   \n",
       "15   0.419004   0.424072   0.444128   0.554295   0.344404   0.186044   \n",
       "16   0.417500   0.424021   0.442800   0.556358   0.343953   0.185359   \n",
       "17   0.416305   0.422209   0.442373   0.553025   0.343495   0.185524   \n",
       "18   0.418940   0.421289   0.442310   0.552022   0.341981   0.184678   \n",
       "19   0.416867   0.422082   0.441178   0.552259   0.342363   0.184280   \n",
       "\n",
       "    500101026  500101031  500101037  500101039  \n",
       "0    0.502538   0.386127   0.339212   0.428888  \n",
       "1    0.409826   0.359999   0.304312   0.420668  \n",
       "2    0.391458   0.351646   0.292062   0.414385  \n",
       "3    0.384602   0.345904   0.284678   0.408163  \n",
       "4    0.380225   0.342101   0.281267   0.398697  \n",
       "5    0.379225   0.338014   0.277630   0.386378  \n",
       "6    0.375129   0.334768   0.276315   0.371611  \n",
       "7    0.374719   0.331472   0.276297   0.350956  \n",
       "8    0.374461   0.330133   0.274712   0.335674  \n",
       "9    0.372572   0.328008   0.273379   0.326247  \n",
       "10   0.372672   0.326116   0.272581   0.321499  \n",
       "11   0.372853   0.324072   0.273068   0.316094  \n",
       "12   0.371591   0.324382   0.272557   0.313901  \n",
       "13   0.370520   0.322408   0.272308   0.310457  \n",
       "14   0.369514   0.323320   0.272278   0.308664  \n",
       "15   0.370637   0.322822   0.271461   0.307074  \n",
       "16   0.368920   0.321171   0.271334   0.305520  \n",
       "17   0.369831   0.321217   0.271052   0.304176  \n",
       "18   0.369852   0.320095   0.270930   0.304505  \n",
       "19   0.367889   0.319105   0.269843   0.302825  \n",
       "\n",
       "[20 rows x 48 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('loss.csv')\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "html-qsiNAWFM-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
