{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import random_split, TensorDataset\n",
    "\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "import optuna\n",
    "import torch.optim as optim\n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the gradient boosting regression model with PyTorch\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, dropout_rate):\n",
    "        super(Net, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout_rate)\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3= nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc4= nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc5= nn.Linear(hidden_size, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.dropout(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc3(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc4(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc5(out)\n",
    "        return out\n",
    "    \n",
    "class CustomLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomLoss, self).__init__()\n",
    "        \n",
    "    def forward(self, outputs, labels):\n",
    "        # Define your custom loss function here\n",
    "        # This is a simple example, replace it with your own function\n",
    "        custom_loss = 3*(torch.abs(labels - outputs))*(torch.abs(labels - 1/3) + torch.abs(labels -2/3))\n",
    "        return custom_loss\n",
    "    \n",
    "    \n",
    "def save_loss_in_csv(model_num, loss):\n",
    "    df = pd.read_csv('loss.csv')\n",
    "    # add all val to the end of df\n",
    "    df[model_num] = loss\n",
    "    df.to_csv('loss.csv')\n",
    "\n",
    "def get_one_hot_weekday(date_str):\n",
    "    # Convert the date string to a datetime object\n",
    "    date = datetime.datetime.strptime(date_str, '%Y%m%d')\n",
    "    \n",
    "    # Get the weekday (Monday is 0, Sunday is 6)\n",
    "    weekday = date.weekday()\n",
    "    \n",
    "    # Create a one-hot encoded list for the weekday\n",
    "    one_hot_weekday = [1 if i == weekday else 0 for i in range(7)]\n",
    "    \n",
    "    return one_hot_weekday\n",
    "\n",
    "def time_to_minute(time_str):\n",
    "    hours, minutes = map(int, time_str.split(':'))\n",
    "    total_minutes = hours * 60 + minutes\n",
    "    return total_minutes\n",
    "\n",
    "    \n",
    "def objective(trial):\n",
    "    # Sample hyperparameters\n",
    "    input_size = 8\n",
    "    hidden_size = trial.suggest_int('hidden_size', 3, 1000, log=True)  # Single hidden size for all layers\n",
    "    output_size = 1\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-1, log=True)\n",
    "    dropout_rate = trial.suggest_float('dropout_rate', 0.0, 0.3)\n",
    "\n",
    "    # Instantiate the model with sampled hyperparameters\n",
    "    model = Net(input_size, hidden_size, output_size, dropout_rate)\n",
    "\n",
    "    # Define the loss function and optimizer\n",
    "    criterion = CustomLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(100):\n",
    "        training_loss = 0.0\n",
    "        eval_loss = 0.0\n",
    "\n",
    "        # Iterate over the DataLoader for training data\n",
    "        for i, data in enumerate(train_loader):\n",
    "            inputs, labels = data\n",
    "            inputs = inputs.float()\n",
    "            labels = labels.float()\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            outputs = outputs.reshape(-1)\n",
    "            loss = criterion(outputs, labels).sum()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            training_loss += loss.item()\n",
    "\n",
    "        model.eval()\n",
    "        outloss = 0\n",
    "        # Iterate over the DataLoader for test data\n",
    "        with torch.no_grad():\n",
    "            for i, data in enumerate(test_loader):\n",
    "                inputs, labels = data\n",
    "                inputs = inputs.float()\n",
    "                labels = labels.float()\n",
    "                outputs = model(inputs).reshape(-1)\n",
    "                loss = criterion(outputs, labels).sum()\n",
    "                eval_loss += loss\n",
    "\n",
    "        # Optuna logs the running loss for each epoch\n",
    "        trial.report(eval_loss, epoch)\n",
    "\n",
    "        # Handle pruning based on the intermediate value\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "    # Return the running loss as the objective value to minimize\n",
    "    return eval_loss\n",
    "\n",
    "\n",
    "class station_model():\n",
    "    def __init__(self, station):\n",
    "        self.station = station\n",
    "\n",
    "    def get_data_loader(self):\n",
    "        df = pd.read_parquet(f'parquets/{self.station}.parquet')\n",
    "        TOT = df['tot'].iloc[0]\n",
    "        df['sbi'] = df['sbi']/TOT\n",
    "        df['time'] = df['time']/1440\n",
    "\n",
    "        # x is dataset without 'sbi', y is 'sbi'\n",
    "        X = df.drop(['tot', 'sbi','bemp' ,'act', 'tot', 'station'], axis=1)\n",
    "        y = df['sbi']\n",
    "\n",
    "        X = X.to_numpy()\n",
    "        y = y.to_numpy()\n",
    "\n",
    "        X = torch.from_numpy(X)\n",
    "        y = torch.from_numpy(y)\n",
    "        dataset = TensorDataset(X, y)\n",
    "\n",
    "        # get train, test loader\n",
    "        self.all_data_loader = torch.utils.data.DataLoader(dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "        # split train, test\n",
    "        train_size = int(0.8 * len(dataset))\n",
    "        test_size = len(dataset) - train_size\n",
    "        train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "        # get train, test loader\n",
    "        self.train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "        self.test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=128, shuffle=True)\n",
    "        print(f'get data loader for {self.station}')\n",
    "\n",
    "    def objective(self,trial):\n",
    "        # Sample hyperparameters\n",
    "        input_size = 8\n",
    "        hidden_size = trial.suggest_int('hidden_size', 3, 2000, log=True)  # Single hidden size for all layers\n",
    "        output_size = 1\n",
    "        learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-1, log=True)\n",
    "        dropout_rate = trial.suggest_float('dropout_rate', 0.2, 0.5)\n",
    "\n",
    "        # Instantiate the model with sampled hyperparameters\n",
    "        model = Net(input_size, hidden_size, output_size, dropout_rate)\n",
    "\n",
    "        # Define the loss function and optimizer\n",
    "        criterion = CustomLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "        # Training loop\n",
    "        for epoch in range(60):\n",
    "            training_loss = 0.0\n",
    "            eval_loss = 0.0\n",
    "            # Iterate over the DataLoader for training data\n",
    "            for i, data in enumerate(self.train_loader):\n",
    "                inputs, labels = data\n",
    "                inputs = inputs.float()\n",
    "                labels = labels.float()\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                outputs = outputs.reshape(-1)\n",
    "                loss = criterion(outputs, labels).sum()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                training_loss += loss.item()\n",
    "\n",
    "            model.eval()\n",
    "            # Iterate over the DataLoader for test data\n",
    "            with torch.no_grad():\n",
    "                for i, data in enumerate(self.test_loader):\n",
    "                    inputs, labels = data\n",
    "                    inputs = inputs.float()\n",
    "                    labels = labels.float()\n",
    "                    outputs = model(inputs).reshape(-1)\n",
    "                    loss = criterion(outputs, labels).sum()\n",
    "                    eval_loss += loss\n",
    "\n",
    "            # Optuna logs the running loss for each epoch\n",
    "            trial.report(eval_loss, epoch)\n",
    "            # Handle pruning based on the intermediate value\n",
    "            if trial.should_prune():\n",
    "                raise optuna.exceptions.TrialPruned()\n",
    "        # Return the running loss as the objective value to minimize\n",
    "        return eval_loss      \n",
    "\n",
    "\n",
    "    def get_best_param(self):\n",
    "        # Create an Optuna Study\n",
    "        study = optuna.create_study(direction='minimize', storage='sqlite:///db.sqlite3', study_name=f'{self.station}', load_if_exists=True)\n",
    "        num_trials = len(study.trials)\n",
    "\n",
    "        if num_trials <= 10:\n",
    "            study.optimize(self.objective, n_trials=5)\n",
    "        # Run the optimization process\n",
    "\n",
    "        print(f'{self.station} eval_loss: {study.best_trial.value}')\n",
    "\n",
    "        # Access the best hyperparameters\n",
    "        best_params = study.best_trial.params\n",
    "\n",
    "        self.best_params = best_params\n",
    "\n",
    "        print(self.station,' Best hyperparameters:', best_params)\n",
    "\n",
    "    def train(self):\n",
    "        # Instantiate the final model with the best hyperparameters\n",
    "        final_model = Net(8, self.best_params['hidden_size'], 1, self.best_params['dropout_rate'])\n",
    "\n",
    "        # ... rest of your training code for the final model\n",
    "        criterion = CustomLoss()\n",
    "        optimizer = optim.Adam(final_model.parameters(), lr=self.best_params['learning_rate'])\n",
    "\n",
    "        # train with whole dataset\n",
    "        running_loss = 0.0\n",
    "        loss_list = []\n",
    "        for epoch in range(10):\n",
    "            for i, data in enumerate(self.all_data_loader):\n",
    "                inputs, labels = data\n",
    "                inputs = inputs.float()\n",
    "                labels = labels.float()\n",
    "                optimizer.zero_grad()\n",
    "                outputs = final_model(inputs)\n",
    "                outputs = outputs.squeeze()\n",
    "                loss = criterion(outputs, labels).sum()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item()/128\n",
    "            print(f'epoch: {epoch+1}, loss: {running_loss/self.all_data_loader.__len__()}')\n",
    "            loss_list.append(running_loss/self.all_data_loader.__len__())\n",
    "            running_loss = 0.0\n",
    "        save_loss_in_csv(self.station, loss_list)\n",
    "        \n",
    "        final_model.eval()\n",
    "        # save model\n",
    "        torch.save(final_model, f'models/{self.station}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create submission data\n",
    "\n",
    "df = pd.read_csv('html.2023.final.data/sample_submission.csv')\n",
    "# ['time', 'mon', 'tue', 'wed', 'thu', 'fri', 'sat', 'sun']\n",
    "submission = pd.DataFrame(columns=['id','time', 'mon', 'tue', 'wed', 'thu', 'fri', 'sat', 'sun'])\n",
    "rows = []\n",
    "for x in df['id']:\n",
    "    date = x[:8]\n",
    "    station = x[9:18]\n",
    "    time = x[19:]\n",
    "    one_hot_weekday = get_one_hot_weekday(date)\n",
    "    time = time_to_minute(time)/1440\n",
    "    row = {'id': x,'time':time, 'mon':one_hot_weekday[0], 'tue':one_hot_weekday[1], 'wed':one_hot_weekday[2], 'thu':one_hot_weekday[3], 'fri':one_hot_weekday[4], 'sat':one_hot_weekday[5], 'sun':one_hot_weekday[6]}\n",
    "    rows.append(row)\n",
    "\n",
    "submission = pd.DataFrame(rows)\n",
    "\n",
    "# create submission loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['500101001', '500101002', '500101003', '500101004', '500101005', '500101006', '500101007', '500101008', '500101009', '500101010', '500101013', '500101014', '500101015', '500101018', '500101019', '500101020', '500101021', '500101022', '500101023', '500101024', '500101025', '500101026', '500101027', '500101028', '500101029', '500101030', '500101031', '500101032', '500101033', '500101034', '500101035', '500101036', '500101037', '500101038', '500101039']\n",
      "save 500101001.csv\n",
      "(792, 2)\n",
      "save 500101002.csv\n",
      "(792, 2)\n",
      "save 500101003.csv\n",
      "(792, 2)\n",
      "save 500101004.csv\n",
      "(792, 2)\n",
      "save 500101005.csv\n",
      "(792, 2)\n",
      "save 500101006.csv\n",
      "(792, 2)\n",
      "save 500101007.csv\n",
      "(792, 2)\n",
      "save 500101008.csv\n",
      "(792, 2)\n",
      "save 500101009.csv\n",
      "(792, 2)\n",
      "save 500101010.csv\n",
      "(792, 2)\n",
      "save 500101013.csv\n",
      "(792, 2)\n",
      "save 500101014.csv\n",
      "(792, 2)\n",
      "save 500101015.csv\n",
      "(792, 2)\n",
      "save 500101018.csv\n",
      "(792, 2)\n",
      "save 500101019.csv\n",
      "(792, 2)\n",
      "save 500101020.csv\n",
      "(792, 2)\n",
      "save 500101021.csv\n",
      "(792, 2)\n",
      "save 500101022.csv\n",
      "(792, 2)\n",
      "save 500101023.csv\n",
      "(792, 2)\n",
      "save 500101024.csv\n",
      "(792, 2)\n",
      "save 500101025.csv\n",
      "(792, 2)\n",
      "save 500101026.csv\n",
      "(792, 2)\n",
      "save 500101027.csv\n",
      "(792, 2)\n",
      "save 500101028.csv\n",
      "(792, 2)\n",
      "save 500101029.csv\n",
      "(792, 2)\n",
      "save 500101030.csv\n",
      "(792, 2)\n",
      "save 500101031.csv\n",
      "(792, 2)\n",
      "save 500101032.csv\n",
      "(792, 2)\n",
      "save 500101033.csv\n",
      "(792, 2)\n",
      "save 500101034.csv\n",
      "(792, 2)\n",
      "save 500101035.csv\n",
      "(792, 2)\n",
      "save 500101036.csv\n",
      "(792, 2)\n",
      "save 500101037.csv\n",
      "(792, 2)\n",
      "save 500101038.csv\n",
      "(792, 2)\n",
      "save 500101039.csv\n",
      "(792, 2)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "folder_path = 'models/'  # replace with your actual folder path\n",
    "trained_models = os.listdir(folder_path)\n",
    "trained_models.sort()\n",
    "print(trained_models)\n",
    "\n",
    "\n",
    "outputdf = pd.DataFrame(columns=['id','sbi'])\n",
    "tmp = []\n",
    "\n",
    "## read models\n",
    "for models in trained_models:\n",
    "    # read json\n",
    "    find_tot = pd.read_json(f'html.2023.final.data/release/20231002/{models}.json', convert_axes=False)\n",
    "    find_tot = find_tot.transpose()\n",
    "    for n in find_tot['tot'].to_numpy():\n",
    "        try:\n",
    "            m = int(n)\n",
    "            TOT = n\n",
    "            break\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    model = torch.load(folder_path + models)\n",
    "    model.eval()\n",
    "    tmp = []\n",
    "\n",
    "    for row in submission.iterrows():\n",
    "        data = row[1]\n",
    "        if data['id'][9:18] == models:\n",
    "            inputs = torch.tensor(data[1:].tolist()).float()\n",
    "            outputs = model(inputs)\n",
    "            outputs = outputs.item()\n",
    "            row = {'id':data.iloc[0], 'sbi':(outputs*TOT)}\n",
    "            tmp.append(row)\n",
    "    # save without index\n",
    "    outputdf = pd.DataFrame(tmp)\n",
    "    outputdf.to_csv(f'predict/{models}.csv', index=False)\n",
    "    print(f'save {models}.csv')\n",
    "    print(outputdf.shape)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "html-qsiNAWFM-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
