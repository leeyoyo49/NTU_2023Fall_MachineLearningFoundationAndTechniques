{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import random_split, TensorDataset\n",
    "\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "import optuna\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the gradient boosting regression model with PyTorch\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, dropout_rate):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.dropout = nn.Dropout(p=dropout_rate)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "    \n",
    "class CustomLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomLoss, self).__init__()\n",
    "        \n",
    "    def forward(self, outputs, labels):\n",
    "        # Define your custom loss function here\n",
    "        # This is a simple example, replace it with your own function\n",
    "        custom_loss = 3*(torch.abs(labels - outputs))*(torch.abs(labels - 1/3) + torch.abs(labels -2/3))\n",
    "        return custom_loss\n",
    "    \n",
    "def objective(trial):\n",
    "    # Sample hyperparameters\n",
    "    input_size = 8\n",
    "    hidden_size = trial.suggest_int('hidden_size', 3, 10000, log=True)  # Single hidden size for all layers\n",
    "    output_size = 1\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-7, 1e1, log=True)\n",
    "    dropout_rate = trial.suggest_float('dropout_rate', 0.0, 0.3)\n",
    "\n",
    "    # Instantiate the model with sampled hyperparameters\n",
    "    model = Net(input_size, hidden_size, output_size, dropout_rate)\n",
    "\n",
    "    # Define the loss function and optimizer\n",
    "    criterion = CustomLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(100):\n",
    "        training_loss = 0.0\n",
    "        eval_loss = 0.0\n",
    "\n",
    "        # Iterate over the DataLoader for training data\n",
    "        for i, data in enumerate(train_loader):\n",
    "            inputs, labels = data\n",
    "            inputs = inputs.float()\n",
    "            labels = labels.float()\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            outputs = outputs.reshape(-1)\n",
    "            loss = criterion(outputs, labels).sum()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            training_loss += loss.item()\n",
    "\n",
    "        model.eval()\n",
    "        # Iterate over the DataLoader for test data\n",
    "        with torch.no_grad():\n",
    "            for i, data in enumerate(test_loader):\n",
    "                inputs, labels = data\n",
    "                inputs = inputs.float()\n",
    "                labels = labels.float()\n",
    "                outputs = model(inputs).reshape(-1)\n",
    "                loss = criterion(outputs, labels).sum()\n",
    "                eval_loss += loss\n",
    "\n",
    "        # Optuna logs the running loss for each epoch\n",
    "        trial.report(eval_loss, epoch)\n",
    "\n",
    "        # Handle pruning based on the intermediate value\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "    # Return the running loss as the objective value to minimize\n",
    "    return eval_loss\n",
    "\n",
    "\n",
    "class station_model():\n",
    "    def __init__(self, station):\n",
    "        self.station = station\n",
    "\n",
    "    def get_data_loader(self):\n",
    "        df = pd.read_parquet(f'parquets/{self.station}.parquet')\n",
    "        TOT = df['tot'].iloc[0]\n",
    "        df['sbi'] = df['sbi']/TOT\n",
    "        df['time'] = df['time']/1440\n",
    "\n",
    "        # x is dataset without 'sbi', y is 'sbi'\n",
    "        X = df.drop(['tot', 'sbi','bemp' ,'act', 'tot', 'station'], axis=1)\n",
    "        y = df['sbi']\n",
    "\n",
    "        X = X.to_numpy()\n",
    "        y = y.to_numpy()\n",
    "\n",
    "        X = torch.from_numpy(X)\n",
    "        y = torch.from_numpy(y)\n",
    "        dataset = TensorDataset(X, y)\n",
    "\n",
    "        # get train, test loader\n",
    "        self.all_data_loader = torch.utils.data.DataLoader(dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "        # split train, test\n",
    "        train_size = int(0.8 * len(dataset))\n",
    "        test_size = len(dataset) - train_size\n",
    "        train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "        # get train, test loader\n",
    "        self.train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "        self.test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=128, shuffle=True)\n",
    "        print(f'get data loader for {self.station}')\n",
    "\n",
    "    def objective(self,trial):\n",
    "        # Sample hyperparameters\n",
    "        input_size = 8\n",
    "        hidden_size = trial.suggest_int('hidden_size', 3, 5000, log=True)  # Single hidden size for all layers\n",
    "        output_size = 1\n",
    "        learning_rate = trial.suggest_float('learning_rate', 1e-7, 1e1, log=True)\n",
    "        dropout_rate = trial.suggest_float('dropout_rate', 0.0, 0.5)\n",
    "\n",
    "        # Instantiate the model with sampled hyperparameters\n",
    "        model = Net(input_size, hidden_size, output_size, dropout_rate)\n",
    "\n",
    "        # Define the loss function and optimizer\n",
    "        criterion = CustomLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "        # Training loop\n",
    "        for epoch in range(100):\n",
    "            training_loss = 0.0\n",
    "            eval_loss = 0.0\n",
    "            # Iterate over the DataLoader for training data\n",
    "            for i, data in enumerate(self.train_loader):\n",
    "                inputs, labels = data\n",
    "                inputs = inputs.float()\n",
    "                labels = labels.float()\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                outputs = outputs.reshape(-1)\n",
    "                loss = criterion(outputs, labels).sum()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                training_loss += loss.item()\n",
    "\n",
    "            model.eval()\n",
    "            # Iterate over the DataLoader for test data\n",
    "            with torch.no_grad():\n",
    "                for i, data in enumerate(self.test_loader):\n",
    "                    inputs, labels = data\n",
    "                    inputs = inputs.float()\n",
    "                    labels = labels.float()\n",
    "                    outputs = model(inputs).reshape(-1)\n",
    "                    loss = criterion(outputs, labels).sum()\n",
    "                    eval_loss += loss\n",
    "\n",
    "            # Optuna logs the running loss for each epoch\n",
    "            trial.report(eval_loss, epoch)\n",
    "            # Handle pruning based on the intermediate value\n",
    "            if trial.should_prune():\n",
    "                raise optuna.exceptions.TrialPruned()\n",
    "        # Return the running loss as the objective value to minimize\n",
    "        return eval_loss      \n",
    "\n",
    "\n",
    "    def get_best_param(self):\n",
    "        # Create an Optuna Study\n",
    "        study = optuna.create_study(direction='minimize', storage='sqlite:///db.sqlite3', study_name=f'{self.station}', load_if_exists=True)\n",
    "\n",
    "        # Run the optimization process\n",
    "        study.optimize(self.objective, n_trials=10)\n",
    "\n",
    "        print(f'{self.station} eval_loss: {study.best_trial.value}')\n",
    "\n",
    "        # Access the best hyperparameters\n",
    "        best_params = study.best_trial.params\n",
    "\n",
    "        self.best_params = best_params\n",
    "\n",
    "        print(self.station,' Best hyperparameters:', best_params)\n",
    "\n",
    "    def train(self):\n",
    "        # Instantiate the final model with the best hyperparameters\n",
    "        final_model = Net(8, self.best_params['hidden_size'], 1, self.best_params['dropout_rate'])\n",
    "\n",
    "        # ... rest of your training code for the final model\n",
    "        criterion = CustomLoss()\n",
    "        optimizer = optim.Adam(final_model.parameters(), lr=self.best_params['learning_rate'])\n",
    "\n",
    "        # train with whole dataset\n",
    "        running_loss = 0.0\n",
    "        for epoch in range(10):\n",
    "            for i, data in enumerate(self.all_data_loader):\n",
    "                inputs, labels = data\n",
    "                inputs = inputs.float()\n",
    "                labels = labels.float()\n",
    "                optimizer.zero_grad()\n",
    "                outputs = final_model(inputs)\n",
    "                outputs = outputs.squeeze()\n",
    "                loss = criterion(outputs, labels).sum()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item()/128\n",
    "            print(f'epoch: {epoch+1}, loss: {running_loss/self.all_data_loader.__len__()}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "        final_model.eval()\n",
    "        # save model\n",
    "        torch.save(final_model.state_dict(), f'models/model_{self.station}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2550613376388636 112 500101002\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Expected state_dict to be dict-like, got <class '__main__.Net'>.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/yl/HTML/final/test.ipynb Cell 3\u001b[0m line \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yl/HTML/final/test.ipynb#W3sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m \u001b[39m# Load the model\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yl/HTML/final/test.ipynb#W3sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m model_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(folder_path, model_file)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/yl/HTML/final/test.ipynb#W3sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m model\u001b[39m.\u001b[39;49mload_state_dict(torch\u001b[39m.\u001b[39;49mload(model_path))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yl/HTML/final/test.ipynb#W3sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m model\u001b[39m.\u001b[39meval()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yl/HTML/final/test.ipynb#W3sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m criterion \u001b[39m=\u001b[39m CustomLoss()\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/html-qsiNAWFM-py3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:2103\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2069\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Copies parameters and buffers from :attr:`state_dict` into\u001b[39;00m\n\u001b[1;32m   2070\u001b[0m \u001b[39mthis module and its descendants. If :attr:`strict` is ``True``, then\u001b[39;00m\n\u001b[1;32m   2071\u001b[0m \u001b[39mthe keys of :attr:`state_dict` must exactly match the keys returned\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2100\u001b[0m \u001b[39m    ``RuntimeError``.\u001b[39;00m\n\u001b[1;32m   2101\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   2102\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(state_dict, Mapping):\n\u001b[0;32m-> 2103\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mExpected state_dict to be dict-like, got \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(state_dict)\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   2105\u001b[0m missing_keys: List[\u001b[39mstr\u001b[39m] \u001b[39m=\u001b[39m []\n\u001b[1;32m   2106\u001b[0m unexpected_keys: List[\u001b[39mstr\u001b[39m] \u001b[39m=\u001b[39m []\n",
      "\u001b[0;31mTypeError\u001b[0m: Expected state_dict to be dict-like, got <class '__main__.Net'>."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "folder_path = 'models/'  # replace with your actual folder path\n",
    "trained_models = os.listdir(folder_path)\n",
    "\n",
    "# read models\n",
    "for model_file in trained_models:\n",
    "        \n",
    "    # Load the study\n",
    "    station = model_file\n",
    "    study = optuna.load_study(study_name=station, storage=\"sqlite:///db.sqlite3\")\n",
    "\n",
    "    df = pd.read_parquet(f'parquets/{station}.parquet')\n",
    "    TOT = df['tot'].iloc[0]\n",
    "    df['sbi'] = df['sbi']/TOT\n",
    "    df['time'] = df['time']/1440\n",
    "\n",
    "    # x is dataset without 'sbi', y is 'sbi'\n",
    "    X = df.drop(['tot', 'sbi','bemp' ,'act', 'tot', 'station'], axis=1)\n",
    "    y = df['sbi']\n",
    "\n",
    "    X = X.to_numpy()\n",
    "    y = y.to_numpy()\n",
    "\n",
    "    X = torch.from_numpy(X)\n",
    "    y = torch.from_numpy(y)\n",
    "    dataset = TensorDataset(X, y)\n",
    "\n",
    "    # get train, test loader\n",
    "    all_data_loader = torch.utils.data.DataLoader(dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "    # split train, test\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    test_size = len(dataset) - train_size\n",
    "    train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "    # get train, test loader\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "\n",
    "    # Get the best parameters\n",
    "    best_params = study.best_params\n",
    "\n",
    "    hidden_size = best_params[\"hidden_size\"]\n",
    "    dropout_rate = best_params[\"dropout_rate\"]\n",
    "    model = Net(8, hidden_size, 1, dropout_rate)\n",
    "    print(dropout_rate, hidden_size, model_file)\n",
    "    \n",
    "    # Load the model\n",
    "    model_path = os.path.join(folder_path, model_file)\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.eval()\n",
    "\n",
    "\n",
    "    criterion = CustomLoss()\n",
    "    eval_loss = 0.0\n",
    "\n",
    "    # Make predictions\n",
    "    for i, data in enumerate(test_loader):\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.float()\n",
    "        labels = labels.float()\n",
    "        outputs = model(inputs).reshape(-1)\n",
    "        loss = criterion(outputs, labels).sum()\n",
    "        eval_loss += loss/128\n",
    "    print(f'{station} eval_loss: {eval_loss/ test_loader.__len__()}')\n",
    "    model.eval()\n",
    "    torch.save(model, f'model_true/{station}')\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=8, out_features=510, bias=True)\n",
      "  (dropout): Dropout(p=0.3103772369753493, inplace=False)\n",
      "  (fc2): Linear(in_features=510, out_features=1, bias=True)\n",
      "  (relu): ReLU()\n",
      ")\n",
      "Index(['time', 'mon', 'tue', 'wed', 'thu', 'fri', 'sat', 'sun'], dtype='object')\n",
      "500101014 eval_loss: 0.2925603985786438\n"
     ]
    }
   ],
   "source": [
    "model = torch.load('model_true/500101001.pth')\n",
    "model.eval()\n",
    "print(model)\n",
    "\n",
    "# Load the study\n",
    "df = pd.read_parquet(f'parquets/500101001.parquet')\n",
    "TOT = df['tot'].iloc[0]\n",
    "df['sbi'] = df['sbi']/TOT\n",
    "df['time'] = df['time']/1440\n",
    "\n",
    "# x is dataset without 'sbi', y is 'sbi'\n",
    "X = df.drop(['tot', 'sbi','bemp' ,'act', 'tot', 'station'], axis=1)\n",
    "print(X.columns)\n",
    "y = df['sbi']\n",
    "\n",
    "X = X.to_numpy()\n",
    "y = y.to_numpy()\n",
    "\n",
    "X = torch.from_numpy(X)\n",
    "y = torch.from_numpy(y)\n",
    "dataset = TensorDataset(X, y)\n",
    "\n",
    "# get train, test loader\n",
    "all_data_loader = torch.utils.data.DataLoader(dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "# split train, test\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# get train, test loader\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "eval_loss = 0.0\n",
    "for i, data in enumerate(test_loader):\n",
    "    inputs, labels = data\n",
    "    inputs = inputs.float()\n",
    "    labels = labels.float()\n",
    "    outputs = model(inputs).reshape(-1)\n",
    "    loss = criterion(outputs, labels).sum()\n",
    "    eval_loss += loss\n",
    "\n",
    "print(f'{station} eval_loss: {eval_loss/test_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['500119091', '500119090', '500119089', '500119088', '500119087',\n",
       "       '500119086', '500119085', '500119084', '500119083', '500119082',\n",
       "       '500119081', '500119080', '500119079', '500119078', '500119077',\n",
       "       '500119076', '500119075', '500119074', '500119072', '500119071',\n",
       "       '500119070', '500119069', '500119068', '500119067', '500119066',\n",
       "       '500119065', '500119064', '500119063', '500119062', '500119061',\n",
       "       '500119060', '500119059', '500119058', '500119057', '500119056',\n",
       "       '500119055', '500119054', '500119053', '500119052', '500119051',\n",
       "       '500119050', '500119049', '500119048', '500119047', '500119046',\n",
       "       '500119045', '500119044', '500119043', '500106004', '500106003',\n",
       "       '500106002', '500105066', '500101219', '500101216', '500101209',\n",
       "       '500101199', '500101193', '500101191', '500101190', '500101189',\n",
       "       '500101188', '500101185', '500101184', '500101181', '500101176',\n",
       "       '500101175', '500101166', '500101123', '500101115', '500101114',\n",
       "       '500101094', '500101093', '500101092', '500101091', '500101042',\n",
       "       '500101041', '500101040', '500101039', '500101038', '500101037',\n",
       "       '500101036', '500101035', '500101034', '500101033', '500101032',\n",
       "       '500101031', '500101030', '500101029', '500101028', '500101027',\n",
       "       '500101026', '500101025', '500101024', '500101023', '500101022'],\n",
       "      dtype='<U9')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "available_station = np.loadtxt('html.2023.final.data/sno_test_set.txt', dtype='str')\n",
    "# available_station = available_station[:35]\n",
    "folder_path = 'models/'  # replace with your actual folder path\n",
    "trained_models = os.listdir(folder_path)\n",
    "available_station = np.setdiff1d(available_station, trained_models)\n",
    "\n",
    "# reverse available_station\n",
    "available_station = available_station[::-1]\n",
    "available_station"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "html-qsiNAWFM-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
